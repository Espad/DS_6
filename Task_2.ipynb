{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача - multi-label классификация - освежить память что это значит (https://scikit-learn.org/stable/modules/multiclass.html).\n",
    "\n",
    "Тартеты: 14 последних колонок - df.iloc[:,-14:]\n",
    "\n",
    "Метрика: Micro averaged F1. Хорошо бы оценивать качество с помощью кросс-валидации.\n",
    "\n",
    "- Построить baseline с использованием линейной модели из scikit-learn\n",
    "\n",
    "\n",
    "- Чистый Tensorflow\n",
    "\n",
    "Построить линейную модель для решения этой задачи. Можно взять существующий код из блокнота Tensorflow examples 1.ipynb и переделать.\n",
    "\n",
    "\n",
    "- Keras\n",
    "\n",
    "Сделать тоже самое как и в Tensorflow но использовав чистый Keras\n",
    "\n",
    "Добавить слоёв и нелинейностей, сравнить качество моделей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pandas in c:\\anaconda\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in c:\\anaconda\\lib\\site-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in c:\\anaconda\\lib\\site-packages (from pandas) (1.13.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in c:\\anaconda\\lib\\site-packages (from pandas) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in c:\\anaconda\\lib\\site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Requirement already up-to-date: scikit-learn in c:\\anaconda\\lib\\site-packages (0.20.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (1.13.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already up-to-date: inflect in c:\\anaconda\\lib\\site-packages (2.1.0)\n",
      "Requirement already up-to-date: seaborn in c:\\anaconda\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: pandas>=0.15.2 in c:\\anaconda\\lib\\site-packages (from seaborn) (0.24.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.3 in c:\\anaconda\\lib\\site-packages (from seaborn) (1.13.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14.0 in c:\\anaconda\\lib\\site-packages (from seaborn) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib>=1.4.3 in c:\\anaconda\\lib\\site-packages (from seaborn) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in c:\\anaconda\\lib\\site-packages (from pandas>=0.15.2->seaborn) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in c:\\anaconda\\lib\\site-packages (from pandas>=0.15.2->seaborn) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in c:\\anaconda\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in c:\\anaconda\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\anaconda\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in c:\\anaconda\\lib\\site-packages (from python-dateutil>=2.5.0->pandas>=0.15.2->seaborn) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\anaconda\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (40.6.3)\n",
      "Requirement already up-to-date: pprint in c:\\anaconda\\lib\\site-packages (0.1)\n",
      "Requirement already up-to-date: tensorflow in c:\\anaconda\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in c:\\anaconda\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.16.1)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in c:\\anaconda\\lib\\site-packages (from tensorflow) (0.32.3)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.13.0,>=1.12.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.12.2)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.0.6)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in c:\\anaconda\\lib\\site-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in c:\\anaconda\\lib\\site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\anaconda\\lib\\site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in c:\\anaconda\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\anaconda\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (40.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas\n",
    "!pip install --upgrade scikit-learn\n",
    "!pip install --upgrade inflect\n",
    "!pip install --upgrade seaborn\n",
    "!pip install --upgrade pprint\n",
    "!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade contractions\n",
    "#!pip install --upgrade wordcloud\n",
    "#!pip install --upgrade umap-learn\n",
    "#!pip install --upgrade gensim\n",
    "#!pip install --upgrade pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "import inflect\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from itertools import compress\n",
    "from collections import Counter\n",
    "#import glob\n",
    "#import contractions\n",
    "#from wordcloud import WordCloud\n",
    "#import umap\n",
    "#import unicodedata\n",
    "#from bs4 import BeautifulSoup\n",
    "\n",
    "#import time\n",
    "\n",
    "#from scipy import interp\n",
    "\n",
    "import pickle\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "seed = 321\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn libs\n",
    "from sklearn.preprocessing import label_binarize,MultiLabelBinarizer,StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline,Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, KFold, cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report,f1_score, make_scorer\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import json\n",
    "import operator\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Bidirectional, GlobalAveragePooling1D\n",
    "from keras.metrics import binary_accuracy,categorical_accuracy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#convert words to sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('names')\n",
    "\n",
    "from nltk.corpus import names\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer # one more stemmer? or lemmatisation\n",
    "from nltk.stem import WordNetLemmatizer,LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = data[idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read first ten row to understand, how to parse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = pickle.load(open('data/task_2/reviews_dataset.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "769251"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "769251"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Потрясающе красивая графика космоса! Уже за это игру можно полюбить. Так же в наличии интересный осмысленный сюжет и удобное управление.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline.\n",
    "\n",
    "Обучить линейную модель на TF-IDF представлении для предсказания оценки пользователя по его отзыву. \n",
    "\n",
    "Посмотреть на важность для каждой оценки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emails(text):\n",
    "    text = re.sub(r'\\S*@\\S*\\s?','',text)    \n",
    "    return text\n",
    "\n",
    "\n",
    "#version on raw text\n",
    "def remove_punctuation2(text_punctuation,text):\n",
    "    \n",
    "    new_text = []\n",
    "    new_text = re.sub('\\n', ' ',text)\n",
    "    new_text = re.sub('\\t', '',new_text)\n",
    "    new_text = re.sub('['+text_punctuation+']', ' ',new_text)\n",
    "    new_text =  new_text.strip()\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "#version after word_tokenizer\n",
    "def remove_punctuation(text_punctuation,text):\n",
    "    \n",
    "    new_text = []\n",
    "    for word in text: \n",
    "        new_word =  re.sub('['+text_punctuation+']', ' ',word)\n",
    "        new_word =  new_word.strip()\n",
    "        new_text.append(new_word)\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "\n",
    "#ntlk word lemmatizer\n",
    "def lemmatize_stemm_text(text):\n",
    "    new_text_lemma = []\n",
    "    new_text_stemm = []\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = LancasterStemmer()\n",
    "    \n",
    "    for word in text:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='n') # v default\n",
    "        new_text_lemma.append(lemma)\n",
    "        \n",
    "        #stemm = stemmer.stem(word)\n",
    "        #new_text_stemm.append(stemm)\n",
    "        \n",
    "    return new_text_lemma, new_text_stemm\n",
    "        \n",
    "\n",
    "StopWords = list(set(stopwords.words('russian')))\n",
    "'''\n",
    "StopWords = list(set( stopwords.words('english') ).union( set(ENGLISH_STOP_WORDS)))\n",
    "newStopWords = ['jfc','jb'] # по ходу работы периодически добавляем стоп-слова\n",
    "newStopWords2 = ['arent', 'didnt', 'doesnt', 'dont', 'hadnt', 'havent', 'isnt', \n",
    "                 'mightnt', 'mustnt', 'neednt', 'shant', 'shes', 'shouldnt', 'shouldve', 'theres','thatll', 'wasnt', \n",
    "                 'werent', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve',\n",
    "                 \"'ll\", \"'re\", \"'ve\", \"n't\", 'need', 'sha', 'anna','n t','ann mari', 'ann marie', \n",
    "                 'anna diana', 'anna diane', 'anna maria', 'anne corinne', 'anne mar', 'anne marie', \n",
    "                 'barbara anne', 'bette ann', 'carol jean', 'diane marie', 'e lane', 'hans peter', \n",
    "                 'helen elizabeth', 'holly anne', 'jean christophe', 'jean francois', 'jean lou', \n",
    "                 'jean luc', 'jean marc', 'jean paul', 'jean pierre', 'jo anne', 'john david', 'john patrick', \n",
    "                 'kara lynn', 'marie ann', 'marie jeanne', 'paula grace', 'sara ann', \n",
    "                 'sheila kathryn', 'sue elle', 'terri jo', 'theresa marie','sza']\n",
    "\n",
    "#StopWords.extend(newStopWords)\n",
    "#StopWords.extend(newStopWords2)\n",
    "\n",
    "\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "\n",
    "Common_First_Names = list(set(male_names).union(set(female_names)))\n",
    "Common_First_Names = list(map(lambda word: word.lower(), Common_First_Names))\n",
    "# temporaly disabled to reduce tfidf time\n",
    "#StopWords.extend(Common_First_Names) ''';\n",
    "\n",
    "def remove_stopwords(stop_words ,text):\n",
    "    \n",
    "    new_words = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            new_words.append(word)\n",
    "\n",
    "    return new_words\n",
    "\n",
    "def remove_short_words(text, word_len):\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if len(word) >= word_len:\n",
    "            new_text.append(word)\n",
    "            \n",
    "    return new_text\n",
    "\n",
    "\n",
    "# getting source from string.punctuation\n",
    "text_punctuation = '!\"#$%&\\'()*+,-.:;<=>?@[\\\\]_`{|}~/^'\n",
    "\n",
    "def tokenize(text):\n",
    "    min_length = 3\n",
    "    \n",
    "    # remove emails from text to prevent overfit\n",
    "    text = remove_emails(text)\n",
    "    \n",
    "    # text to lowercase\n",
    "    text =  text.lower()\n",
    "\n",
    "    \n",
    "    #remove punctuation\n",
    "    text = remove_punctuation2(text_punctuation, text) \n",
    "\n",
    "    # tokenize text\n",
    "    words = word_tokenize(text,language='english')\n",
    "\n",
    "       \n",
    "    # remove stopwords\n",
    "    words = remove_stopwords(StopWords,words)\n",
    "    \n",
    "    #lemmatize words ,improve to 0.69 f1score\n",
    "    words,_ = lemmatize_stemm_text(words)\n",
    "    \n",
    "    #filter short words\n",
    "    words = remove_short_words(words, 2) #default 3\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = tokenize(x_data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['таки',\n",
       " 'удержался',\n",
       " 'написать',\n",
       " 'топ',\n",
       " 'скорей',\n",
       " 'сотрут',\n",
       " 'прочтет',\n",
       " 'ужасной',\n",
       " 'игры',\n",
       " 'star',\n",
       " 'war',\n",
       " 'empire',\n",
       " 'at',\n",
       " 'war',\n",
       " 'dvd',\n",
       " 'ещё',\n",
       " 'поискать',\n",
       " 'нормального',\n",
       " 'сюжета',\n",
       " 'стратегия',\n",
       " 'это',\n",
       " 'вовсе',\n",
       " 'провал',\n",
       " 'давно',\n",
       " 'убедился',\n",
       " 'всё',\n",
       " 'берётся',\n",
       " '1с',\n",
       " 'плане',\n",
       " 'игр',\n",
       " 'отстой',\n",
       " 'хорошая',\n",
       " 'стратегия',\n",
       " 'эпизодам',\n",
       " 'star',\n",
       " 'war',\n",
       " 'это',\n",
       " 'star',\n",
       " 'war',\n",
       " 'galactic',\n",
       " 'battleground',\n",
       " 'играл',\n",
       " 'поймет']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, random_state = seed, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF + LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=StopWords,\n",
    "                             tokenizer=tokenize, \n",
    "                             min_df=4, # ignore terms with freq less that т, lower majoring 4\n",
    "                             max_df=0.8, # ignore terms with freq more that n, upper majoring 0.8\n",
    "                             #max_features=100000,\n",
    "                             use_idf=True,  \n",
    "                             sublinear_tf=True,\n",
    "                             norm='l2',\n",
    "                             #ngram_range= (1, 3) # также используем н-граммы\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vectorised_train_documents = vectorizer.fit_transform(x_train)\n",
    "vectorised_test_documents = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(538475, 143312)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorised_train_documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230776, 143312)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorised_test_documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_score(classifier, X_train, X_test, y_train, y_test):\n",
    "    clf = classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    pred_train = clf.predict(X_train)\n",
    "    #print (\"Train score\")\n",
    "    f1_train = f1_score(y_train, pred_train, average='weighted')\n",
    "\n",
    "    #print (\"Test score\")\n",
    "    pred_test = clf.predict(X_test)\n",
    "    f1_test = f1_score(y_test, pred_test, average='weighted')\n",
    "    \n",
    "    return f1_train, f1_test, clf\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Paired):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"black\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty='l2', tol=0.0001, C=1, \n",
    "                             class_weight='balanced', random_state=seed, \n",
    "                             max_iter=1000, solver = 'newton-cg',\n",
    "                             multi_class='ovr', verbose=0, \n",
    "                             n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_,_, clf = classify_and_score(log_reg, vectorised_train_documents, \n",
    "                           vectorised_test_documents, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.57      0.48     16683\n",
      "           2       0.16      0.22      0.19      9679\n",
      "           3       0.24      0.34      0.28     17653\n",
      "           4       0.34      0.41      0.37     33086\n",
      "           5       0.89      0.76      0.82    153675\n",
      "\n",
      "   micro avg       0.64      0.64      0.64    230776\n",
      "   macro avg       0.41      0.46      0.43    230776\n",
      "weighted avg       0.70      0.64      0.66    230776\n",
      "\n",
      "Confusion matrix, without normalization\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = clf.classes_\n",
    "pred_test = clf.predict(vectorised_test_documents)\n",
    "print(classification_report(y_test,pred_test,labels = labels))\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, pred_test)\n",
    "print(plot_confusion_matrix(cnf_matrix, classes = labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучить рекуррентную нейронную сеть для предсказания оценки пользователя по его отзыву. Использовать случайную инициализация весов.\n",
    "\n",
    "- add stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, random_state = seed, \n",
    "                                                    test_size=0.3, shuffle=True,\n",
    "                                                    stratify = y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters:\n",
    "num_word = 15000\n",
    "max_features = 20000\n",
    "batch_size = 128\n",
    "embedding_dims = 128\n",
    "epochs = 3\n",
    "maxlen = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485401\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer(num_words=num_word)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "print (vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "encoded_y_test = encoder.transform(y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "y_train = np_utils.to_categorical(encoded_y_train)\n",
    "y_test = np_utils.to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = np.array(y_train)\n",
    "#y_test = np.array(y_test)\n",
    "\n",
    "print (vocab_size)\n",
    "\n",
    "print(x_train[0])\n",
    "\n",
    "print(len(x_train[0]))\n",
    "\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6579   365     7 14591     1  2034    15  7231  2724    11    28    20\n",
      "  3633    39     1   548    13    80     4  6272  3817  7044  8109    11\n",
      "  1967  1093  3690    19     6  2560     1     2  1696  6620   828     1\n",
      "    39     5 12961     9  4132  4398  7125     3     7  6858 12318  3059\n",
      "  1786     7 12318  3059     2     4  3784   102  3817   665  1525  3729\n",
      "     6    33    14 13308  1708     7 12318  3059   879  4643     7  2726\n",
      "    10    54 10758    15   283 14297   558  3611]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62131328"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size * 128 # params of first layer, 128 is number of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 80, 128)           62131328  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 62,230,789\n",
      "Trainable params: 62,230,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = 128, \n",
    "                    input_length=maxlen))#weights!\n",
    "model.add(Bidirectional(LSTM(64)))# weights!\n",
    "model.add(Dropout(0.1))\n",
    "#model.add(Dropout(0.5)) #1\n",
    "model.add(Dense(5, activation='softmax')) #softmax due to multiclass\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=2, verbose=2, mode='max',restore_best_weights=True)\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no stratify\n",
    "x_train_part,y_train_part = next_batch(15000, x_train, y_train)\n",
    "x_test_part,y_test_part = next_batch(5000, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      " - 219s - loss: 1.0673 - acc: 0.6558 - val_loss: 0.9358 - val_acc: 0.6658\n",
      "Epoch 2/3\n",
      " - 214s - loss: 0.8162 - acc: 0.6897 - val_loss: 0.9086 - val_acc: 0.6704\n",
      "Epoch 3/3\n",
      " - 221s - loss: 0.6613 - acc: 0.7504 - val_loss: 0.9878 - val_acc: 0.6750\n",
      "Wall time: 10min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Train...')\n",
    "model.fit(x_train_part, y_train_part,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose = 2,\n",
    "          callbacks=[earlyStopping],\n",
    "          validation_data=[x_test_part, y_test_part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.11      0.19       369\n",
      "           1       0.00      0.00      0.00       206\n",
      "           2       0.00      0.00      0.00       384\n",
      "           3       0.41      0.11      0.17       717\n",
      "           4       0.81      0.86      0.83      3324\n",
      "\n",
      "   micro avg       0.78      0.60      0.68      5000\n",
      "   macro avg       0.37      0.22      0.24      5000\n",
      "weighted avg       0.64      0.60      0.59      5000\n",
      " samples avg       0.60      0.60      0.60      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predicted_scores = model.predict(x_test_part)\n",
    "y_predicted_scores[y_predicted_scores>=0.5] = 1\n",
    "y_predicted_scores[y_predicted_scores<0.5] = 0\n",
    "\n",
    "print('Classification report\\n')\n",
    "print(classification_report(y_test_part, y_predicted_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fasttext.\n",
    "\n",
    "Обучить «fasttext» модель на n-граммах используя случайную инициализацию эмбедингов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, random_state = seed, \n",
    "                                                    test_size=0.3, shuffle=True,\n",
    "                                                    stratify = y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 2\n",
    "max_features = 20000\n",
    "maxlen = 250\n",
    "batch_size = 128\n",
    "embedding_dims = 64\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "encoded_y_test = encoder.transform(y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "y_train = np_utils.to_categorical(encoded_y_train)\n",
    "y_test = np_utils.to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 2-gram features\n",
      "Average train sequence length: 54\n",
      "Average test sequence length: 52\n"
     ]
    }
   ],
   "source": [
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_test)), dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (538475, 250)\n",
      "x_test shape: (230776, 250)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = x_train.max()#len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_part,y_train_part = next_batch(25000, x_train, y_train)\n",
    "x_test_part,y_test_part = next_batch(11000, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 250, 64)           132820032 \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_7 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 132,820,357\n",
      "Trainable params: 132,820,357\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "# используя случайную инициализацию эмбедингов\n",
    "model.add(Embedding(input_dim = vocab_size,\n",
    "                    output_dim = embedding_dims,\n",
    "                    input_length=maxlen, \n",
    "                    embeddings_initializer = 'RandomNormal')) \n",
    "\n",
    "# we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "# of all words in the document\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a softmax:\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=2, verbose=2, mode='max',restore_best_weights=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = sgd,\n",
    "              #optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#model.compile(loss='mse', optimizer=TFOptimizer(tf.train.GradientDescentOptimizer(0.1)))\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:109: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 132820032 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 11000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - ETA: 27:38 - loss: 1.6295 - acc: 0.05 - ETA: 16:07 - loss: 1.6277 - acc: 0.05 - ETA: 12:16 - loss: 1.6247 - acc: 0.05 - ETA: 10:15 - loss: 1.6185 - acc: 0.04 - ETA: 9:03 - loss: 1.6122 - acc: 0.0797 - ETA: 8:10 - loss: 1.6034 - acc: 0.181 - ETA: 7:32 - loss: 1.5934 - acc: 0.248 - ETA: 7:03 - loss: 1.5849 - acc: 0.293 - ETA: 6:41 - loss: 1.5754 - acc: 0.327 - ETA: 6:22 - loss: 1.5626 - acc: 0.362 - ETA: 6:07 - loss: 1.5487 - acc: 0.393 - ETA: 5:54 - loss: 1.5347 - acc: 0.418 - ETA: 5:43 - loss: 1.5221 - acc: 0.437 - ETA: 5:34 - loss: 1.5129 - acc: 0.448 - ETA: 5:24 - loss: 1.4980 - acc: 0.465 - ETA: 5:17 - loss: 1.4860 - acc: 0.476 - ETA: 5:10 - loss: 1.4759 - acc: 0.483 - ETA: 5:04 - loss: 1.4647 - acc: 0.491 - ETA: 4:58 - loss: 1.4511 - acc: 0.501 - ETA: 4:53 - loss: 1.4372 - acc: 0.511 - ETA: 4:48 - loss: 1.4228 - acc: 0.520 - ETA: 4:44 - loss: 1.4090 - acc: 0.529 - ETA: 4:39 - loss: 1.4019 - acc: 0.532 - ETA: 4:35 - loss: 1.3919 - acc: 0.537 - ETA: 4:31 - loss: 1.3818 - acc: 0.543 - ETA: 4:28 - loss: 1.3717 - acc: 0.547 - ETA: 4:25 - loss: 1.3618 - acc: 0.552 - ETA: 4:22 - loss: 1.3590 - acc: 0.552 - ETA: 4:20 - loss: 1.3473 - acc: 0.558 - ETA: 4:18 - loss: 1.3425 - acc: 0.559 - ETA: 4:15 - loss: 1.3382 - acc: 0.561 - ETA: 4:12 - loss: 1.3306 - acc: 0.564 - ETA: 4:09 - loss: 1.3252 - acc: 0.567 - ETA: 4:06 - loss: 1.3144 - acc: 0.571 - ETA: 4:03 - loss: 1.3101 - acc: 0.573 - ETA: 4:00 - loss: 1.3072 - acc: 0.574 - ETA: 3:58 - loss: 1.3003 - acc: 0.577 - ETA: 3:55 - loss: 1.2932 - acc: 0.580 - ETA: 3:53 - loss: 1.2877 - acc: 0.582 - ETA: 3:50 - loss: 1.2804 - acc: 0.585 - ETA: 3:48 - loss: 1.2805 - acc: 0.585 - ETA: 3:46 - loss: 1.2782 - acc: 0.586 - ETA: 3:44 - loss: 1.2741 - acc: 0.587 - ETA: 3:43 - loss: 1.2688 - acc: 0.589 - ETA: 3:41 - loss: 1.2670 - acc: 0.590 - ETA: 3:39 - loss: 1.2650 - acc: 0.591 - ETA: 3:37 - loss: 1.2593 - acc: 0.593 - ETA: 3:35 - loss: 1.2555 - acc: 0.594 - ETA: 3:34 - loss: 1.2529 - acc: 0.595 - ETA: 3:32 - loss: 1.2510 - acc: 0.596 - ETA: 3:30 - loss: 1.2483 - acc: 0.597 - ETA: 3:28 - loss: 1.2426 - acc: 0.600 - ETA: 3:27 - loss: 1.2391 - acc: 0.601 - ETA: 3:25 - loss: 1.2349 - acc: 0.603 - ETA: 3:23 - loss: 1.2311 - acc: 0.605 - ETA: 3:21 - loss: 1.2275 - acc: 0.606 - ETA: 3:19 - loss: 1.2279 - acc: 0.605 - ETA: 3:17 - loss: 1.2249 - acc: 0.606 - ETA: 3:15 - loss: 1.2238 - acc: 0.607 - ETA: 3:13 - loss: 1.2214 - acc: 0.608 - ETA: 3:12 - loss: 1.2193 - acc: 0.609 - ETA: 3:10 - loss: 1.2181 - acc: 0.609 - ETA: 3:08 - loss: 1.2159 - acc: 0.610 - ETA: 3:06 - loss: 1.2156 - acc: 0.609 - ETA: 3:05 - loss: 1.2135 - acc: 0.610 - ETA: 3:03 - loss: 1.2124 - acc: 0.611 - ETA: 3:01 - loss: 1.2099 - acc: 0.612 - ETA: 3:00 - loss: 1.2084 - acc: 0.612 - ETA: 2:58 - loss: 1.2079 - acc: 0.612 - ETA: 2:56 - loss: 1.2054 - acc: 0.613 - ETA: 2:55 - loss: 1.2040 - acc: 0.614 - ETA: 2:53 - loss: 1.2013 - acc: 0.615 - ETA: 2:52 - loss: 1.1984 - acc: 0.616 - ETA: 2:50 - loss: 1.1980 - acc: 0.616 - ETA: 2:49 - loss: 1.1974 - acc: 0.616 - ETA: 2:47 - loss: 1.1962 - acc: 0.616 - ETA: 2:46 - loss: 1.1946 - acc: 0.617 - ETA: 2:44 - loss: 1.1925 - acc: 0.618 - ETA: 2:42 - loss: 1.1902 - acc: 0.619 - ETA: 2:41 - loss: 1.1868 - acc: 0.621 - ETA: 2:39 - loss: 1.1872 - acc: 0.620 - ETA: 2:38 - loss: 1.1856 - acc: 0.621 - ETA: 2:36 - loss: 1.1846 - acc: 0.621 - ETA: 2:35 - loss: 1.1849 - acc: 0.621 - ETA: 2:33 - loss: 1.1826 - acc: 0.622 - ETA: 2:32 - loss: 1.1821 - acc: 0.622 - ETA: 2:31 - loss: 1.1813 - acc: 0.622 - ETA: 2:29 - loss: 1.1791 - acc: 0.623 - ETA: 2:27 - loss: 1.1787 - acc: 0.624 - ETA: 2:26 - loss: 1.1784 - acc: 0.624 - ETA: 2:24 - loss: 1.1780 - acc: 0.623 - ETA: 2:23 - loss: 1.1777 - acc: 0.623 - ETA: 2:22 - loss: 1.1773 - acc: 0.623 - ETA: 2:20 - loss: 1.1760 - acc: 0.624 - ETA: 2:19 - loss: 1.1748 - acc: 0.624 - ETA: 2:17 - loss: 1.1733 - acc: 0.625 - ETA: 2:16 - loss: 1.1719 - acc: 0.625 - ETA: 2:14 - loss: 1.1703 - acc: 0.626 - ETA: 2:13 - loss: 1.1694 - acc: 0.626 - ETA: 2:11 - loss: 1.1674 - acc: 0.627 - ETA: 2:10 - loss: 1.1660 - acc: 0.628 - ETA: 2:08 - loss: 1.1657 - acc: 0.628 - ETA: 2:07 - loss: 1.1643 - acc: 0.628 - ETA: 2:05 - loss: 1.1630 - acc: 0.628 - ETA: 2:04 - loss: 1.1624 - acc: 0.628 - ETA: 2:02 - loss: 1.1625 - acc: 0.628 - ETA: 2:01 - loss: 1.1612 - acc: 0.629 - ETA: 1:59 - loss: 1.1602 - acc: 0.629 - ETA: 1:58 - loss: 1.1599 - acc: 0.629 - ETA: 1:57 - loss: 1.1579 - acc: 0.630 - ETA: 1:55 - loss: 1.1590 - acc: 0.630 - ETA: 1:54 - loss: 1.1584 - acc: 0.630 - ETA: 1:52 - loss: 1.1570 - acc: 0.630 - ETA: 1:51 - loss: 1.1563 - acc: 0.631 - ETA: 1:50 - loss: 1.1543 - acc: 0.632 - ETA: 1:48 - loss: 1.1543 - acc: 0.631 - ETA: 1:47 - loss: 1.1540 - acc: 0.631 - ETA: 1:45 - loss: 1.1528 - acc: 0.632 - ETA: 1:44 - loss: 1.1520 - acc: 0.632 - ETA: 1:43 - loss: 1.1516 - acc: 0.632 - ETA: 1:41 - loss: 1.1516 - acc: 0.632 - ETA: 1:40 - loss: 1.1513 - acc: 0.633 - ETA: 1:38 - loss: 1.1512 - acc: 0.633 - ETA: 1:37 - loss: 1.1506 - acc: 0.633 - ETA: 1:36 - loss: 1.1501 - acc: 0.633 - ETA: 1:34 - loss: 1.1496 - acc: 0.633 - ETA: 1:33 - loss: 1.1487 - acc: 0.634 - ETA: 1:32 - loss: 1.1483 - acc: 0.634 - ETA: 1:30 - loss: 1.1478 - acc: 0.634 - ETA: 1:29 - loss: 1.1463 - acc: 0.635 - ETA: 1:28 - loss: 1.1461 - acc: 0.635 - ETA: 1:26 - loss: 1.1468 - acc: 0.634 - ETA: 1:25 - loss: 1.1454 - acc: 0.635 - ETA: 1:23 - loss: 1.1443 - acc: 0.635 - ETA: 1:22 - loss: 1.1437 - acc: 0.636 - ETA: 1:21 - loss: 1.1429 - acc: 0.636 - ETA: 1:19 - loss: 1.1437 - acc: 0.636 - ETA: 1:18 - loss: 1.1429 - acc: 0.636 - ETA: 1:17 - loss: 1.1418 - acc: 0.636 - ETA: 1:15 - loss: 1.1404 - acc: 0.637 - ETA: 1:14 - loss: 1.1406 - acc: 0.637 - ETA: 1:13 - loss: 1.1406 - acc: 0.637 - ETA: 1:11 - loss: 1.1404 - acc: 0.637 - ETA: 1:10 - loss: 1.1389 - acc: 0.637 - ETA: 1:09 - loss: 1.1385 - acc: 0.638 - ETA: 1:07 - loss: 1.1381 - acc: 0.638 - ETA: 1:06 - loss: 1.1382 - acc: 0.638 - ETA: 1:04 - loss: 1.1373 - acc: 0.638 - ETA: 1:03 - loss: 1.1362 - acc: 0.638 - ETA: 1:02 - loss: 1.1370 - acc: 0.638 - ETA: 1:00 - loss: 1.1364 - acc: 0.638 - ETA: 59s - loss: 1.1360 - acc: 0.639 - ETA: 57s - loss: 1.1356 - acc: 0.63 - ETA: 56s - loss: 1.1349 - acc: 0.63 - ETA: 55s - loss: 1.1344 - acc: 0.63 - ETA: 53s - loss: 1.1329 - acc: 0.64 - ETA: 52s - loss: 1.1324 - acc: 0.64 - ETA: 50s - loss: 1.1324 - acc: 0.64 - ETA: 49s - loss: 1.1312 - acc: 0.64 - ETA: 48s - loss: 1.1301 - acc: 0.64 - ETA: 46s - loss: 1.1306 - acc: 0.64 - ETA: 45s - loss: 1.1299 - acc: 0.64 - ETA: 44s - loss: 1.1301 - acc: 0.64 - ETA: 42s - loss: 1.1299 - acc: 0.64 - ETA: 41s - loss: 1.1294 - acc: 0.64 - ETA: 39s - loss: 1.1289 - acc: 0.64 - ETA: 38s - loss: 1.1281 - acc: 0.64 - ETA: 37s - loss: 1.1271 - acc: 0.64 - ETA: 35s - loss: 1.1269 - acc: 0.64 - ETA: 34s - loss: 1.1268 - acc: 0.64 - ETA: 33s - loss: 1.1259 - acc: 0.64 - ETA: 31s - loss: 1.1243 - acc: 0.64 - ETA: 30s - loss: 1.1245 - acc: 0.64 - ETA: 29s - loss: 1.1246 - acc: 0.64 - ETA: 27s - loss: 1.1248 - acc: 0.64 - ETA: 26s - loss: 1.1248 - acc: 0.64 - ETA: 24s - loss: 1.1245 - acc: 0.64 - ETA: 23s - loss: 1.1245 - acc: 0.64 - ETA: 22s - loss: 1.1247 - acc: 0.64 - ETA: 20s - loss: 1.1248 - acc: 0.64 - ETA: 19s - loss: 1.1244 - acc: 0.64 - ETA: 18s - loss: 1.1235 - acc: 0.64 - ETA: 16s - loss: 1.1228 - acc: 0.64 - ETA: 15s - loss: 1.1226 - acc: 0.64 - ETA: 13s - loss: 1.1224 - acc: 0.64 - ETA: 12s - loss: 1.1221 - acc: 0.64 - ETA: 11s - loss: 1.1226 - acc: 0.64 - ETA: 9s - loss: 1.1219 - acc: 0.6445 - ETA: 8s - loss: 1.1215 - acc: 0.644 - ETA: 7s - loss: 1.1215 - acc: 0.644 - ETA: 5s - loss: 1.1210 - acc: 0.644 - ETA: 4s - loss: 1.1206 - acc: 0.645 - ETA: 3s - loss: 1.1206 - acc: 0.645 - ETA: 1s - loss: 1.1208 - acc: 0.644 - ETA: 0s - loss: 1.1210 - acc: 0.644 - 266s 11ms/step - loss: 1.1210 - acc: 0.6447 - val_loss: 1.0822 - val_acc: 0.6605\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - ETA: 4:07 - loss: 1.0741 - acc: 0.656 - ETA: 4:10 - loss: 1.0352 - acc: 0.679 - ETA: 4:09 - loss: 0.9831 - acc: 0.703 - ETA: 4:08 - loss: 1.0380 - acc: 0.681 - ETA: 4:06 - loss: 1.0402 - acc: 0.676 - ETA: 4:04 - loss: 1.0430 - acc: 0.675 - ETA: 4:03 - loss: 1.0424 - acc: 0.678 - ETA: 4:01 - loss: 1.0381 - acc: 0.681 - ETA: 4:02 - loss: 1.0476 - acc: 0.676 - ETA: 3:59 - loss: 1.0575 - acc: 0.670 - ETA: 3:58 - loss: 1.0476 - acc: 0.674 - ETA: 3:56 - loss: 1.0601 - acc: 0.668 - ETA: 3:55 - loss: 1.0669 - acc: 0.664 - ETA: 3:54 - loss: 1.0642 - acc: 0.664 - ETA: 3:53 - loss: 1.0675 - acc: 0.663 - ETA: 3:52 - loss: 1.0638 - acc: 0.664 - ETA: 3:50 - loss: 1.0736 - acc: 0.660 - ETA: 3:49 - loss: 1.0699 - acc: 0.662 - ETA: 3:48 - loss: 1.0692 - acc: 0.661 - ETA: 3:46 - loss: 1.0694 - acc: 0.660 - ETA: 3:45 - loss: 1.0690 - acc: 0.660 - ETA: 3:44 - loss: 1.0674 - acc: 0.660 - ETA: 3:42 - loss: 1.0655 - acc: 0.662 - ETA: 3:40 - loss: 1.0650 - acc: 0.662 - ETA: 3:39 - loss: 1.0646 - acc: 0.663 - ETA: 3:38 - loss: 1.0640 - acc: 0.664 - ETA: 3:37 - loss: 1.0679 - acc: 0.662 - ETA: 3:35 - loss: 1.0655 - acc: 0.664 - ETA: 3:34 - loss: 1.0653 - acc: 0.665 - ETA: 3:33 - loss: 1.0686 - acc: 0.663 - ETA: 3:31 - loss: 1.0665 - acc: 0.664 - ETA: 3:30 - loss: 1.0670 - acc: 0.664 - ETA: 3:29 - loss: 1.0638 - acc: 0.666 - ETA: 3:27 - loss: 1.0655 - acc: 0.665 - ETA: 3:26 - loss: 1.0692 - acc: 0.663 - ETA: 3:25 - loss: 1.0715 - acc: 0.661 - ETA: 3:24 - loss: 1.0710 - acc: 0.662 - ETA: 3:22 - loss: 1.0741 - acc: 0.660 - ETA: 3:21 - loss: 1.0738 - acc: 0.660 - ETA: 3:20 - loss: 1.0758 - acc: 0.659 - ETA: 3:18 - loss: 1.0759 - acc: 0.658 - ETA: 3:17 - loss: 1.0763 - acc: 0.659 - ETA: 3:16 - loss: 1.0745 - acc: 0.660 - ETA: 3:14 - loss: 1.0735 - acc: 0.660 - ETA: 3:13 - loss: 1.0751 - acc: 0.660 - ETA: 3:12 - loss: 1.0749 - acc: 0.660 - ETA: 3:10 - loss: 1.0784 - acc: 0.658 - ETA: 3:09 - loss: 1.0801 - acc: 0.657 - ETA: 3:08 - loss: 1.0783 - acc: 0.658 - ETA: 3:07 - loss: 1.0780 - acc: 0.658 - ETA: 3:05 - loss: 1.0807 - acc: 0.657 - ETA: 3:04 - loss: 1.0814 - acc: 0.657 - ETA: 3:03 - loss: 1.0811 - acc: 0.657 - ETA: 3:01 - loss: 1.0810 - acc: 0.657 - ETA: 3:00 - loss: 1.0804 - acc: 0.657 - ETA: 2:59 - loss: 1.0822 - acc: 0.656 - ETA: 2:57 - loss: 1.0803 - acc: 0.658 - ETA: 2:56 - loss: 1.0794 - acc: 0.658 - ETA: 2:55 - loss: 1.0793 - acc: 0.658 - ETA: 2:54 - loss: 1.0791 - acc: 0.658 - ETA: 2:53 - loss: 1.0806 - acc: 0.657 - ETA: 2:51 - loss: 1.0806 - acc: 0.657 - ETA: 2:50 - loss: 1.0794 - acc: 0.658 - ETA: 2:49 - loss: 1.0812 - acc: 0.657 - ETA: 2:47 - loss: 1.0791 - acc: 0.658 - ETA: 2:46 - loss: 1.0797 - acc: 0.658 - ETA: 2:45 - loss: 1.0819 - acc: 0.657 - ETA: 2:44 - loss: 1.0812 - acc: 0.658 - ETA: 2:42 - loss: 1.0813 - acc: 0.658 - ETA: 2:41 - loss: 1.0824 - acc: 0.657 - ETA: 2:40 - loss: 1.0817 - acc: 0.657 - ETA: 2:38 - loss: 1.0824 - acc: 0.657 - ETA: 2:37 - loss: 1.0826 - acc: 0.657 - ETA: 2:36 - loss: 1.0818 - acc: 0.657 - ETA: 2:35 - loss: 1.0832 - acc: 0.656 - ETA: 2:33 - loss: 1.0835 - acc: 0.656 - ETA: 2:32 - loss: 1.0827 - acc: 0.656 - ETA: 2:31 - loss: 1.0822 - acc: 0.657 - ETA: 2:29 - loss: 1.0820 - acc: 0.657 - ETA: 2:28 - loss: 1.0800 - acc: 0.658 - ETA: 2:27 - loss: 1.0816 - acc: 0.657 - ETA: 2:25 - loss: 1.0827 - acc: 0.657 - ETA: 2:24 - loss: 1.0845 - acc: 0.656 - ETA: 2:23 - loss: 1.0843 - acc: 0.656 - ETA: 2:22 - loss: 1.0851 - acc: 0.655 - ETA: 2:20 - loss: 1.0851 - acc: 0.656 - ETA: 2:19 - loss: 1.0836 - acc: 0.657 - ETA: 2:18 - loss: 1.0825 - acc: 0.657 - ETA: 2:17 - loss: 1.0822 - acc: 0.657 - ETA: 2:15 - loss: 1.0828 - acc: 0.657 - ETA: 2:14 - loss: 1.0831 - acc: 0.657 - ETA: 2:13 - loss: 1.0832 - acc: 0.657 - ETA: 2:11 - loss: 1.0806 - acc: 0.658 - ETA: 2:10 - loss: 1.0812 - acc: 0.658 - ETA: 2:09 - loss: 1.0831 - acc: 0.657 - ETA: 2:07 - loss: 1.0832 - acc: 0.657 - ETA: 2:06 - loss: 1.0817 - acc: 0.658 - ETA: 2:05 - loss: 1.0817 - acc: 0.657 - ETA: 2:04 - loss: 1.0813 - acc: 0.658 - ETA: 2:02 - loss: 1.0819 - acc: 0.657 - ETA: 2:01 - loss: 1.0814 - acc: 0.658 - ETA: 2:00 - loss: 1.0821 - acc: 0.657 - ETA: 1:58 - loss: 1.0827 - acc: 0.657 - ETA: 1:57 - loss: 1.0820 - acc: 0.657 - ETA: 1:56 - loss: 1.0803 - acc: 0.658 - ETA: 1:54 - loss: 1.0801 - acc: 0.658 - ETA: 1:53 - loss: 1.0796 - acc: 0.659 - ETA: 1:52 - loss: 1.0802 - acc: 0.658 - ETA: 1:51 - loss: 1.0802 - acc: 0.658 - ETA: 1:49 - loss: 1.0793 - acc: 0.659 - ETA: 1:48 - loss: 1.0790 - acc: 0.659 - ETA: 1:47 - loss: 1.0787 - acc: 0.659 - ETA: 1:45 - loss: 1.0785 - acc: 0.659 - ETA: 1:44 - loss: 1.0794 - acc: 0.658 - ETA: 1:43 - loss: 1.0784 - acc: 0.659 - ETA: 1:42 - loss: 1.0783 - acc: 0.659 - ETA: 1:40 - loss: 1.0787 - acc: 0.659 - ETA: 1:39 - loss: 1.0792 - acc: 0.658 - ETA: 1:38 - loss: 1.0782 - acc: 0.659 - ETA: 1:36 - loss: 1.0783 - acc: 0.659 - ETA: 1:35 - loss: 1.0787 - acc: 0.659 - ETA: 1:34 - loss: 1.0789 - acc: 0.658 - ETA: 1:33 - loss: 1.0796 - acc: 0.658 - ETA: 1:31 - loss: 1.0790 - acc: 0.658 - ETA: 1:30 - loss: 1.0806 - acc: 0.658 - ETA: 1:29 - loss: 1.0817 - acc: 0.657 - ETA: 1:27 - loss: 1.0816 - acc: 0.657 - ETA: 1:26 - loss: 1.0821 - acc: 0.657 - ETA: 1:25 - loss: 1.0824 - acc: 0.657 - ETA: 1:23 - loss: 1.0814 - acc: 0.657 - ETA: 1:22 - loss: 1.0814 - acc: 0.657 - ETA: 1:21 - loss: 1.0806 - acc: 0.658 - ETA: 1:20 - loss: 1.0800 - acc: 0.658 - ETA: 1:18 - loss: 1.0801 - acc: 0.658 - ETA: 1:17 - loss: 1.0809 - acc: 0.658 - ETA: 1:16 - loss: 1.0806 - acc: 0.658 - ETA: 1:14 - loss: 1.0802 - acc: 0.658 - ETA: 1:13 - loss: 1.0805 - acc: 0.658 - ETA: 1:12 - loss: 1.0811 - acc: 0.658 - ETA: 1:11 - loss: 1.0810 - acc: 0.657 - ETA: 1:09 - loss: 1.0813 - acc: 0.657 - ETA: 1:08 - loss: 1.0807 - acc: 0.658 - ETA: 1:07 - loss: 1.0795 - acc: 0.658 - ETA: 1:05 - loss: 1.0805 - acc: 0.658 - ETA: 1:04 - loss: 1.0804 - acc: 0.658 - ETA: 1:03 - loss: 1.0798 - acc: 0.658 - ETA: 1:02 - loss: 1.0796 - acc: 0.658 - ETA: 1:00 - loss: 1.0795 - acc: 0.658 - ETA: 59s - loss: 1.0799 - acc: 0.658 - ETA: 58s - loss: 1.0802 - acc: 0.65 - ETA: 56s - loss: 1.0803 - acc: 0.65 - ETA: 55s - loss: 1.0798 - acc: 0.65 - ETA: 54s - loss: 1.0793 - acc: 0.65 - ETA: 53s - loss: 1.0793 - acc: 0.65 - ETA: 51s - loss: 1.0794 - acc: 0.65 - ETA: 50s - loss: 1.0793 - acc: 0.65 - ETA: 49s - loss: 1.0792 - acc: 0.65 - ETA: 47s - loss: 1.0792 - acc: 0.65 - ETA: 46s - loss: 1.0783 - acc: 0.65 - ETA: 45s - loss: 1.0789 - acc: 0.65 - ETA: 44s - loss: 1.0796 - acc: 0.65 - ETA: 42s - loss: 1.0798 - acc: 0.65 - ETA: 41s - loss: 1.0804 - acc: 0.65 - ETA: 40s - loss: 1.0796 - acc: 0.65 - ETA: 38s - loss: 1.0787 - acc: 0.65 - ETA: 37s - loss: 1.0783 - acc: 0.66 - ETA: 36s - loss: 1.0780 - acc: 0.66 - ETA: 35s - loss: 1.0777 - acc: 0.66 - ETA: 33s - loss: 1.0776 - acc: 0.66 - ETA: 32s - loss: 1.0780 - acc: 0.66 - ETA: 31s - loss: 1.0787 - acc: 0.66 - ETA: 29s - loss: 1.0781 - acc: 0.66 - ETA: 28s - loss: 1.0775 - acc: 0.66 - ETA: 27s - loss: 1.0777 - acc: 0.66 - ETA: 26s - loss: 1.0773 - acc: 0.66 - ETA: 24s - loss: 1.0779 - acc: 0.66 - ETA: 23s - loss: 1.0786 - acc: 0.65 - ETA: 22s - loss: 1.0798 - acc: 0.65 - ETA: 20s - loss: 1.0793 - acc: 0.65 - ETA: 19s - loss: 1.0795 - acc: 0.65 - ETA: 18s - loss: 1.0798 - acc: 0.65 - ETA: 17s - loss: 1.0802 - acc: 0.65 - ETA: 15s - loss: 1.0806 - acc: 0.65 - ETA: 14s - loss: 1.0808 - acc: 0.65 - ETA: 13s - loss: 1.0802 - acc: 0.65 - ETA: 11s - loss: 1.0805 - acc: 0.65 - ETA: 10s - loss: 1.0812 - acc: 0.65 - ETA: 9s - loss: 1.0805 - acc: 0.6590 - ETA: 8s - loss: 1.0802 - acc: 0.659 - ETA: 6s - loss: 1.0796 - acc: 0.659 - ETA: 5s - loss: 1.0801 - acc: 0.659 - ETA: 4s - loss: 1.0806 - acc: 0.659 - ETA: 2s - loss: 1.0805 - acc: 0.659 - ETA: 1s - loss: 1.0799 - acc: 0.659 - ETA: 0s - loss: 1.0795 - acc: 0.659 - 252s 10ms/step - loss: 1.0795 - acc: 0.6594 - val_loss: 1.0817 - val_acc: 0.6605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29c17d85908>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_part, y_train_part,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3, #epochs\n",
    "          callbacks=[earlyStopping],\n",
    "          validation_data=(x_test_part, y_test_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_scores = model.predict(x_test_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       777\n",
      "           1       0.00      0.00      0.00       490\n",
      "           2       0.00      0.00      0.00       906\n",
      "           3       0.00      0.00      0.00      1561\n",
      "           4       0.66      0.98      0.79      7266\n",
      "\n",
      "   micro avg       0.66      0.65      0.65     11000\n",
      "   macro avg       0.13      0.20      0.16     11000\n",
      "weighted avg       0.44      0.65      0.52     11000\n",
      " samples avg       0.65      0.65      0.65     11000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predicted_scores[y_predicted_scores>=0.5] = 1\n",
    "y_predicted_scores[y_predicted_scores<0.5] = 0\n",
    "\n",
    "print('Classification report\\n')\n",
    "print(classification_report(y_test_part, y_predicted_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучить сharacter based модель (вместо слов мы подаём предложение посимвольно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, random_state = seed, \n",
    "                                                    test_size=0.3, shuffle=True,\n",
    "                                                    stratify = y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 250\n",
    "batch_size = 128\n",
    "embedding_dims = 64\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer(\n",
    "    char_level=True,\n",
    "    filters=None,\n",
    "    lower=False,\n",
    "    num_words=5000\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, padding='post', maxlen=400)\n",
    "x_test = sequence.pad_sequences(x_test, padding='post', maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_part,y_train_part = next_batch(25000, x_train, y_train)\n",
    "x_test_part,y_test_part = next_batch(11000, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_part = np.reshape(x_train_part, (x_train_part.shape[0], 1, x_train_part.shape[1]))\n",
    "x_test_part = np.reshape(x_test_part, (x_test_part.shape[0], 1, x_test_part.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 400)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_17 (LSTM)               (None, 400, 64)           6416640   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 400, 5)            325       \n",
      "=================================================================\n",
      "Total params: 6,416,965\n",
      "Trainable params: 6,416,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(400, 25000), activation=\"tanh\", return_sequences=True)) # 400 is a average length, and 5000,second value is total number of symbols\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=2, verbose=2, mode='max',restore_best_weights=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = sgd,\n",
    "              #optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_17_input to have 3 dimensions, but got array with shape (25000, 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-970117e9c790>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearlyStopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m           validation_data=(x_test_part, y_test_part))\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_17_input to have 3 dimensions, but got array with shape (25000, 400)"
     ]
    }
   ],
   "source": [
    "model.fit(x_train_part, y_train_part,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3, #epochs\n",
    "          callbacks=[earlyStopping],\n",
    "          validation_data=(x_test_part, y_test_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_scores = model.predict(x_test_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       777\n",
      "           1       0.00      0.00      0.00       490\n",
      "           2       0.00      0.00      0.00       906\n",
      "           3       0.00      0.00      0.00      1561\n",
      "           4       0.66      0.98      0.79      7266\n",
      "\n",
      "   micro avg       0.66      0.65      0.65     11000\n",
      "   macro avg       0.13      0.20      0.16     11000\n",
      "weighted avg       0.44      0.65      0.52     11000\n",
      " samples avg       0.65      0.65      0.65     11000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predicted_scores[y_predicted_scores>=0.5] = 1\n",
    "y_predicted_scores[y_predicted_scores<0.5] = 0\n",
    "\n",
    "print('Classification report\\n')\n",
    "print(classification_report(y_test_part, y_predicted_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
