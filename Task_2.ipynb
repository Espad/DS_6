{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача - multi-label классификация - освежить память что это значит (https://scikit-learn.org/stable/modules/multiclass.html).\n",
    "\n",
    "Тартеты: 14 последних колонок - df.iloc[:,-14:]\n",
    "\n",
    "Метрика: Micro averaged F1. Хорошо бы оценивать качество с помощью кросс-валидации.\n",
    "\n",
    "- Построить baseline с использованием линейной модели из scikit-learn\n",
    "\n",
    "\n",
    "- Чистый Tensorflow\n",
    "\n",
    "Построить линейную модель для решения этой задачи. Можно взять существующий код из блокнота Tensorflow examples 1.ipynb и переделать.\n",
    "\n",
    "\n",
    "- Keras\n",
    "\n",
    "Сделать тоже самое как и в Tensorflow но использовав чистый Keras\n",
    "\n",
    "Добавить слоёв и нелинейностей, сравнить качество моделей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pandas in c:\\anaconda\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in c:\\anaconda\\lib\\site-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in c:\\anaconda\\lib\\site-packages (from pandas) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in c:\\anaconda\\lib\\site-packages (from pandas) (1.13.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in c:\\anaconda\\lib\\site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Requirement already up-to-date: scikit-learn in c:\\anaconda\\lib\\site-packages (0.20.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (1.13.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in c:\\anaconda\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already up-to-date: inflect in c:\\anaconda\\lib\\site-packages (2.1.0)\n",
      "Requirement already up-to-date: seaborn in c:\\anaconda\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14.0 in c:\\anaconda\\lib\\site-packages (from seaborn) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pandas>=0.15.2 in c:\\anaconda\\lib\\site-packages (from seaborn) (0.24.1)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib>=1.4.3 in c:\\anaconda\\lib\\site-packages (from seaborn) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.3 in c:\\anaconda\\lib\\site-packages (from seaborn) (1.13.3)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in c:\\anaconda\\lib\\site-packages (from pandas>=0.15.2->seaborn) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in c:\\anaconda\\lib\\site-packages (from pandas>=0.15.2->seaborn) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in c:\\anaconda\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in c:\\anaconda\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\anaconda\\lib\\site-packages (from matplotlib>=1.4.3->seaborn) (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in c:\\anaconda\\lib\\site-packages (from python-dateutil>=2.5.0->pandas>=0.15.2->seaborn) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\anaconda\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (40.6.3)\n",
      "Requirement already up-to-date: pprint in c:\\anaconda\\lib\\site-packages (0.1)\n",
      "Requirement already up-to-date: tensorflow in c:\\anaconda\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in c:\\anaconda\\lib\\site-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in c:\\anaconda\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in c:\\anaconda\\lib\\site-packages (from tensorflow) (0.32.3)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.16.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.0.6)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.13.0,>=1.12.0 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.12.2)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in c:\\anaconda\\lib\\site-packages (from tensorflow) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\anaconda\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (40.6.3)\n",
      "Requirement already satisfied, skipping upgrade: h5py in c:\\anaconda\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in c:\\anaconda\\lib\\site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\anaconda\\lib\\site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas\n",
    "!pip install --upgrade scikit-learn\n",
    "!pip install --upgrade inflect\n",
    "!pip install --upgrade seaborn\n",
    "!pip install --upgrade pprint\n",
    "!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade contractions\n",
    "#!pip install --upgrade wordcloud\n",
    "#!pip install --upgrade umap-learn\n",
    "#!pip install --upgrade gensim\n",
    "#!pip install --upgrade pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "import inflect\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from itertools import compress\n",
    "from collections import Counter\n",
    "#import glob\n",
    "#import contractions\n",
    "#from wordcloud import WordCloud\n",
    "#import umap\n",
    "#import unicodedata\n",
    "#from bs4 import BeautifulSoup\n",
    "\n",
    "#import time\n",
    "\n",
    "#from scipy import interp\n",
    "\n",
    "import pickle\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "seed = 321\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn libs\n",
    "from sklearn.preprocessing import label_binarize,MultiLabelBinarizer,StandardScaler\n",
    "from sklearn.pipeline import make_pipeline,Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.cluster import KMeans,DBSCAN\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split \n",
    "from sklearn.model_selection import KFold,RandomizedSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report,f1_score, make_scorer\n",
    "from sklearn.metrics import homogeneity_score,completeness_score, v_measure_score\n",
    "from sklearn.metrics import adjusted_rand_score,adjusted_mutual_info_score,silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import json\n",
    "import operator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.metrics import binary_accuracy,categorical_accuracy\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\chief\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('names')\n",
    "\n",
    "from nltk.corpus import names\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer # one more stemmer? or lemmatisation\n",
    "from nltk.stem import WordNetLemmatizer,LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read first ten row to understand, how to parse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = pickle.load(open('data/task_2/reviews_dataset.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "769251"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "769251"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Потрясающе красивая графика космоса! Уже за это игру можно полюбить. Так же в наличии интересный осмысленный сюжет и удобное управление.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline.\n",
    "\n",
    "Обучить линейную модель на TF-IDF представлении для предсказания оценки пользователя по его отзыву. \n",
    "\n",
    "Посмотреть на важность для каждой оценки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emails(text):\n",
    "    text = re.sub(r'\\S*@\\S*\\s?','',text)    \n",
    "    return text\n",
    "\n",
    "\n",
    "#version on raw text\n",
    "def remove_punctuation2(text_punctuation,text):\n",
    "    \n",
    "    new_text = []\n",
    "    new_text = re.sub('\\n', ' ',text)\n",
    "    new_text = re.sub('\\t', '',new_text)\n",
    "    new_text = re.sub('['+text_punctuation+']', ' ',new_text)\n",
    "    new_text =  new_text.strip()\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "#version after word_tokenizer\n",
    "def remove_punctuation(text_punctuation,text):\n",
    "    \n",
    "    new_text = []\n",
    "    for word in text: \n",
    "        new_word =  re.sub('['+text_punctuation+']', ' ',word)\n",
    "        new_word =  new_word.strip()\n",
    "        new_text.append(new_word)\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "\n",
    "#ntlk word lemmatizer\n",
    "def lemmatize_stemm_text(text):\n",
    "    new_text_lemma = []\n",
    "    new_text_stemm = []\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = LancasterStemmer()\n",
    "    \n",
    "    for word in text:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='n') # v default\n",
    "        new_text_lemma.append(lemma)\n",
    "        \n",
    "        #stemm = stemmer.stem(word)\n",
    "        #new_text_stemm.append(stemm)\n",
    "        \n",
    "    return new_text_lemma, new_text_stemm\n",
    "        \n",
    "\n",
    "StopWords = list(set(stopwords.words('russian')))\n",
    "'''\n",
    "StopWords = list(set( stopwords.words('english') ).union( set(ENGLISH_STOP_WORDS)))\n",
    "newStopWords = ['jfc','jb'] # по ходу работы периодически добавляем стоп-слова\n",
    "newStopWords2 = ['arent', 'didnt', 'doesnt', 'dont', 'hadnt', 'havent', 'isnt', \n",
    "                 'mightnt', 'mustnt', 'neednt', 'shant', 'shes', 'shouldnt', 'shouldve', 'theres','thatll', 'wasnt', \n",
    "                 'werent', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve',\n",
    "                 \"'ll\", \"'re\", \"'ve\", \"n't\", 'need', 'sha', 'anna','n t','ann mari', 'ann marie', \n",
    "                 'anna diana', 'anna diane', 'anna maria', 'anne corinne', 'anne mar', 'anne marie', \n",
    "                 'barbara anne', 'bette ann', 'carol jean', 'diane marie', 'e lane', 'hans peter', \n",
    "                 'helen elizabeth', 'holly anne', 'jean christophe', 'jean francois', 'jean lou', \n",
    "                 'jean luc', 'jean marc', 'jean paul', 'jean pierre', 'jo anne', 'john david', 'john patrick', \n",
    "                 'kara lynn', 'marie ann', 'marie jeanne', 'paula grace', 'sara ann', \n",
    "                 'sheila kathryn', 'sue elle', 'terri jo', 'theresa marie','sza']\n",
    "\n",
    "#StopWords.extend(newStopWords)\n",
    "#StopWords.extend(newStopWords2)\n",
    "\n",
    "\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "\n",
    "Common_First_Names = list(set(male_names).union(set(female_names)))\n",
    "Common_First_Names = list(map(lambda word: word.lower(), Common_First_Names))\n",
    "# temporaly disabled to reduce tfidf time\n",
    "#StopWords.extend(Common_First_Names) ''';\n",
    "\n",
    "def remove_stopwords(stop_words ,text):\n",
    "    \n",
    "    new_words = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            new_words.append(word)\n",
    "\n",
    "    return new_words\n",
    "\n",
    "def remove_short_words(text, word_len):\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if len(word) >= word_len:\n",
    "            new_text.append(word)\n",
    "            \n",
    "    return new_text\n",
    "\n",
    "\n",
    "# getting source from string.punctuation\n",
    "text_punctuation = '!\"#$%&\\'()*+,-.:;<=>?@[\\\\]_`{|}~/^'\n",
    "\n",
    "def tokenize(text):\n",
    "    min_length = 3\n",
    "    \n",
    "    # remove emails from text to prevent overfit\n",
    "    text = remove_emails(text)\n",
    "    \n",
    "    # text to lowercase\n",
    "    text =  text.lower()\n",
    "\n",
    "    \n",
    "    #remove punctuation\n",
    "    text = remove_punctuation2(text_punctuation, text) \n",
    "\n",
    "    # tokenize text\n",
    "    words = word_tokenize(text,language='english')\n",
    "\n",
    "       \n",
    "    # remove stopwords\n",
    "    words = remove_stopwords(StopWords,words)\n",
    "    \n",
    "    #lemmatize words ,improve to 0.69 f1score\n",
    "    words,_ = lemmatize_stemm_text(words)\n",
    "    \n",
    "    #filter short words\n",
    "    words = remove_short_words(words, 2) #default 3\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = tokenize(x_data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['таки',\n",
       " 'удержался',\n",
       " 'написать',\n",
       " 'топ',\n",
       " 'скорей',\n",
       " 'сотрут',\n",
       " 'прочтет',\n",
       " 'ужасной',\n",
       " 'игры',\n",
       " 'star',\n",
       " 'war',\n",
       " 'empire',\n",
       " 'at',\n",
       " 'war',\n",
       " 'dvd',\n",
       " 'ещё',\n",
       " 'поискать',\n",
       " 'нормального',\n",
       " 'сюжета',\n",
       " 'стратегия',\n",
       " 'это',\n",
       " 'вовсе',\n",
       " 'провал',\n",
       " 'давно',\n",
       " 'убедился',\n",
       " 'всё',\n",
       " 'берётся',\n",
       " '1с',\n",
       " 'плане',\n",
       " 'игр',\n",
       " 'отстой',\n",
       " 'хорошая',\n",
       " 'стратегия',\n",
       " 'эпизодам',\n",
       " 'star',\n",
       " 'war',\n",
       " 'это',\n",
       " 'star',\n",
       " 'war',\n",
       " 'galactic',\n",
       " 'battleground',\n",
       " 'играл',\n",
       " 'поймет']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, random_state = seed, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=StopWords,\n",
    "                             tokenizer=tokenize, \n",
    "                             min_df=4, # ignore terms with freq less that т, lower majoring 4\n",
    "                             max_df=0.8, # ignore terms with freq more that n, upper majoring 0.8\n",
    "                             #max_features=100000,\n",
    "                             use_idf=True,  \n",
    "                             sublinear_tf=True,\n",
    "                             norm='l2',\n",
    "                             #ngram_range= (1, 3) # также используем н-граммы\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vectorised_train_documents = vectorizer.fit_transform(x_train)\n",
    "vectorised_test_documents = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(538475, 143312)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorised_train_documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230776, 143312)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorised_test_documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_score(classifier, X_train, X_test, y_train, y_test):\n",
    "    clf = classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    pred_train = clf.predict(X_train)\n",
    "    #print (\"Train score\")\n",
    "    f1_train = f1_score(y_train, pred_train, average='weighted')\n",
    "\n",
    "    #print (\"Test score\")\n",
    "    pred_test = clf.predict(X_test)\n",
    "    f1_test = f1_score(y_test, pred_test, average='weighted')\n",
    "    \n",
    "    return f1_train, f1_test, clf\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Paired):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"black\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty='l2', tol=0.0001, C=1, \n",
    "                             class_weight='balanced', random_state=seed, \n",
    "                             max_iter=1000, solver = 'newton-cg',\n",
    "                             multi_class='ovr', verbose=0, \n",
    "                             n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_,_, clf = classify_and_score(log_reg, vectorised_train_documents, \n",
    "                           vectorised_test_documents, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.57      0.48     16683\n",
      "           2       0.16      0.22      0.19      9679\n",
      "           3       0.24      0.34      0.28     17653\n",
      "           4       0.34      0.41      0.37     33086\n",
      "           5       0.89      0.76      0.82    153675\n",
      "\n",
      "   micro avg       0.64      0.64      0.64    230776\n",
      "   macro avg       0.41      0.46      0.43    230776\n",
      "weighted avg       0.70      0.64      0.66    230776\n",
      "\n",
      "Confusion matrix, without normalization\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = clf.classes_\n",
    "pred_test = clf.predict(vectorised_test_documents)\n",
    "print(classification_report(y_test,pred_test,labels = labels))\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, pred_test)\n",
    "print(plot_confusion_matrix(cnf_matrix, classes = labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучить рекуррентную нейронную сеть для предсказания оценки пользователя по его отзыву. Использовать случайную инициализация весов.\n",
    "\n",
    "- add stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, random_state = seed, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert words to sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,GlobalAveragePooling1D\n",
    "from keras.datasets import imdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "encoded_y_test = encoder.transform(y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "y_train = np_utils.to_categorical(encoded_y_train)\n",
    "y_test = np_utils.to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = np.array(y_train)\n",
    "#y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485477\n",
      "[851, 7, 12, 7, 3059, 3303, 4643, 618, 7, 44, 1, 12, 202, 257, 207, 2474, 466, 4, 4996, 1332, 1807, 51, 1203, 92, 1258, 3, 2526, 504, 1, 3278, 2034, 11, 2968, 629, 1336, 581, 52, 3, 546, 1109, 2282, 308, 108, 23, 2842, 144, 1202, 11, 925, 2968, 898, 52, 2431, 1758, 33, 3394, 546, 1109, 1686, 2, 4355, 1, 2, 3, 33, 3, 4441, 1075, 1, 365, 7, 1, 2034, 15, 2724, 11, 28, 20, 3633, 39, 1, 548, 13, 80, 4, 3817, 11, 1967, 1093, 3690, 19, 6, 2560, 1, 2, 1696, 828, 1, 39, 5, 9, 4132, 4398, 3, 7, 3059, 1786, 7, 3059, 2, 4, 3784, 102, 3817, 665, 1525, 3729, 6, 33, 14, 1708, 7, 3059, 879, 4643, 7, 2726, 10, 54, 15, 283, 558, 3611]\n",
      "133\n",
      "538475\n"
     ]
    }
   ],
   "source": [
    "print (vocab_size)\n",
    "\n",
    "print(x_train[0])\n",
    "\n",
    "print(len(x_train[0]))\n",
    "\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  23 2842  144 1202   11  925 2968  898   52 2431 1758   33 3394  546 1109\n",
      " 1686    2 4355    1    2    3   33    3 4441 1075    1  365    7    1 2034\n",
      "   15 2724   11   28   20 3633   39    1  548   13   80    4 3817   11 1967\n",
      " 1093 3690   19    6 2560    1    2 1696  828    1   39    5    9 4132 4398\n",
      "    3    7 3059 1786    7 3059    2    4 3784  102 3817  665 1525 3729    6\n",
      "   33   14 1708    7 3059  879 4643    7 2726   10   54   15  283  558 3611]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "maxlen = 90\n",
    "batch_size = 128\n",
    "max_features = 20000\n",
    "\n",
    "'''\n",
    "vocab_size\n",
    "embedding_dim = 50\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "                           \n",
    "                           \n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))   \n",
    "\n",
    "\n",
    "input_dim: the size of the vocabulary\n",
    "output_dim: the size of the dense vector\n",
    "input_length: the length of the sequence\n",
    "                           ''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 90, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,659,461\n",
      "Trainable params: 2,659,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_features, output_dim= 128, \n",
    "                    input_length=maxlen, \n",
    "                    embeddings_initializer = 'RandomNormal'))#weights!\n",
    "model.add(Bidirectional(LSTM(64)))# weights!\n",
    "model.add(Dropout(0.1))\n",
    "#model.add(Dropout(0.5)) #1\n",
    "model.add(Dense(5, activation='softmax')) #softmax due to multiclass\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=2, verbose=2, mode='max',restore_best_weights=True)\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = data[idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_part,y_train_part = next_batch(50000, x_train, y_train)\n",
    "x_test_part,y_test_part = next_batch(16000, x_test, y_test)\n",
    "#y_train_part = y_train\n",
    "\n",
    "#x_test_part = x_test\n",
    "#y_test_part = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 50000 samples, validate on 16000 samples\n",
      "Epoch 1/4\n",
      " - 211s - loss: 0.9431 - acc: 0.6700 - val_loss: 0.8425 - val_acc: 0.6909\n",
      "Epoch 2/4\n",
      " - 213s - loss: 0.7835 - acc: 0.7082 - val_loss: 0.8160 - val_acc: 0.7054\n",
      "Epoch 3/4\n",
      " - 215s - loss: 0.7266 - acc: 0.7260 - val_loss: 0.8172 - val_acc: 0.6991\n",
      "Epoch 4/4\n",
      " - 200s - loss: 0.6871 - acc: 0.7397 - val_loss: 0.8345 - val_acc: 0.7001\n",
      "Wall time: 14min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Train...')\n",
    "model.fit(x_train_part, y_train_part,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          verbose = 2,\n",
    "          callbacks=[earlyStopping],\n",
    "          validation_data=[x_test_part, y_test_part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.21      0.32      1197\n",
      "           1       0.00      0.00      0.00       629\n",
      "           2       0.37      0.02      0.04      1206\n",
      "           3       0.44      0.08      0.14      2264\n",
      "           4       0.82      0.88      0.85     10704\n",
      "\n",
      "   micro avg       0.80      0.62      0.70     16000\n",
      "   macro avg       0.45      0.24      0.27     16000\n",
      "weighted avg       0.68      0.62      0.61     16000\n",
      " samples avg       0.62      0.62      0.62     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predicted_scores = model.predict(x_test_part)\n",
    "y_predicted_scores[y_predicted_scores>=0.5] = 1\n",
    "y_predicted_scores[y_predicted_scores<0.5] = 0\n",
    "\n",
    "print('Classification report\\n')\n",
    "print(classification_report(y_test_part, y_predicted_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "          validation_data=[x_test, y_test])\n",
    "\n",
    "y_predicted_scores = model.predict(x_test)\n",
    "y_predicted_scores[y_predicted_scores>=0.5] = 1\n",
    "y_predicted_scores[y_predicted_scores<0.5] = 0\n",
    "\n",
    "print('Classification report\\n')\n",
    "print(classification_report(y_test, y_predicted_scores))''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучить «fasttext» модель на n-граммах используя случайную инициализацию эмбедингов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 2\n",
    "max_features = 20000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 2-gram features\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-857bee56c2fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Augmenting x_train and x_test with n-grams features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_ngram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_indice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_ngram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_indice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     print('Average train sequence length: {}'.format(\n",
      "\u001b[1;32m<ipython-input-73-929cd4bc42f0>\u001b[0m in \u001b[0;36madd_ngram\u001b[1;34m(sequences, token_indice, ngram_range)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mngram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mngram_value\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mngram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoken_indice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                     \u001b[0mnew_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_indice\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mnew_sequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "'''print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(\n",
    "    np.mean(list(map(len, x_test)), dtype=int)))'''\n",
    "\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_part,y_train_part = next_batch(50000, x_train, y_train)\n",
    "x_test_part,y_test_part = next_batch(16000, x_test, y_test)\n",
    "#y_train_part = y_train\n",
    "\n",
    "#x_test_part = x_test\n",
    "#y_test_part = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,   10,    1, 2523, 3231,    2,   22, 2789, 3260,    1,\n",
       "       1073,    3,  216,    1,    8,    1, 1778,  640,   11,  207,  528,\n",
       "       1889, 1864, 2762,    5, 2111,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_part[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (538475, 400)\n",
      "x_test shape: (230776, 400)\n",
      "Build model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_9 to have shape (1,) but got array with shape (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-e77191ea9b47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m           validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_9 to have shape (1,) but got array with shape (5,)"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "# we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "# of all words in the document\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''Trains a Bidirectional LSTM on the IMDB sentiment classification task.\n",
    "Output after 4 epochs on CPU: ~0.8146\n",
    "Time per epoch on CPU (Core i7): ~150s.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "batch_size = 32\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1415,    33,     6,    22,    12,   215,    28,    77,    52,\n",
       "           5,    14,   407,    16,    82, 10311,     8,     4,   107,\n",
       "         117,  5952,    15,   256,     4,     2,     7,  3766,     5,\n",
       "         723,    36,    71,    43,   530,   476,    26,   400,   317,\n",
       "          46,     7,     4, 12118,  1029,    13,   104,    88,     4,\n",
       "         381,    15,   297,    98,    32,  2071,    56,    26,   141,\n",
       "           6,   194,  7486,    18,     4,   226,    22,    21,   134,\n",
       "         476,    26,   480,     5,   144,    30,  5535,    18,    51,\n",
       "          36,    28,   224,    92,    25,   104,     4,   226,    65,\n",
       "          16,    38,  1334,    88,    12,    16,   283,     5,    16,\n",
       "        4472,   113,   103,    32,    15,    16,  5345,    19,   178,    32])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1415,    33,     6,    22,    12,   215,    28,    77,    52,\n",
       "           5,    14,   407,    16,    82, 10311,     8,     4,   107,\n",
       "         117,  5952,    15,   256,     4,     2,     7,  3766,     5,\n",
       "         723,    36,    71,    43,   530,   476,    26,   400,   317,\n",
       "          46,     7,     4, 12118,  1029,    13,   104,    88,     4,\n",
       "         381,    15,   297,    98,    32,  2071,    56,    26,   141,\n",
       "           6,   194,  7486,    18,     4,   226,    22,    21,   134,\n",
       "         476,    26,   480,     5,   144,    30,  5535,    18,    51,\n",
       "          36,    28,   224,    92,    25,   104,     4,   226,    65,\n",
       "          16,    38,  1334,    88,    12,    16,   283,     5,    16,\n",
       "        4472,   113,   103,    32,    15,    16,  5345,    19,   178,    32])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - ETA: 4: - ETA: 1: - ETA: 59s - ETA: 34 - ETA: 28 - ETA: 16 - ETA: 16 - ETA: 11 - ETA: 10 - ETA: 8 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 4s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6528/25000 [======>.......................] - ETA: 24:53 - loss: 0.6929 - acc: 0.50 - ETA: 13:26 - loss: 0.6941 - acc: 0.43 - ETA: 9:41 - loss: 0.6937 - acc: 0.4688 - ETA: 7:48 - loss: 0.6931 - acc: 0.507 - ETA: 6:39 - loss: 0.6929 - acc: 0.512 - ETA: 5:56 - loss: 0.6923 - acc: 0.531 - ETA: 5:26 - loss: 0.6919 - acc: 0.544 - ETA: 5:06 - loss: 0.6921 - acc: 0.550 - ETA: 4:48 - loss: 0.6927 - acc: 0.534 - ETA: 4:45 - loss: 0.6924 - acc: 0.537 - ETA: 4:33 - loss: 0.6929 - acc: 0.519 - ETA: 4:25 - loss: 0.6928 - acc: 0.520 - ETA: 4:16 - loss: 0.6925 - acc: 0.528 - ETA: 4:09 - loss: 0.6922 - acc: 0.535 - ETA: 4:02 - loss: 0.6931 - acc: 0.520 - ETA: 3:58 - loss: 0.6932 - acc: 0.511 - ETA: 3:53 - loss: 0.6937 - acc: 0.500 - ETA: 3:48 - loss: 0.6940 - acc: 0.498 - ETA: 3:43 - loss: 0.6938 - acc: 0.498 - ETA: 3:39 - loss: 0.6937 - acc: 0.503 - ETA: 3:35 - loss: 0.6936 - acc: 0.504 - ETA: 3:32 - loss: 0.6934 - acc: 0.511 - ETA: 3:28 - loss: 0.6933 - acc: 0.513 - ETA: 3:24 - loss: 0.6932 - acc: 0.516 - ETA: 3:21 - loss: 0.6930 - acc: 0.522 - ETA: 3:17 - loss: 0.6928 - acc: 0.526 - ETA: 3:14 - loss: 0.6924 - acc: 0.533 - ETA: 3:11 - loss: 0.6923 - acc: 0.536 - ETA: 3:08 - loss: 0.6923 - acc: 0.537 - ETA: 3:06 - loss: 0.6922 - acc: 0.540 - ETA: 3:03 - loss: 0.6921 - acc: 0.543 - ETA: 3:01 - loss: 0.6919 - acc: 0.548 - ETA: 2:59 - loss: 0.6919 - acc: 0.548 - ETA: 2:56 - loss: 0.6917 - acc: 0.551 - ETA: 2:55 - loss: 0.6917 - acc: 0.552 - ETA: 2:53 - loss: 0.6917 - acc: 0.548 - ETA: 2:51 - loss: 0.6916 - acc: 0.552 - ETA: 2:49 - loss: 0.6916 - acc: 0.551 - ETA: 2:48 - loss: 0.6915 - acc: 0.554 - ETA: 2:48 - loss: 0.6914 - acc: 0.554 - ETA: 2:48 - loss: 0.6912 - acc: 0.557 - ETA: 2:48 - loss: 0.6910 - acc: 0.562 - ETA: 2:46 - loss: 0.6907 - acc: 0.564 - ETA: 2:45 - loss: 0.6906 - acc: 0.563 - ETA: 2:44 - loss: 0.6904 - acc: 0.564 - ETA: 2:43 - loss: 0.6900 - acc: 0.567 - ETA: 2:42 - loss: 0.6898 - acc: 0.571 - ETA: 2:41 - loss: 0.6892 - acc: 0.571 - ETA: 2:40 - loss: 0.6892 - acc: 0.572 - ETA: 2:39 - loss: 0.6889 - acc: 0.570 - ETA: 2:38 - loss: 0.6889 - acc: 0.570 - ETA: 2:37 - loss: 0.6882 - acc: 0.572 - ETA: 2:36 - loss: 0.6877 - acc: 0.574 - ETA: 2:35 - loss: 0.6864 - acc: 0.577 - ETA: 2:34 - loss: 0.6837 - acc: 0.581 - ETA: 2:33 - loss: 0.6790 - acc: 0.587 - ETA: 2:32 - loss: 0.6798 - acc: 0.589 - ETA: 2:32 - loss: 0.6774 - acc: 0.592 - ETA: 2:31 - loss: 0.6772 - acc: 0.594 - ETA: 2:30 - loss: 0.6773 - acc: 0.594 - ETA: 2:29 - loss: 0.6789 - acc: 0.594 - ETA: 2:28 - loss: 0.6787 - acc: 0.593 - ETA: 2:27 - loss: 0.6770 - acc: 0.595 - ETA: 2:26 - loss: 0.6755 - acc: 0.597 - ETA: 2:26 - loss: 0.6761 - acc: 0.595 - ETA: 2:25 - loss: 0.6746 - acc: 0.598 - ETA: 2:24 - loss: 0.6739 - acc: 0.598 - ETA: 2:23 - loss: 0.6736 - acc: 0.597 - ETA: 2:23 - loss: 0.6727 - acc: 0.599 - ETA: 2:22 - loss: 0.6719 - acc: 0.601 - ETA: 2:21 - loss: 0.6718 - acc: 0.601 - ETA: 2:21 - loss: 0.6708 - acc: 0.604 - ETA: 2:21 - loss: 0.6696 - acc: 0.607 - ETA: 2:20 - loss: 0.6688 - acc: 0.608 - ETA: 2:19 - loss: 0.6679 - acc: 0.610 - ETA: 2:19 - loss: 0.6673 - acc: 0.611 - ETA: 2:18 - loss: 0.6664 - acc: 0.613 - ETA: 2:18 - loss: 0.6649 - acc: 0.616 - ETA: 2:17 - loss: 0.6645 - acc: 0.616 - ETA: 2:17 - loss: 0.6638 - acc: 0.618 - ETA: 2:16 - loss: 0.6619 - acc: 0.621 - ETA: 2:15 - loss: 0.6599 - acc: 0.624 - ETA: 2:15 - loss: 0.6592 - acc: 0.625 - ETA: 2:15 - loss: 0.6577 - acc: 0.626 - ETA: 2:14 - loss: 0.6568 - acc: 0.628 - ETA: 2:14 - loss: 0.6553 - acc: 0.629 - ETA: 2:13 - loss: 0.6542 - acc: 0.630 - ETA: 2:13 - loss: 0.6529 - acc: 0.632 - ETA: 2:12 - loss: 0.6515 - acc: 0.633 - ETA: 2:12 - loss: 0.6513 - acc: 0.633 - ETA: 2:11 - loss: 0.6510 - acc: 0.633 - ETA: 2:11 - loss: 0.6495 - acc: 0.634 - ETA: 2:11 - loss: 0.6491 - acc: 0.635 - ETA: 2:10 - loss: 0.6479 - acc: 0.636 - ETA: 2:10 - loss: 0.6468 - acc: 0.638 - ETA: 2:09 - loss: 0.6457 - acc: 0.640 - ETA: 2:09 - loss: 0.6442 - acc: 0.641 - ETA: 2:08 - loss: 0.6428 - acc: 0.643 - ETA: 2:08 - loss: 0.6417 - acc: 0.644 - ETA: 2:08 - loss: 0.6411 - acc: 0.645 - ETA: 2:07 - loss: 0.6401 - acc: 0.645 - ETA: 2:07 - loss: 0.6388 - acc: 0.646 - ETA: 2:06 - loss: 0.6377 - acc: 0.647 - ETA: 2:06 - loss: 0.6361 - acc: 0.648 - ETA: 2:06 - loss: 0.6343 - acc: 0.649 - ETA: 2:05 - loss: 0.6335 - acc: 0.650 - ETA: 2:05 - loss: 0.6322 - acc: 0.651 - ETA: 2:05 - loss: 0.6306 - acc: 0.652 - ETA: 2:04 - loss: 0.6310 - acc: 0.651 - ETA: 2:04 - loss: 0.6302 - acc: 0.652 - ETA: 2:04 - loss: 0.6289 - acc: 0.654 - ETA: 2:03 - loss: 0.6278 - acc: 0.654 - ETA: 2:03 - loss: 0.6257 - acc: 0.656 - ETA: 2:03 - loss: 0.6249 - acc: 0.657 - ETA: 2:02 - loss: 0.6236 - acc: 0.659 - ETA: 2:02 - loss: 0.6220 - acc: 0.660 - ETA: 2:02 - loss: 0.6214 - acc: 0.660 - ETA: 2:01 - loss: 0.6196 - acc: 0.662 - ETA: 2:01 - loss: 0.6190 - acc: 0.662 - ETA: 2:01 - loss: 0.6176 - acc: 0.664 - ETA: 2:00 - loss: 0.6160 - acc: 0.665 - ETA: 2:00 - loss: 0.6150 - acc: 0.667 - ETA: 2:00 - loss: 0.6143 - acc: 0.667 - ETA: 1:59 - loss: 0.6124 - acc: 0.668 - ETA: 1:59 - loss: 0.6112 - acc: 0.670 - ETA: 1:59 - loss: 0.6097 - acc: 0.671 - ETA: 1:58 - loss: 0.6090 - acc: 0.671 - ETA: 1:58 - loss: 0.6077 - acc: 0.672 - ETA: 1:58 - loss: 0.6065 - acc: 0.673 - ETA: 1:57 - loss: 0.6050 - acc: 0.674 - ETA: 1:57 - loss: 0.6032 - acc: 0.675 - ETA: 1:57 - loss: 0.6026 - acc: 0.675 - ETA: 1:57 - loss: 0.6021 - acc: 0.676 - ETA: 1:56 - loss: 0.6007 - acc: 0.676 - ETA: 1:56 - loss: 0.5992 - acc: 0.677 - ETA: 1:56 - loss: 0.5993 - acc: 0.677 - ETA: 1:55 - loss: 0.5985 - acc: 0.677 - ETA: 1:55 - loss: 0.5984 - acc: 0.677 - ETA: 1:55 - loss: 0.5981 - acc: 0.678 - ETA: 1:55 - loss: 0.5966 - acc: 0.679 - ETA: 1:54 - loss: 0.5962 - acc: 0.679 - ETA: 1:54 - loss: 0.5959 - acc: 0.680 - ETA: 1:54 - loss: 0.5958 - acc: 0.680 - ETA: 1:53 - loss: 0.5949 - acc: 0.681 - ETA: 1:53 - loss: 0.5942 - acc: 0.682 - ETA: 1:53 - loss: 0.5930 - acc: 0.683 - ETA: 1:53 - loss: 0.5916 - acc: 0.683 - ETA: 1:52 - loss: 0.5904 - acc: 0.684 - ETA: 1:52 - loss: 0.5905 - acc: 0.684 - ETA: 1:52 - loss: 0.5887 - acc: 0.685 - ETA: 1:51 - loss: 0.5880 - acc: 0.686 - ETA: 1:51 - loss: 0.5874 - acc: 0.686 - ETA: 1:51 - loss: 0.5884 - acc: 0.686 - ETA: 1:51 - loss: 0.5879 - acc: 0.686 - ETA: 1:50 - loss: 0.5866 - acc: 0.687 - ETA: 1:50 - loss: 0.5859 - acc: 0.688 - ETA: 1:50 - loss: 0.5855 - acc: 0.688 - ETA: 1:50 - loss: 0.5852 - acc: 0.689 - ETA: 1:49 - loss: 0.5838 - acc: 0.690 - ETA: 1:49 - loss: 0.5827 - acc: 0.691 - ETA: 1:49 - loss: 0.5818 - acc: 0.692 - ETA: 1:48 - loss: 0.5816 - acc: 0.692 - ETA: 1:48 - loss: 0.5801 - acc: 0.694 - ETA: 1:48 - loss: 0.5785 - acc: 0.695 - ETA: 1:48 - loss: 0.5778 - acc: 0.695 - ETA: 1:47 - loss: 0.5777 - acc: 0.695 - ETA: 1:47 - loss: 0.5762 - acc: 0.696 - ETA: 1:47 - loss: 0.5747 - acc: 0.697 - ETA: 1:47 - loss: 0.5740 - acc: 0.698 - ETA: 1:46 - loss: 0.5735 - acc: 0.698 - ETA: 1:46 - loss: 0.5727 - acc: 0.699 - ETA: 1:46 - loss: 0.5711 - acc: 0.700 - ETA: 1:46 - loss: 0.5699 - acc: 0.701 - ETA: 1:46 - loss: 0.5693 - acc: 0.701 - ETA: 1:45 - loss: 0.5685 - acc: 0.702 - ETA: 1:45 - loss: 0.5671 - acc: 0.702 - ETA: 1:45 - loss: 0.5665 - acc: 0.703 - ETA: 1:45 - loss: 0.5664 - acc: 0.703 - ETA: 1:44 - loss: 0.5650 - acc: 0.704 - ETA: 1:44 - loss: 0.5632 - acc: 0.705 - ETA: 1:44 - loss: 0.5623 - acc: 0.706 - ETA: 1:44 - loss: 0.5619 - acc: 0.706 - ETA: 1:44 - loss: 0.5610 - acc: 0.707 - ETA: 1:43 - loss: 0.5602 - acc: 0.708 - ETA: 1:43 - loss: 0.5594 - acc: 0.709 - ETA: 1:43 - loss: 0.5589 - acc: 0.709 - ETA: 1:43 - loss: 0.5579 - acc: 0.710 - ETA: 1:42 - loss: 0.5571 - acc: 0.710 - ETA: 1:42 - loss: 0.5568 - acc: 0.711 - ETA: 1:42 - loss: 0.5555 - acc: 0.712 - ETA: 1:42 - loss: 0.5556 - acc: 0.712 - ETA: 1:41 - loss: 0.5538 - acc: 0.713 - ETA: 1:41 - loss: 0.5533 - acc: 0.714 - ETA: 1:41 - loss: 0.5536 - acc: 0.713 - ETA: 1:41 - loss: 0.5536 - acc: 0.714 - ETA: 1:41 - loss: 0.5527 - acc: 0.714 - ETA: 1:40 - loss: 0.5518 - acc: 0.715 - ETA: 1:40 - loss: 0.5508 - acc: 0.716 - ETA: 1:40 - loss: 0.5499 - acc: 0.717 - ETA: 1:40 - loss: 0.5490 - acc: 0.718 - ETA: 1:40 - loss: 0.5480 - acc: 0.718 - ETA: 1:39 - loss: 0.5478 - acc: 0.718 - ETA: 1:39 - loss: 0.5469 - acc: 0.719 - ETA: 1:39 - loss: 0.5461 - acc: 0.720013056/25000 [==============>...............] - ETA: 1:39 - loss: 0.5462 - acc: 0.719 - ETA: 1:39 - loss: 0.5452 - acc: 0.720 - ETA: 1:38 - loss: 0.5442 - acc: 0.721 - ETA: 1:38 - loss: 0.5436 - acc: 0.721 - ETA: 1:38 - loss: 0.5421 - acc: 0.722 - ETA: 1:38 - loss: 0.5418 - acc: 0.722 - ETA: 1:38 - loss: 0.5405 - acc: 0.723 - ETA: 1:37 - loss: 0.5393 - acc: 0.723 - ETA: 1:37 - loss: 0.5391 - acc: 0.724 - ETA: 1:37 - loss: 0.5382 - acc: 0.724 - ETA: 1:37 - loss: 0.5377 - acc: 0.725 - ETA: 1:37 - loss: 0.5365 - acc: 0.725 - ETA: 1:36 - loss: 0.5359 - acc: 0.726 - ETA: 1:36 - loss: 0.5348 - acc: 0.727 - ETA: 1:36 - loss: 0.5335 - acc: 0.727 - ETA: 1:36 - loss: 0.5339 - acc: 0.727 - ETA: 1:36 - loss: 0.5334 - acc: 0.728 - ETA: 1:35 - loss: 0.5328 - acc: 0.729 - ETA: 1:35 - loss: 0.5320 - acc: 0.729 - ETA: 1:35 - loss: 0.5309 - acc: 0.730 - ETA: 1:35 - loss: 0.5304 - acc: 0.731 - ETA: 1:35 - loss: 0.5304 - acc: 0.731 - ETA: 1:34 - loss: 0.5300 - acc: 0.731 - ETA: 1:34 - loss: 0.5297 - acc: 0.731 - ETA: 1:34 - loss: 0.5293 - acc: 0.732 - ETA: 1:34 - loss: 0.5286 - acc: 0.732 - ETA: 1:34 - loss: 0.5280 - acc: 0.733 - ETA: 1:33 - loss: 0.5280 - acc: 0.733 - ETA: 1:33 - loss: 0.5274 - acc: 0.733 - ETA: 1:33 - loss: 0.5259 - acc: 0.734 - ETA: 1:33 - loss: 0.5253 - acc: 0.734 - ETA: 1:32 - loss: 0.5249 - acc: 0.735 - ETA: 1:32 - loss: 0.5244 - acc: 0.735 - ETA: 1:32 - loss: 0.5244 - acc: 0.735 - ETA: 1:32 - loss: 0.5241 - acc: 0.735 - ETA: 1:32 - loss: 0.5235 - acc: 0.735 - ETA: 1:32 - loss: 0.5242 - acc: 0.735 - ETA: 1:31 - loss: 0.5230 - acc: 0.735 - ETA: 1:31 - loss: 0.5224 - acc: 0.736 - ETA: 1:31 - loss: 0.5215 - acc: 0.736 - ETA: 1:31 - loss: 0.5206 - acc: 0.737 - ETA: 1:31 - loss: 0.5201 - acc: 0.737 - ETA: 1:30 - loss: 0.5194 - acc: 0.738 - ETA: 1:30 - loss: 0.5192 - acc: 0.738 - ETA: 1:30 - loss: 0.5184 - acc: 0.738 - ETA: 1:30 - loss: 0.5174 - acc: 0.739 - ETA: 1:30 - loss: 0.5165 - acc: 0.739 - ETA: 1:29 - loss: 0.5156 - acc: 0.740 - ETA: 1:29 - loss: 0.5155 - acc: 0.740 - ETA: 1:29 - loss: 0.5149 - acc: 0.741 - ETA: 1:29 - loss: 0.5144 - acc: 0.741 - ETA: 1:29 - loss: 0.5144 - acc: 0.741 - ETA: 1:28 - loss: 0.5137 - acc: 0.741 - ETA: 1:28 - loss: 0.5138 - acc: 0.741 - ETA: 1:28 - loss: 0.5131 - acc: 0.742 - ETA: 1:28 - loss: 0.5122 - acc: 0.742 - ETA: 1:28 - loss: 0.5121 - acc: 0.743 - ETA: 1:27 - loss: 0.5116 - acc: 0.743 - ETA: 1:27 - loss: 0.5115 - acc: 0.743 - ETA: 1:27 - loss: 0.5111 - acc: 0.743 - ETA: 1:27 - loss: 0.5108 - acc: 0.743 - ETA: 1:27 - loss: 0.5114 - acc: 0.743 - ETA: 1:26 - loss: 0.5107 - acc: 0.743 - ETA: 1:26 - loss: 0.5100 - acc: 0.744 - ETA: 1:26 - loss: 0.5090 - acc: 0.745 - ETA: 1:26 - loss: 0.5083 - acc: 0.745 - ETA: 1:26 - loss: 0.5078 - acc: 0.746 - ETA: 1:25 - loss: 0.5072 - acc: 0.746 - ETA: 1:25 - loss: 0.5073 - acc: 0.746 - ETA: 1:25 - loss: 0.5069 - acc: 0.746 - ETA: 1:25 - loss: 0.5059 - acc: 0.747 - ETA: 1:25 - loss: 0.5057 - acc: 0.747 - ETA: 1:24 - loss: 0.5050 - acc: 0.748 - ETA: 1:24 - loss: 0.5046 - acc: 0.748 - ETA: 1:24 - loss: 0.5038 - acc: 0.748 - ETA: 1:24 - loss: 0.5033 - acc: 0.749 - ETA: 1:24 - loss: 0.5035 - acc: 0.749 - ETA: 1:24 - loss: 0.5032 - acc: 0.749 - ETA: 1:23 - loss: 0.5023 - acc: 0.749 - ETA: 1:23 - loss: 0.5020 - acc: 0.750 - ETA: 1:23 - loss: 0.5021 - acc: 0.750 - ETA: 1:23 - loss: 0.5015 - acc: 0.750 - ETA: 1:23 - loss: 0.5009 - acc: 0.750 - ETA: 1:22 - loss: 0.5007 - acc: 0.750 - ETA: 1:22 - loss: 0.5006 - acc: 0.751 - ETA: 1:22 - loss: 0.5002 - acc: 0.751 - ETA: 1:22 - loss: 0.5000 - acc: 0.751 - ETA: 1:22 - loss: 0.4999 - acc: 0.751 - ETA: 1:22 - loss: 0.4994 - acc: 0.752 - ETA: 1:21 - loss: 0.4999 - acc: 0.751 - ETA: 1:21 - loss: 0.4995 - acc: 0.751 - ETA: 1:21 - loss: 0.4990 - acc: 0.752 - ETA: 1:21 - loss: 0.4983 - acc: 0.752 - ETA: 1:21 - loss: 0.4976 - acc: 0.753 - ETA: 1:20 - loss: 0.4970 - acc: 0.753 - ETA: 1:20 - loss: 0.4968 - acc: 0.753 - ETA: 1:20 - loss: 0.4964 - acc: 0.753 - ETA: 1:20 - loss: 0.4956 - acc: 0.754 - ETA: 1:20 - loss: 0.4953 - acc: 0.754 - ETA: 1:19 - loss: 0.4949 - acc: 0.755 - ETA: 1:19 - loss: 0.4943 - acc: 0.755 - ETA: 1:19 - loss: 0.4942 - acc: 0.755 - ETA: 1:19 - loss: 0.4935 - acc: 0.756 - ETA: 1:19 - loss: 0.4925 - acc: 0.756 - ETA: 1:19 - loss: 0.4917 - acc: 0.757 - ETA: 1:18 - loss: 0.4912 - acc: 0.757 - ETA: 1:18 - loss: 0.4904 - acc: 0.758 - ETA: 1:18 - loss: 0.4898 - acc: 0.758 - ETA: 1:18 - loss: 0.4896 - acc: 0.758 - ETA: 1:18 - loss: 0.4894 - acc: 0.759 - ETA: 1:18 - loss: 0.4893 - acc: 0.759 - ETA: 1:17 - loss: 0.4891 - acc: 0.759 - ETA: 1:17 - loss: 0.4887 - acc: 0.759 - ETA: 1:17 - loss: 0.4887 - acc: 0.759 - ETA: 1:17 - loss: 0.4884 - acc: 0.760 - ETA: 1:17 - loss: 0.4884 - acc: 0.759 - ETA: 1:16 - loss: 0.4886 - acc: 0.759 - ETA: 1:16 - loss: 0.4882 - acc: 0.760 - ETA: 1:16 - loss: 0.4880 - acc: 0.759 - ETA: 1:16 - loss: 0.4877 - acc: 0.760 - ETA: 1:16 - loss: 0.4880 - acc: 0.760 - ETA: 1:15 - loss: 0.4874 - acc: 0.760 - ETA: 1:15 - loss: 0.4871 - acc: 0.760 - ETA: 1:15 - loss: 0.4870 - acc: 0.760 - ETA: 1:15 - loss: 0.4867 - acc: 0.761 - ETA: 1:15 - loss: 0.4864 - acc: 0.761 - ETA: 1:15 - loss: 0.4858 - acc: 0.761 - ETA: 1:14 - loss: 0.4859 - acc: 0.761 - ETA: 1:14 - loss: 0.4854 - acc: 0.762 - ETA: 1:14 - loss: 0.4850 - acc: 0.762 - ETA: 1:14 - loss: 0.4846 - acc: 0.762 - ETA: 1:14 - loss: 0.4843 - acc: 0.762 - ETA: 1:13 - loss: 0.4840 - acc: 0.763 - ETA: 1:13 - loss: 0.4843 - acc: 0.762 - ETA: 1:13 - loss: 0.4842 - acc: 0.763 - ETA: 1:13 - loss: 0.4838 - acc: 0.763 - ETA: 1:13 - loss: 0.4837 - acc: 0.763 - ETA: 1:12 - loss: 0.4839 - acc: 0.763 - ETA: 1:12 - loss: 0.4842 - acc: 0.762 - ETA: 1:12 - loss: 0.4838 - acc: 0.763 - ETA: 1:12 - loss: 0.4833 - acc: 0.763 - ETA: 1:12 - loss: 0.4827 - acc: 0.764 - ETA: 1:12 - loss: 0.4829 - acc: 0.764 - ETA: 1:11 - loss: 0.4822 - acc: 0.764 - ETA: 1:11 - loss: 0.4819 - acc: 0.764 - ETA: 1:11 - loss: 0.4815 - acc: 0.765 - ETA: 1:11 - loss: 0.4811 - acc: 0.765 - ETA: 1:11 - loss: 0.4803 - acc: 0.765 - ETA: 1:10 - loss: 0.4806 - acc: 0.765 - ETA: 1:10 - loss: 0.4804 - acc: 0.765 - ETA: 1:10 - loss: 0.4799 - acc: 0.766 - ETA: 1:10 - loss: 0.4797 - acc: 0.766 - ETA: 1:10 - loss: 0.4796 - acc: 0.766 - ETA: 1:10 - loss: 0.4792 - acc: 0.766 - ETA: 1:09 - loss: 0.4794 - acc: 0.766 - ETA: 1:09 - loss: 0.4792 - acc: 0.766 - ETA: 1:09 - loss: 0.4786 - acc: 0.767 - ETA: 1:09 - loss: 0.4786 - acc: 0.767 - ETA: 1:09 - loss: 0.4782 - acc: 0.767 - ETA: 1:09 - loss: 0.4782 - acc: 0.767 - ETA: 1:08 - loss: 0.4775 - acc: 0.767 - ETA: 1:08 - loss: 0.4771 - acc: 0.768 - ETA: 1:08 - loss: 0.4768 - acc: 0.768 - ETA: 1:08 - loss: 0.4763 - acc: 0.768 - ETA: 1:08 - loss: 0.4758 - acc: 0.768 - ETA: 1:07 - loss: 0.4753 - acc: 0.769 - ETA: 1:07 - loss: 0.4748 - acc: 0.769 - ETA: 1:07 - loss: 0.4744 - acc: 0.769 - ETA: 1:07 - loss: 0.4743 - acc: 0.769 - ETA: 1:07 - loss: 0.4745 - acc: 0.769 - ETA: 1:07 - loss: 0.4741 - acc: 0.769 - ETA: 1:06 - loss: 0.4738 - acc: 0.770 - ETA: 1:06 - loss: 0.4735 - acc: 0.770 - ETA: 1:06 - loss: 0.4736 - acc: 0.770 - ETA: 1:06 - loss: 0.4734 - acc: 0.770 - ETA: 1:06 - loss: 0.4731 - acc: 0.770 - ETA: 1:05 - loss: 0.4727 - acc: 0.770 - ETA: 1:05 - loss: 0.4724 - acc: 0.770 - ETA: 1:05 - loss: 0.4723 - acc: 0.770 - ETA: 1:05 - loss: 0.4718 - acc: 0.770 - ETA: 1:05 - loss: 0.4712 - acc: 0.771 - ETA: 1:05 - loss: 0.4708 - acc: 0.771 - ETA: 1:04 - loss: 0.4709 - acc: 0.771 - ETA: 1:04 - loss: 0.4709 - acc: 0.771 - ETA: 1:04 - loss: 0.4703 - acc: 0.772 - ETA: 1:04 - loss: 0.4697 - acc: 0.772 - ETA: 1:04 - loss: 0.4694 - acc: 0.772 - ETA: 1:04 - loss: 0.4689 - acc: 0.773 - ETA: 1:03 - loss: 0.4685 - acc: 0.773 - ETA: 1:03 - loss: 0.4684 - acc: 0.773 - ETA: 1:03 - loss: 0.4679 - acc: 0.773 - ETA: 1:03 - loss: 0.4676 - acc: 0.773 - ETA: 1:03 - loss: 0.4673 - acc: 0.773 - ETA: 1:03 - loss: 0.4670 - acc: 0.774 - ETA: 1:02 - loss: 0.4664 - acc: 0.774 - ETA: 1:02 - loss: 0.4662 - acc: 0.774 - ETA: 1:02 - loss: 0.4666 - acc: 0.774 - ETA: 1:02 - loss: 0.4668 - acc: 0.774 - ETA: 1:02 - loss: 0.4666 - acc: 0.774 - ETA: 1:02 - loss: 0.4664 - acc: 0.774 - ETA: 1:01 - loss: 0.4660 - acc: 0.774 - ETA: 1:01 - loss: 0.4655 - acc: 0.774 - ETA: 1:01 - loss: 0.4650 - acc: 0.775 - ETA: 1:01 - loss: 0.4648 - acc: 0.775619904/25000 [======================>.......] - ETA: 1:01 - loss: 0.4642 - acc: 0.775 - ETA: 1:01 - loss: 0.4639 - acc: 0.776 - ETA: 1:00 - loss: 0.4637 - acc: 0.776 - ETA: 1:00 - loss: 0.4635 - acc: 0.776 - ETA: 1:00 - loss: 0.4634 - acc: 0.776 - ETA: 1:00 - loss: 0.4632 - acc: 0.776 - ETA: 1:00 - loss: 0.4630 - acc: 0.776 - ETA: 1:00 - loss: 0.4629 - acc: 0.776 - ETA: 1:00 - loss: 0.4627 - acc: 0.777 - ETA: 59s - loss: 0.4624 - acc: 0.777 - ETA: 59s - loss: 0.4620 - acc: 0.77 - ETA: 59s - loss: 0.4616 - acc: 0.77 - ETA: 59s - loss: 0.4612 - acc: 0.77 - ETA: 59s - loss: 0.4607 - acc: 0.77 - ETA: 59s - loss: 0.4601 - acc: 0.77 - ETA: 58s - loss: 0.4597 - acc: 0.77 - ETA: 58s - loss: 0.4594 - acc: 0.77 - ETA: 58s - loss: 0.4592 - acc: 0.77 - ETA: 58s - loss: 0.4587 - acc: 0.77 - ETA: 58s - loss: 0.4586 - acc: 0.77 - ETA: 58s - loss: 0.4587 - acc: 0.77 - ETA: 57s - loss: 0.4585 - acc: 0.77 - ETA: 57s - loss: 0.4583 - acc: 0.78 - ETA: 57s - loss: 0.4582 - acc: 0.78 - ETA: 57s - loss: 0.4583 - acc: 0.78 - ETA: 57s - loss: 0.4582 - acc: 0.78 - ETA: 56s - loss: 0.4580 - acc: 0.78 - ETA: 56s - loss: 0.4577 - acc: 0.78 - ETA: 56s - loss: 0.4574 - acc: 0.78 - ETA: 56s - loss: 0.4570 - acc: 0.78 - ETA: 56s - loss: 0.4567 - acc: 0.78 - ETA: 56s - loss: 0.4567 - acc: 0.78 - ETA: 55s - loss: 0.4563 - acc: 0.78 - ETA: 55s - loss: 0.4561 - acc: 0.78 - ETA: 55s - loss: 0.4559 - acc: 0.78 - ETA: 55s - loss: 0.4558 - acc: 0.78 - ETA: 55s - loss: 0.4553 - acc: 0.78 - ETA: 55s - loss: 0.4552 - acc: 0.78 - ETA: 54s - loss: 0.4553 - acc: 0.78 - ETA: 54s - loss: 0.4551 - acc: 0.78 - ETA: 54s - loss: 0.4551 - acc: 0.78 - ETA: 54s - loss: 0.4548 - acc: 0.78 - ETA: 54s - loss: 0.4550 - acc: 0.78 - ETA: 54s - loss: 0.4548 - acc: 0.78 - ETA: 53s - loss: 0.4544 - acc: 0.78 - ETA: 53s - loss: 0.4541 - acc: 0.78 - ETA: 53s - loss: 0.4537 - acc: 0.78 - ETA: 53s - loss: 0.4535 - acc: 0.78 - ETA: 53s - loss: 0.4537 - acc: 0.78 - ETA: 53s - loss: 0.4536 - acc: 0.78 - ETA: 52s - loss: 0.4535 - acc: 0.78 - ETA: 52s - loss: 0.4535 - acc: 0.78 - ETA: 52s - loss: 0.4533 - acc: 0.78 - ETA: 52s - loss: 0.4531 - acc: 0.78 - ETA: 52s - loss: 0.4527 - acc: 0.78 - ETA: 52s - loss: 0.4524 - acc: 0.78 - ETA: 51s - loss: 0.4521 - acc: 0.78 - ETA: 51s - loss: 0.4517 - acc: 0.78 - ETA: 51s - loss: 0.4518 - acc: 0.78 - ETA: 51s - loss: 0.4515 - acc: 0.78 - ETA: 51s - loss: 0.4512 - acc: 0.78 - ETA: 51s - loss: 0.4510 - acc: 0.78 - ETA: 50s - loss: 0.4506 - acc: 0.78 - ETA: 50s - loss: 0.4502 - acc: 0.78 - ETA: 50s - loss: 0.4495 - acc: 0.78 - ETA: 50s - loss: 0.4494 - acc: 0.78 - ETA: 50s - loss: 0.4490 - acc: 0.78 - ETA: 50s - loss: 0.4487 - acc: 0.78 - ETA: 49s - loss: 0.4482 - acc: 0.78 - ETA: 49s - loss: 0.4480 - acc: 0.78 - ETA: 49s - loss: 0.4482 - acc: 0.78 - ETA: 49s - loss: 0.4479 - acc: 0.78 - ETA: 49s - loss: 0.4479 - acc: 0.78 - ETA: 49s - loss: 0.4475 - acc: 0.78 - ETA: 48s - loss: 0.4474 - acc: 0.78 - ETA: 48s - loss: 0.4471 - acc: 0.78 - ETA: 48s - loss: 0.4472 - acc: 0.78 - ETA: 48s - loss: 0.4469 - acc: 0.78 - ETA: 48s - loss: 0.4468 - acc: 0.78 - ETA: 48s - loss: 0.4467 - acc: 0.78 - ETA: 47s - loss: 0.4464 - acc: 0.78 - ETA: 47s - loss: 0.4463 - acc: 0.78 - ETA: 47s - loss: 0.4460 - acc: 0.78 - ETA: 47s - loss: 0.4456 - acc: 0.78 - ETA: 47s - loss: 0.4452 - acc: 0.78 - ETA: 47s - loss: 0.4453 - acc: 0.78 - ETA: 46s - loss: 0.4451 - acc: 0.78 - ETA: 46s - loss: 0.4450 - acc: 0.78 - ETA: 46s - loss: 0.4444 - acc: 0.78 - ETA: 46s - loss: 0.4447 - acc: 0.78 - ETA: 46s - loss: 0.4447 - acc: 0.78 - ETA: 46s - loss: 0.4441 - acc: 0.78 - ETA: 45s - loss: 0.4441 - acc: 0.78 - ETA: 45s - loss: 0.4437 - acc: 0.78 - ETA: 45s - loss: 0.4436 - acc: 0.78 - ETA: 45s - loss: 0.4435 - acc: 0.78 - ETA: 45s - loss: 0.4432 - acc: 0.79 - ETA: 45s - loss: 0.4429 - acc: 0.79 - ETA: 44s - loss: 0.4428 - acc: 0.79 - ETA: 44s - loss: 0.4430 - acc: 0.79 - ETA: 44s - loss: 0.4430 - acc: 0.79 - ETA: 44s - loss: 0.4430 - acc: 0.78 - ETA: 44s - loss: 0.4433 - acc: 0.78 - ETA: 44s - loss: 0.4434 - acc: 0.78 - ETA: 43s - loss: 0.4432 - acc: 0.79 - ETA: 43s - loss: 0.4433 - acc: 0.79 - ETA: 43s - loss: 0.4434 - acc: 0.78 - ETA: 43s - loss: 0.4431 - acc: 0.79 - ETA: 43s - loss: 0.4427 - acc: 0.79 - ETA: 43s - loss: 0.4423 - acc: 0.79 - ETA: 42s - loss: 0.4424 - acc: 0.79 - ETA: 42s - loss: 0.4426 - acc: 0.79 - ETA: 42s - loss: 0.4427 - acc: 0.79 - ETA: 42s - loss: 0.4424 - acc: 0.79 - ETA: 42s - loss: 0.4421 - acc: 0.79 - ETA: 42s - loss: 0.4420 - acc: 0.79 - ETA: 41s - loss: 0.4419 - acc: 0.79 - ETA: 41s - loss: 0.4418 - acc: 0.79 - ETA: 41s - loss: 0.4415 - acc: 0.79 - ETA: 41s - loss: 0.4412 - acc: 0.79 - ETA: 41s - loss: 0.4411 - acc: 0.79 - ETA: 41s - loss: 0.4405 - acc: 0.79 - ETA: 40s - loss: 0.4404 - acc: 0.79 - ETA: 40s - loss: 0.4403 - acc: 0.79 - ETA: 40s - loss: 0.4408 - acc: 0.79 - ETA: 40s - loss: 0.4404 - acc: 0.79 - ETA: 40s - loss: 0.4405 - acc: 0.79 - ETA: 40s - loss: 0.4407 - acc: 0.79 - ETA: 39s - loss: 0.4409 - acc: 0.79 - ETA: 39s - loss: 0.4412 - acc: 0.79 - ETA: 39s - loss: 0.4408 - acc: 0.79 - ETA: 39s - loss: 0.4407 - acc: 0.79 - ETA: 39s - loss: 0.4403 - acc: 0.79 - ETA: 39s - loss: 0.4399 - acc: 0.79 - ETA: 38s - loss: 0.4396 - acc: 0.79 - ETA: 38s - loss: 0.4398 - acc: 0.79 - ETA: 38s - loss: 0.4395 - acc: 0.79 - ETA: 38s - loss: 0.4395 - acc: 0.79 - ETA: 38s - loss: 0.4393 - acc: 0.79 - ETA: 38s - loss: 0.4391 - acc: 0.79 - ETA: 38s - loss: 0.4390 - acc: 0.79 - ETA: 37s - loss: 0.4388 - acc: 0.79 - ETA: 37s - loss: 0.4388 - acc: 0.79 - ETA: 37s - loss: 0.4389 - acc: 0.79 - ETA: 37s - loss: 0.4386 - acc: 0.79 - ETA: 37s - loss: 0.4384 - acc: 0.79 - ETA: 37s - loss: 0.4382 - acc: 0.79 - ETA: 36s - loss: 0.4381 - acc: 0.79 - ETA: 36s - loss: 0.4377 - acc: 0.79 - ETA: 36s - loss: 0.4378 - acc: 0.79 - ETA: 36s - loss: 0.4377 - acc: 0.79 - ETA: 36s - loss: 0.4377 - acc: 0.79 - ETA: 36s - loss: 0.4378 - acc: 0.79 - ETA: 35s - loss: 0.4376 - acc: 0.79 - ETA: 35s - loss: 0.4372 - acc: 0.79 - ETA: 35s - loss: 0.4371 - acc: 0.79 - ETA: 35s - loss: 0.4367 - acc: 0.79 - ETA: 35s - loss: 0.4365 - acc: 0.79 - ETA: 35s - loss: 0.4364 - acc: 0.79 - ETA: 34s - loss: 0.4365 - acc: 0.79 - ETA: 34s - loss: 0.4362 - acc: 0.79 - ETA: 34s - loss: 0.4361 - acc: 0.79 - ETA: 34s - loss: 0.4359 - acc: 0.79 - ETA: 34s - loss: 0.4358 - acc: 0.79 - ETA: 34s - loss: 0.4357 - acc: 0.79 - ETA: 34s - loss: 0.4354 - acc: 0.79 - ETA: 33s - loss: 0.4354 - acc: 0.79 - ETA: 33s - loss: 0.4352 - acc: 0.79 - ETA: 33s - loss: 0.4349 - acc: 0.79 - ETA: 33s - loss: 0.4346 - acc: 0.79 - ETA: 33s - loss: 0.4344 - acc: 0.79 - ETA: 33s - loss: 0.4341 - acc: 0.79 - ETA: 32s - loss: 0.4337 - acc: 0.79 - ETA: 32s - loss: 0.4335 - acc: 0.79 - ETA: 32s - loss: 0.4334 - acc: 0.79 - ETA: 32s - loss: 0.4333 - acc: 0.79 - ETA: 32s - loss: 0.4334 - acc: 0.79 - ETA: 32s - loss: 0.4335 - acc: 0.79 - ETA: 31s - loss: 0.4332 - acc: 0.79 - ETA: 31s - loss: 0.4330 - acc: 0.79 - ETA: 31s - loss: 0.4327 - acc: 0.79 - ETA: 31s - loss: 0.4329 - acc: 0.79 - ETA: 31s - loss: 0.4329 - acc: 0.79 - ETA: 31s - loss: 0.4327 - acc: 0.79 - ETA: 31s - loss: 0.4326 - acc: 0.79 - ETA: 30s - loss: 0.4323 - acc: 0.79 - ETA: 30s - loss: 0.4321 - acc: 0.79 - ETA: 30s - loss: 0.4318 - acc: 0.79 - ETA: 30s - loss: 0.4318 - acc: 0.79 - ETA: 30s - loss: 0.4315 - acc: 0.79 - ETA: 30s - loss: 0.4313 - acc: 0.79 - ETA: 30s - loss: 0.4315 - acc: 0.79 - ETA: 29s - loss: 0.4313 - acc: 0.79 - ETA: 29s - loss: 0.4311 - acc: 0.79 - ETA: 29s - loss: 0.4308 - acc: 0.79 - ETA: 29s - loss: 0.4306 - acc: 0.79 - ETA: 29s - loss: 0.4303 - acc: 0.79 - ETA: 29s - loss: 0.4300 - acc: 0.79 - ETA: 29s - loss: 0.4299 - acc: 0.79 - ETA: 29s - loss: 0.4297 - acc: 0.79 - ETA: 28s - loss: 0.4296 - acc: 0.79 - ETA: 28s - loss: 0.4299 - acc: 0.79 - ETA: 28s - loss: 0.4298 - acc: 0.79 - ETA: 28s - loss: 0.4298 - acc: 0.79 - ETA: 28s - loss: 0.4295 - acc: 0.79 - ETA: 28s - loss: 0.4294 - acc: 0.79 - ETA: 28s - loss: 0.4293 - acc: 0.79 - ETA: 27s - loss: 0.4291 - acc: 0.79 - ETA: 27s - loss: 0.4290 - acc: 0.79 - ETA: 27s - loss: 0.4289 - acc: 0.79 - ETA: 27s - loss: 0.4288 - acc: 0.79 - ETA: 27s - loss: 0.4285 - acc: 0.79 - ETA: 27s - loss: 0.4287 - acc: 0.79 - ETA: 26s - loss: 0.4286 - acc: 0.7993"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - ETA: 26s - loss: 0.4288 - acc: 0.79 - ETA: 26s - loss: 0.4286 - acc: 0.79 - ETA: 26s - loss: 0.4287 - acc: 0.79 - ETA: 26s - loss: 0.4284 - acc: 0.79 - ETA: 26s - loss: 0.4284 - acc: 0.79 - ETA: 25s - loss: 0.4280 - acc: 0.79 - ETA: 25s - loss: 0.4280 - acc: 0.79 - ETA: 25s - loss: 0.4280 - acc: 0.79 - ETA: 25s - loss: 0.4277 - acc: 0.79 - ETA: 25s - loss: 0.4278 - acc: 0.79 - ETA: 25s - loss: 0.4277 - acc: 0.79 - ETA: 24s - loss: 0.4275 - acc: 0.80 - ETA: 24s - loss: 0.4276 - acc: 0.79 - ETA: 24s - loss: 0.4275 - acc: 0.80 - ETA: 24s - loss: 0.4272 - acc: 0.80 - ETA: 24s - loss: 0.4273 - acc: 0.80 - ETA: 24s - loss: 0.4272 - acc: 0.80 - ETA: 23s - loss: 0.4270 - acc: 0.80 - ETA: 23s - loss: 0.4267 - acc: 0.80 - ETA: 23s - loss: 0.4264 - acc: 0.80 - ETA: 23s - loss: 0.4265 - acc: 0.80 - ETA: 23s - loss: 0.4261 - acc: 0.80 - ETA: 23s - loss: 0.4260 - acc: 0.80 - ETA: 22s - loss: 0.4260 - acc: 0.80 - ETA: 22s - loss: 0.4259 - acc: 0.80 - ETA: 22s - loss: 0.4256 - acc: 0.80 - ETA: 22s - loss: 0.4254 - acc: 0.80 - ETA: 22s - loss: 0.4251 - acc: 0.80 - ETA: 22s - loss: 0.4256 - acc: 0.80 - ETA: 21s - loss: 0.4251 - acc: 0.80 - ETA: 21s - loss: 0.4251 - acc: 0.80 - ETA: 21s - loss: 0.4249 - acc: 0.80 - ETA: 21s - loss: 0.4249 - acc: 0.80 - ETA: 21s - loss: 0.4250 - acc: 0.80 - ETA: 20s - loss: 0.4251 - acc: 0.80 - ETA: 20s - loss: 0.4251 - acc: 0.80 - ETA: 20s - loss: 0.4250 - acc: 0.80 - ETA: 20s - loss: 0.4249 - acc: 0.80 - ETA: 20s - loss: 0.4248 - acc: 0.80 - ETA: 20s - loss: 0.4248 - acc: 0.80 - ETA: 19s - loss: 0.4248 - acc: 0.80 - ETA: 19s - loss: 0.4247 - acc: 0.80 - ETA: 19s - loss: 0.4245 - acc: 0.80 - ETA: 19s - loss: 0.4245 - acc: 0.80 - ETA: 19s - loss: 0.4242 - acc: 0.80 - ETA: 19s - loss: 0.4242 - acc: 0.80 - ETA: 18s - loss: 0.4242 - acc: 0.80 - ETA: 18s - loss: 0.4241 - acc: 0.80 - ETA: 18s - loss: 0.4238 - acc: 0.80 - ETA: 18s - loss: 0.4234 - acc: 0.80 - ETA: 18s - loss: 0.4234 - acc: 0.80 - ETA: 18s - loss: 0.4233 - acc: 0.80 - ETA: 17s - loss: 0.4232 - acc: 0.80 - ETA: 17s - loss: 0.4228 - acc: 0.80 - ETA: 17s - loss: 0.4227 - acc: 0.80 - ETA: 17s - loss: 0.4225 - acc: 0.80 - ETA: 17s - loss: 0.4226 - acc: 0.80 - ETA: 17s - loss: 0.4231 - acc: 0.80 - ETA: 16s - loss: 0.4227 - acc: 0.80 - ETA: 16s - loss: 0.4224 - acc: 0.80 - ETA: 16s - loss: 0.4222 - acc: 0.80 - ETA: 16s - loss: 0.4219 - acc: 0.80 - ETA: 16s - loss: 0.4216 - acc: 0.80 - ETA: 16s - loss: 0.4212 - acc: 0.80 - ETA: 15s - loss: 0.4211 - acc: 0.80 - ETA: 15s - loss: 0.4208 - acc: 0.80 - ETA: 15s - loss: 0.4205 - acc: 0.80 - ETA: 15s - loss: 0.4202 - acc: 0.80 - ETA: 15s - loss: 0.4199 - acc: 0.80 - ETA: 15s - loss: 0.4198 - acc: 0.80 - ETA: 14s - loss: 0.4195 - acc: 0.80 - ETA: 14s - loss: 0.4194 - acc: 0.80 - ETA: 14s - loss: 0.4192 - acc: 0.80 - ETA: 14s - loss: 0.4189 - acc: 0.80 - ETA: 14s - loss: 0.4186 - acc: 0.80 - ETA: 14s - loss: 0.4186 - acc: 0.80 - ETA: 13s - loss: 0.4184 - acc: 0.80 - ETA: 13s - loss: 0.4181 - acc: 0.80 - ETA: 13s - loss: 0.4179 - acc: 0.80 - ETA: 13s - loss: 0.4179 - acc: 0.80 - ETA: 13s - loss: 0.4179 - acc: 0.80 - ETA: 13s - loss: 0.4175 - acc: 0.80 - ETA: 12s - loss: 0.4176 - acc: 0.80 - ETA: 12s - loss: 0.4174 - acc: 0.80 - ETA: 12s - loss: 0.4171 - acc: 0.80 - ETA: 12s - loss: 0.4172 - acc: 0.80 - ETA: 12s - loss: 0.4171 - acc: 0.80 - ETA: 12s - loss: 0.4170 - acc: 0.80 - ETA: 11s - loss: 0.4167 - acc: 0.80 - ETA: 11s - loss: 0.4167 - acc: 0.80 - ETA: 11s - loss: 0.4167 - acc: 0.80 - ETA: 11s - loss: 0.4167 - acc: 0.80 - ETA: 11s - loss: 0.4165 - acc: 0.80 - ETA: 11s - loss: 0.4165 - acc: 0.80 - ETA: 10s - loss: 0.4163 - acc: 0.80 - ETA: 10s - loss: 0.4163 - acc: 0.80 - ETA: 10s - loss: 0.4163 - acc: 0.80 - ETA: 10s - loss: 0.4162 - acc: 0.80 - ETA: 10s - loss: 0.4160 - acc: 0.80 - ETA: 10s - loss: 0.4158 - acc: 0.80 - ETA: 9s - loss: 0.4157 - acc: 0.8077 - ETA: 9s - loss: 0.4156 - acc: 0.807 - ETA: 9s - loss: 0.4154 - acc: 0.807 - ETA: 9s - loss: 0.4154 - acc: 0.807 - ETA: 9s - loss: 0.4153 - acc: 0.807 - ETA: 8s - loss: 0.4156 - acc: 0.807 - ETA: 8s - loss: 0.4157 - acc: 0.807 - ETA: 8s - loss: 0.4155 - acc: 0.807 - ETA: 8s - loss: 0.4154 - acc: 0.807 - ETA: 8s - loss: 0.4156 - acc: 0.807 - ETA: 8s - loss: 0.4156 - acc: 0.807 - ETA: 7s - loss: 0.4156 - acc: 0.807 - ETA: 7s - loss: 0.4155 - acc: 0.807 - ETA: 7s - loss: 0.4155 - acc: 0.807 - ETA: 7s - loss: 0.4157 - acc: 0.807 - ETA: 7s - loss: 0.4157 - acc: 0.807 - ETA: 7s - loss: 0.4155 - acc: 0.807 - ETA: 6s - loss: 0.4154 - acc: 0.807 - ETA: 6s - loss: 0.4152 - acc: 0.807 - ETA: 6s - loss: 0.4153 - acc: 0.808 - ETA: 6s - loss: 0.4151 - acc: 0.808 - ETA: 6s - loss: 0.4151 - acc: 0.808 - ETA: 6s - loss: 0.4150 - acc: 0.808 - ETA: 5s - loss: 0.4149 - acc: 0.808 - ETA: 5s - loss: 0.4146 - acc: 0.808 - ETA: 5s - loss: 0.4144 - acc: 0.808 - ETA: 5s - loss: 0.4143 - acc: 0.808 - ETA: 5s - loss: 0.4144 - acc: 0.808 - ETA: 5s - loss: 0.4147 - acc: 0.808 - ETA: 4s - loss: 0.4145 - acc: 0.808 - ETA: 4s - loss: 0.4143 - acc: 0.808 - ETA: 4s - loss: 0.4141 - acc: 0.808 - ETA: 4s - loss: 0.4138 - acc: 0.809 - ETA: 4s - loss: 0.4136 - acc: 0.809 - ETA: 4s - loss: 0.4137 - acc: 0.809 - ETA: 3s - loss: 0.4135 - acc: 0.809 - ETA: 3s - loss: 0.4133 - acc: 0.809 - ETA: 3s - loss: 0.4133 - acc: 0.809 - ETA: 3s - loss: 0.4134 - acc: 0.809 - ETA: 3s - loss: 0.4133 - acc: 0.809 - ETA: 3s - loss: 0.4132 - acc: 0.809 - ETA: 2s - loss: 0.4133 - acc: 0.809 - ETA: 2s - loss: 0.4131 - acc: 0.809 - ETA: 2s - loss: 0.4130 - acc: 0.809 - ETA: 2s - loss: 0.4129 - acc: 0.809 - ETA: 2s - loss: 0.4130 - acc: 0.809 - ETA: 2s - loss: 0.4131 - acc: 0.809 - ETA: 1s - loss: 0.4131 - acc: 0.809 - ETA: 1s - loss: 0.4130 - acc: 0.809 - ETA: 1s - loss: 0.4130 - acc: 0.809 - ETA: 1s - loss: 0.4129 - acc: 0.809 - ETA: 1s - loss: 0.4128 - acc: 0.809 - ETA: 1s - loss: 0.4126 - acc: 0.809 - ETA: 0s - loss: 0.4124 - acc: 0.809 - ETA: 0s - loss: 0.4123 - acc: 0.809 - ETA: 0s - loss: 0.4123 - acc: 0.809 - ETA: 0s - loss: 0.4122 - acc: 0.809 - ETA: 0s - loss: 0.4119 - acc: 0.810 - ETA: 0s - loss: 0.4116 - acc: 0.810 - 152s 6ms/step - loss: 0.4117 - acc: 0.8102 - val_loss: 0.3397 - val_acc: 0.8524\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6528/25000 [======>.......................] - ETA: 2:10 - loss: 0.1308 - acc: 0.968 - ETA: 2:09 - loss: 0.1917 - acc: 0.921 - ETA: 2:06 - loss: 0.1713 - acc: 0.927 - ETA: 2:05 - loss: 0.1675 - acc: 0.937 - ETA: 2:05 - loss: 0.1681 - acc: 0.943 - ETA: 2:03 - loss: 0.1731 - acc: 0.947 - ETA: 2:02 - loss: 0.1789 - acc: 0.937 - ETA: 2:02 - loss: 0.1817 - acc: 0.937 - ETA: 2:01 - loss: 0.1948 - acc: 0.934 - ETA: 2:02 - loss: 0.1969 - acc: 0.931 - ETA: 2:01 - loss: 0.2041 - acc: 0.929 - ETA: 2:01 - loss: 0.2060 - acc: 0.932 - ETA: 2:00 - loss: 0.2093 - acc: 0.932 - ETA: 2:00 - loss: 0.2090 - acc: 0.933 - ETA: 2:00 - loss: 0.2207 - acc: 0.927 - ETA: 2:00 - loss: 0.2184 - acc: 0.923 - ETA: 2:00 - loss: 0.2163 - acc: 0.924 - ETA: 1:59 - loss: 0.2179 - acc: 0.923 - ETA: 1:59 - loss: 0.2143 - acc: 0.924 - ETA: 1:59 - loss: 0.2067 - acc: 0.928 - ETA: 1:58 - loss: 0.2037 - acc: 0.928 - ETA: 1:58 - loss: 0.2074 - acc: 0.923 - ETA: 1:59 - loss: 0.2044 - acc: 0.923 - ETA: 1:59 - loss: 0.2016 - acc: 0.924 - ETA: 2:00 - loss: 0.2004 - acc: 0.926 - ETA: 2:00 - loss: 0.2060 - acc: 0.924 - ETA: 2:00 - loss: 0.2053 - acc: 0.923 - ETA: 2:01 - loss: 0.2122 - acc: 0.920 - ETA: 2:01 - loss: 0.2111 - acc: 0.920 - ETA: 2:01 - loss: 0.2138 - acc: 0.918 - ETA: 2:01 - loss: 0.2136 - acc: 0.918 - ETA: 2:01 - loss: 0.2284 - acc: 0.913 - ETA: 2:01 - loss: 0.2275 - acc: 0.914 - ETA: 2:00 - loss: 0.2250 - acc: 0.916 - ETA: 2:00 - loss: 0.2233 - acc: 0.917 - ETA: 2:01 - loss: 0.2211 - acc: 0.920 - ETA: 2:01 - loss: 0.2225 - acc: 0.918 - ETA: 2:01 - loss: 0.2234 - acc: 0.917 - ETA: 2:01 - loss: 0.2209 - acc: 0.919 - ETA: 2:01 - loss: 0.2186 - acc: 0.919 - ETA: 2:01 - loss: 0.2173 - acc: 0.920 - ETA: 2:02 - loss: 0.2167 - acc: 0.920 - ETA: 2:02 - loss: 0.2148 - acc: 0.920 - ETA: 2:02 - loss: 0.2137 - acc: 0.921 - ETA: 2:02 - loss: 0.2147 - acc: 0.921 - ETA: 2:02 - loss: 0.2128 - acc: 0.921 - ETA: 2:03 - loss: 0.2107 - acc: 0.922 - ETA: 2:03 - loss: 0.2101 - acc: 0.921 - ETA: 2:02 - loss: 0.2096 - acc: 0.921 - ETA: 2:03 - loss: 0.2083 - acc: 0.921 - ETA: 2:03 - loss: 0.2080 - acc: 0.921 - ETA: 2:03 - loss: 0.2090 - acc: 0.920 - ETA: 2:03 - loss: 0.2064 - acc: 0.922 - ETA: 2:03 - loss: 0.2051 - acc: 0.923 - ETA: 2:02 - loss: 0.2032 - acc: 0.923 - ETA: 2:02 - loss: 0.2075 - acc: 0.922 - ETA: 2:03 - loss: 0.2058 - acc: 0.923 - ETA: 2:03 - loss: 0.2066 - acc: 0.923 - ETA: 2:02 - loss: 0.2063 - acc: 0.922 - ETA: 2:02 - loss: 0.2054 - acc: 0.922 - ETA: 2:02 - loss: 0.2042 - acc: 0.923 - ETA: 2:02 - loss: 0.2050 - acc: 0.922 - ETA: 2:02 - loss: 0.2075 - acc: 0.921 - ETA: 2:01 - loss: 0.2073 - acc: 0.921 - ETA: 2:01 - loss: 0.2077 - acc: 0.920 - ETA: 2:01 - loss: 0.2088 - acc: 0.920 - ETA: 2:00 - loss: 0.2094 - acc: 0.920 - ETA: 2:00 - loss: 0.2112 - acc: 0.920 - ETA: 2:00 - loss: 0.2107 - acc: 0.920 - ETA: 1:59 - loss: 0.2102 - acc: 0.920 - ETA: 1:59 - loss: 0.2085 - acc: 0.921 - ETA: 1:59 - loss: 0.2086 - acc: 0.921 - ETA: 1:58 - loss: 0.2078 - acc: 0.921 - ETA: 1:58 - loss: 0.2097 - acc: 0.920 - ETA: 1:58 - loss: 0.2076 - acc: 0.921 - ETA: 1:57 - loss: 0.2063 - acc: 0.921 - ETA: 1:57 - loss: 0.2053 - acc: 0.921 - ETA: 1:57 - loss: 0.2044 - acc: 0.921 - ETA: 1:57 - loss: 0.2044 - acc: 0.922 - ETA: 1:56 - loss: 0.2021 - acc: 0.923 - ETA: 1:56 - loss: 0.2016 - acc: 0.923 - ETA: 1:56 - loss: 0.2076 - acc: 0.923 - ETA: 1:55 - loss: 0.2066 - acc: 0.923 - ETA: 1:55 - loss: 0.2062 - acc: 0.923 - ETA: 1:55 - loss: 0.2060 - acc: 0.923 - ETA: 1:55 - loss: 0.2056 - acc: 0.923 - ETA: 1:54 - loss: 0.2043 - acc: 0.924 - ETA: 1:54 - loss: 0.2051 - acc: 0.924 - ETA: 1:54 - loss: 0.2049 - acc: 0.924 - ETA: 1:54 - loss: 0.2032 - acc: 0.925 - ETA: 1:53 - loss: 0.2030 - acc: 0.925 - ETA: 1:53 - loss: 0.2017 - acc: 0.926 - ETA: 1:53 - loss: 0.2011 - acc: 0.926 - ETA: 1:53 - loss: 0.2011 - acc: 0.926 - ETA: 1:52 - loss: 0.2021 - acc: 0.926 - ETA: 1:52 - loss: 0.2011 - acc: 0.926 - ETA: 1:52 - loss: 0.2009 - acc: 0.926 - ETA: 1:52 - loss: 0.2007 - acc: 0.927 - ETA: 1:51 - loss: 0.2007 - acc: 0.926 - ETA: 1:52 - loss: 0.2025 - acc: 0.925 - ETA: 1:52 - loss: 0.2023 - acc: 0.926 - ETA: 1:51 - loss: 0.2026 - acc: 0.925 - ETA: 1:51 - loss: 0.2042 - acc: 0.925 - ETA: 1:51 - loss: 0.2046 - acc: 0.925 - ETA: 1:51 - loss: 0.2061 - acc: 0.925 - ETA: 1:51 - loss: 0.2064 - acc: 0.924 - ETA: 1:51 - loss: 0.2059 - acc: 0.925 - ETA: 1:50 - loss: 0.2062 - acc: 0.925 - ETA: 1:50 - loss: 0.2060 - acc: 0.925 - ETA: 1:50 - loss: 0.2065 - acc: 0.925 - ETA: 1:50 - loss: 0.2061 - acc: 0.925 - ETA: 1:50 - loss: 0.2059 - acc: 0.926 - ETA: 1:50 - loss: 0.2071 - acc: 0.925 - ETA: 1:50 - loss: 0.2066 - acc: 0.926 - ETA: 1:49 - loss: 0.2066 - acc: 0.926 - ETA: 1:49 - loss: 0.2060 - acc: 0.926 - ETA: 1:49 - loss: 0.2059 - acc: 0.926 - ETA: 1:49 - loss: 0.2058 - acc: 0.926 - ETA: 1:48 - loss: 0.2053 - acc: 0.926 - ETA: 1:48 - loss: 0.2053 - acc: 0.926 - ETA: 1:48 - loss: 0.2051 - acc: 0.926 - ETA: 1:48 - loss: 0.2037 - acc: 0.927 - ETA: 1:48 - loss: 0.2028 - acc: 0.927 - ETA: 1:47 - loss: 0.2045 - acc: 0.927 - ETA: 1:47 - loss: 0.2049 - acc: 0.927 - ETA: 1:47 - loss: 0.2043 - acc: 0.927 - ETA: 1:47 - loss: 0.2042 - acc: 0.927 - ETA: 1:46 - loss: 0.2047 - acc: 0.926 - ETA: 1:46 - loss: 0.2041 - acc: 0.926 - ETA: 1:46 - loss: 0.2041 - acc: 0.927 - ETA: 1:46 - loss: 0.2048 - acc: 0.927 - ETA: 1:46 - loss: 0.2047 - acc: 0.926 - ETA: 1:46 - loss: 0.2045 - acc: 0.926 - ETA: 1:45 - loss: 0.2038 - acc: 0.926 - ETA: 1:45 - loss: 0.2036 - acc: 0.926 - ETA: 1:45 - loss: 0.2035 - acc: 0.926 - ETA: 1:45 - loss: 0.2037 - acc: 0.926 - ETA: 1:45 - loss: 0.2037 - acc: 0.926 - ETA: 1:45 - loss: 0.2039 - acc: 0.926 - ETA: 1:44 - loss: 0.2034 - acc: 0.926 - ETA: 1:44 - loss: 0.2030 - acc: 0.926 - ETA: 1:44 - loss: 0.2027 - acc: 0.926 - ETA: 1:44 - loss: 0.2024 - acc: 0.926 - ETA: 1:44 - loss: 0.2020 - acc: 0.926 - ETA: 1:43 - loss: 0.2018 - acc: 0.927 - ETA: 1:43 - loss: 0.2010 - acc: 0.927 - ETA: 1:43 - loss: 0.2009 - acc: 0.927 - ETA: 1:43 - loss: 0.2004 - acc: 0.927 - ETA: 1:43 - loss: 0.2005 - acc: 0.927 - ETA: 1:42 - loss: 0.2003 - acc: 0.927 - ETA: 1:42 - loss: 0.1998 - acc: 0.927 - ETA: 1:42 - loss: 0.1996 - acc: 0.927 - ETA: 1:42 - loss: 0.1992 - acc: 0.927 - ETA: 1:42 - loss: 0.1996 - acc: 0.927 - ETA: 1:42 - loss: 0.1991 - acc: 0.928 - ETA: 1:41 - loss: 0.1993 - acc: 0.927 - ETA: 1:41 - loss: 0.1993 - acc: 0.927 - ETA: 1:41 - loss: 0.1983 - acc: 0.928 - ETA: 1:41 - loss: 0.1977 - acc: 0.928 - ETA: 1:41 - loss: 0.1973 - acc: 0.928 - ETA: 1:41 - loss: 0.1966 - acc: 0.929 - ETA: 1:40 - loss: 0.1968 - acc: 0.929 - ETA: 1:40 - loss: 0.1970 - acc: 0.929 - ETA: 1:40 - loss: 0.1963 - acc: 0.929 - ETA: 1:40 - loss: 0.1957 - acc: 0.929 - ETA: 1:40 - loss: 0.1952 - acc: 0.929 - ETA: 1:40 - loss: 0.1951 - acc: 0.929 - ETA: 1:40 - loss: 0.1957 - acc: 0.929 - ETA: 1:39 - loss: 0.1954 - acc: 0.929 - ETA: 1:39 - loss: 0.1961 - acc: 0.928 - ETA: 1:39 - loss: 0.1955 - acc: 0.929 - ETA: 1:39 - loss: 0.1952 - acc: 0.929 - ETA: 1:39 - loss: 0.1955 - acc: 0.928 - ETA: 1:39 - loss: 0.1958 - acc: 0.928 - ETA: 1:39 - loss: 0.1957 - acc: 0.928 - ETA: 1:38 - loss: 0.1957 - acc: 0.928 - ETA: 1:38 - loss: 0.1952 - acc: 0.929 - ETA: 1:38 - loss: 0.1952 - acc: 0.929 - ETA: 1:38 - loss: 0.1947 - acc: 0.929 - ETA: 1:38 - loss: 0.1949 - acc: 0.929 - ETA: 1:38 - loss: 0.1963 - acc: 0.928 - ETA: 1:37 - loss: 0.1963 - acc: 0.928 - ETA: 1:37 - loss: 0.1961 - acc: 0.928 - ETA: 1:37 - loss: 0.1969 - acc: 0.928 - ETA: 1:37 - loss: 0.1972 - acc: 0.928 - ETA: 1:37 - loss: 0.1972 - acc: 0.927 - ETA: 1:37 - loss: 0.1971 - acc: 0.928 - ETA: 1:37 - loss: 0.1969 - acc: 0.928 - ETA: 1:36 - loss: 0.1972 - acc: 0.927 - ETA: 1:36 - loss: 0.1977 - acc: 0.927 - ETA: 1:36 - loss: 0.1976 - acc: 0.927 - ETA: 1:36 - loss: 0.1982 - acc: 0.927 - ETA: 1:36 - loss: 0.1993 - acc: 0.926 - ETA: 1:36 - loss: 0.1990 - acc: 0.926 - ETA: 1:36 - loss: 0.1994 - acc: 0.926 - ETA: 1:35 - loss: 0.1999 - acc: 0.926 - ETA: 1:35 - loss: 0.2002 - acc: 0.926 - ETA: 1:35 - loss: 0.2004 - acc: 0.926 - ETA: 1:35 - loss: 0.2004 - acc: 0.926 - ETA: 1:35 - loss: 0.2002 - acc: 0.926 - ETA: 1:35 - loss: 0.2000 - acc: 0.926 - ETA: 1:35 - loss: 0.2001 - acc: 0.926 - ETA: 1:34 - loss: 0.1996 - acc: 0.926 - ETA: 1:34 - loss: 0.1993 - acc: 0.926813056/25000 [==============>...............] - ETA: 1:34 - loss: 0.1988 - acc: 0.926 - ETA: 1:34 - loss: 0.1990 - acc: 0.926 - ETA: 1:34 - loss: 0.1987 - acc: 0.926 - ETA: 1:34 - loss: 0.1993 - acc: 0.926 - ETA: 1:34 - loss: 0.1992 - acc: 0.926 - ETA: 1:33 - loss: 0.1999 - acc: 0.926 - ETA: 1:33 - loss: 0.2001 - acc: 0.926 - ETA: 1:33 - loss: 0.2004 - acc: 0.926 - ETA: 1:33 - loss: 0.2004 - acc: 0.926 - ETA: 1:33 - loss: 0.2012 - acc: 0.925 - ETA: 1:33 - loss: 0.2016 - acc: 0.925 - ETA: 1:33 - loss: 0.2019 - acc: 0.925 - ETA: 1:33 - loss: 0.2019 - acc: 0.925 - ETA: 1:32 - loss: 0.2017 - acc: 0.925 - ETA: 1:32 - loss: 0.2020 - acc: 0.925 - ETA: 1:32 - loss: 0.2020 - acc: 0.925 - ETA: 1:32 - loss: 0.2019 - acc: 0.925 - ETA: 1:32 - loss: 0.2017 - acc: 0.926 - ETA: 1:32 - loss: 0.2017 - acc: 0.926 - ETA: 1:32 - loss: 0.2019 - acc: 0.925 - ETA: 1:32 - loss: 0.2015 - acc: 0.926 - ETA: 1:32 - loss: 0.2018 - acc: 0.925 - ETA: 1:31 - loss: 0.2019 - acc: 0.925 - ETA: 1:31 - loss: 0.2023 - acc: 0.925 - ETA: 1:31 - loss: 0.2020 - acc: 0.925 - ETA: 1:31 - loss: 0.2021 - acc: 0.926 - ETA: 1:31 - loss: 0.2016 - acc: 0.926 - ETA: 1:30 - loss: 0.2017 - acc: 0.926 - ETA: 1:30 - loss: 0.2014 - acc: 0.926 - ETA: 1:30 - loss: 0.2023 - acc: 0.925 - ETA: 1:30 - loss: 0.2024 - acc: 0.925 - ETA: 1:30 - loss: 0.2022 - acc: 0.925 - ETA: 1:29 - loss: 0.2018 - acc: 0.926 - ETA: 1:29 - loss: 0.2020 - acc: 0.925 - ETA: 1:29 - loss: 0.2027 - acc: 0.925 - ETA: 1:29 - loss: 0.2024 - acc: 0.925 - ETA: 1:29 - loss: 0.2021 - acc: 0.926 - ETA: 1:29 - loss: 0.2016 - acc: 0.926 - ETA: 1:28 - loss: 0.2012 - acc: 0.926 - ETA: 1:28 - loss: 0.2011 - acc: 0.926 - ETA: 1:28 - loss: 0.2010 - acc: 0.926 - ETA: 1:28 - loss: 0.2010 - acc: 0.926 - ETA: 1:28 - loss: 0.2017 - acc: 0.926 - ETA: 1:28 - loss: 0.2014 - acc: 0.926 - ETA: 1:28 - loss: 0.2020 - acc: 0.926 - ETA: 1:28 - loss: 0.2031 - acc: 0.926 - ETA: 1:27 - loss: 0.2029 - acc: 0.926 - ETA: 1:27 - loss: 0.2028 - acc: 0.926 - ETA: 1:27 - loss: 0.2034 - acc: 0.926 - ETA: 1:27 - loss: 0.2030 - acc: 0.926 - ETA: 1:27 - loss: 0.2032 - acc: 0.926 - ETA: 1:27 - loss: 0.2036 - acc: 0.926 - ETA: 1:27 - loss: 0.2041 - acc: 0.925 - ETA: 1:26 - loss: 0.2039 - acc: 0.926 - ETA: 1:26 - loss: 0.2047 - acc: 0.925 - ETA: 1:26 - loss: 0.2047 - acc: 0.925 - ETA: 1:26 - loss: 0.2042 - acc: 0.925 - ETA: 1:26 - loss: 0.2047 - acc: 0.925 - ETA: 1:25 - loss: 0.2050 - acc: 0.925 - ETA: 1:25 - loss: 0.2050 - acc: 0.925 - ETA: 1:25 - loss: 0.2047 - acc: 0.925 - ETA: 1:25 - loss: 0.2046 - acc: 0.925 - ETA: 1:25 - loss: 0.2045 - acc: 0.925 - ETA: 1:25 - loss: 0.2046 - acc: 0.925 - ETA: 1:25 - loss: 0.2045 - acc: 0.925 - ETA: 1:24 - loss: 0.2043 - acc: 0.925 - ETA: 1:24 - loss: 0.2050 - acc: 0.924 - ETA: 1:24 - loss: 0.2049 - acc: 0.925 - ETA: 1:24 - loss: 0.2052 - acc: 0.924 - ETA: 1:24 - loss: 0.2049 - acc: 0.924 - ETA: 1:23 - loss: 0.2050 - acc: 0.924 - ETA: 1:23 - loss: 0.2047 - acc: 0.924 - ETA: 1:23 - loss: 0.2045 - acc: 0.924 - ETA: 1:23 - loss: 0.2040 - acc: 0.924 - ETA: 1:23 - loss: 0.2044 - acc: 0.924 - ETA: 1:23 - loss: 0.2049 - acc: 0.924 - ETA: 1:22 - loss: 0.2052 - acc: 0.924 - ETA: 1:22 - loss: 0.2064 - acc: 0.923 - ETA: 1:22 - loss: 0.2064 - acc: 0.923 - ETA: 1:22 - loss: 0.2064 - acc: 0.923 - ETA: 1:22 - loss: 0.2070 - acc: 0.923 - ETA: 1:22 - loss: 0.2071 - acc: 0.923 - ETA: 1:22 - loss: 0.2070 - acc: 0.923 - ETA: 1:21 - loss: 0.2067 - acc: 0.923 - ETA: 1:21 - loss: 0.2071 - acc: 0.923 - ETA: 1:21 - loss: 0.2070 - acc: 0.923 - ETA: 1:21 - loss: 0.2069 - acc: 0.923 - ETA: 1:21 - loss: 0.2073 - acc: 0.923 - ETA: 1:21 - loss: 0.2070 - acc: 0.923 - ETA: 1:20 - loss: 0.2067 - acc: 0.923 - ETA: 1:20 - loss: 0.2072 - acc: 0.923 - ETA: 1:20 - loss: 0.2072 - acc: 0.923 - ETA: 1:20 - loss: 0.2070 - acc: 0.923 - ETA: 1:20 - loss: 0.2066 - acc: 0.923 - ETA: 1:20 - loss: 0.2067 - acc: 0.923 - ETA: 1:19 - loss: 0.2074 - acc: 0.922 - ETA: 1:19 - loss: 0.2083 - acc: 0.922 - ETA: 1:19 - loss: 0.2091 - acc: 0.922 - ETA: 1:19 - loss: 0.2089 - acc: 0.922 - ETA: 1:19 - loss: 0.2092 - acc: 0.922 - ETA: 1:19 - loss: 0.2093 - acc: 0.922 - ETA: 1:18 - loss: 0.2094 - acc: 0.922 - ETA: 1:18 - loss: 0.2098 - acc: 0.922 - ETA: 1:18 - loss: 0.2099 - acc: 0.922 - ETA: 1:18 - loss: 0.2100 - acc: 0.922 - ETA: 1:18 - loss: 0.2103 - acc: 0.922 - ETA: 1:18 - loss: 0.2105 - acc: 0.921 - ETA: 1:18 - loss: 0.2104 - acc: 0.922 - ETA: 1:18 - loss: 0.2103 - acc: 0.921 - ETA: 1:17 - loss: 0.2102 - acc: 0.922 - ETA: 1:17 - loss: 0.2106 - acc: 0.922 - ETA: 1:17 - loss: 0.2105 - acc: 0.922 - ETA: 1:17 - loss: 0.2105 - acc: 0.921 - ETA: 1:17 - loss: 0.2104 - acc: 0.921 - ETA: 1:16 - loss: 0.2103 - acc: 0.921 - ETA: 1:16 - loss: 0.2109 - acc: 0.921 - ETA: 1:16 - loss: 0.2104 - acc: 0.921 - ETA: 1:16 - loss: 0.2102 - acc: 0.921 - ETA: 1:16 - loss: 0.2098 - acc: 0.921 - ETA: 1:15 - loss: 0.2102 - acc: 0.921 - ETA: 1:15 - loss: 0.2098 - acc: 0.921 - ETA: 1:15 - loss: 0.2093 - acc: 0.922 - ETA: 1:15 - loss: 0.2101 - acc: 0.921 - ETA: 1:15 - loss: 0.2104 - acc: 0.921 - ETA: 1:15 - loss: 0.2102 - acc: 0.921 - ETA: 1:14 - loss: 0.2100 - acc: 0.921 - ETA: 1:14 - loss: 0.2097 - acc: 0.921 - ETA: 1:14 - loss: 0.2094 - acc: 0.922 - ETA: 1:14 - loss: 0.2095 - acc: 0.922 - ETA: 1:14 - loss: 0.2093 - acc: 0.922 - ETA: 1:13 - loss: 0.2090 - acc: 0.922 - ETA: 1:13 - loss: 0.2089 - acc: 0.922 - ETA: 1:13 - loss: 0.2087 - acc: 0.922 - ETA: 1:13 - loss: 0.2088 - acc: 0.922 - ETA: 1:13 - loss: 0.2091 - acc: 0.922 - ETA: 1:13 - loss: 0.2090 - acc: 0.922 - ETA: 1:12 - loss: 0.2088 - acc: 0.922 - ETA: 1:12 - loss: 0.2085 - acc: 0.922 - ETA: 1:12 - loss: 0.2088 - acc: 0.922 - ETA: 1:12 - loss: 0.2093 - acc: 0.922 - ETA: 1:12 - loss: 0.2099 - acc: 0.921 - ETA: 1:11 - loss: 0.2098 - acc: 0.922 - ETA: 1:11 - loss: 0.2099 - acc: 0.921 - ETA: 1:11 - loss: 0.2100 - acc: 0.921 - ETA: 1:11 - loss: 0.2101 - acc: 0.921 - ETA: 1:11 - loss: 0.2100 - acc: 0.921 - ETA: 1:11 - loss: 0.2101 - acc: 0.921 - ETA: 1:10 - loss: 0.2102 - acc: 0.921 - ETA: 1:10 - loss: 0.2098 - acc: 0.921 - ETA: 1:10 - loss: 0.2098 - acc: 0.922 - ETA: 1:10 - loss: 0.2100 - acc: 0.921 - ETA: 1:10 - loss: 0.2104 - acc: 0.921 - ETA: 1:10 - loss: 0.2108 - acc: 0.921 - ETA: 1:10 - loss: 0.2113 - acc: 0.921 - ETA: 1:09 - loss: 0.2112 - acc: 0.921 - ETA: 1:09 - loss: 0.2111 - acc: 0.921 - ETA: 1:09 - loss: 0.2111 - acc: 0.921 - ETA: 1:09 - loss: 0.2113 - acc: 0.920 - ETA: 1:09 - loss: 0.2110 - acc: 0.921 - ETA: 1:09 - loss: 0.2113 - acc: 0.920 - ETA: 1:09 - loss: 0.2115 - acc: 0.920 - ETA: 1:09 - loss: 0.2119 - acc: 0.920 - ETA: 1:08 - loss: 0.2117 - acc: 0.920 - ETA: 1:08 - loss: 0.2115 - acc: 0.920 - ETA: 1:08 - loss: 0.2112 - acc: 0.920 - ETA: 1:08 - loss: 0.2111 - acc: 0.920 - ETA: 1:08 - loss: 0.2108 - acc: 0.921 - ETA: 1:07 - loss: 0.2107 - acc: 0.921 - ETA: 1:07 - loss: 0.2105 - acc: 0.921 - ETA: 1:07 - loss: 0.2107 - acc: 0.921 - ETA: 1:07 - loss: 0.2106 - acc: 0.921 - ETA: 1:07 - loss: 0.2107 - acc: 0.921 - ETA: 1:07 - loss: 0.2106 - acc: 0.921 - ETA: 1:06 - loss: 0.2107 - acc: 0.921 - ETA: 1:06 - loss: 0.2105 - acc: 0.921 - ETA: 1:06 - loss: 0.2106 - acc: 0.921 - ETA: 1:06 - loss: 0.2108 - acc: 0.920 - ETA: 1:06 - loss: 0.2108 - acc: 0.920 - ETA: 1:06 - loss: 0.2108 - acc: 0.920 - ETA: 1:05 - loss: 0.2105 - acc: 0.921 - ETA: 1:05 - loss: 0.2102 - acc: 0.921 - ETA: 1:05 - loss: 0.2101 - acc: 0.921 - ETA: 1:05 - loss: 0.2101 - acc: 0.920 - ETA: 1:05 - loss: 0.2104 - acc: 0.920 - ETA: 1:05 - loss: 0.2101 - acc: 0.920 - ETA: 1:04 - loss: 0.2101 - acc: 0.920 - ETA: 1:04 - loss: 0.2101 - acc: 0.920 - ETA: 1:04 - loss: 0.2101 - acc: 0.920 - ETA: 1:04 - loss: 0.2101 - acc: 0.920 - ETA: 1:04 - loss: 0.2103 - acc: 0.920 - ETA: 1:04 - loss: 0.2101 - acc: 0.920 - ETA: 1:03 - loss: 0.2100 - acc: 0.920 - ETA: 1:03 - loss: 0.2099 - acc: 0.920 - ETA: 1:03 - loss: 0.2098 - acc: 0.920 - ETA: 1:03 - loss: 0.2095 - acc: 0.920 - ETA: 1:03 - loss: 0.2093 - acc: 0.920 - ETA: 1:03 - loss: 0.2094 - acc: 0.920 - ETA: 1:03 - loss: 0.2096 - acc: 0.920 - ETA: 1:02 - loss: 0.2094 - acc: 0.920 - ETA: 1:02 - loss: 0.2091 - acc: 0.920 - ETA: 1:02 - loss: 0.2088 - acc: 0.921 - ETA: 1:02 - loss: 0.2088 - acc: 0.921 - ETA: 1:02 - loss: 0.2091 - acc: 0.921 - ETA: 1:02 - loss: 0.2093 - acc: 0.921319904/25000 [======================>.......] - ETA: 1:02 - loss: 0.2101 - acc: 0.921 - ETA: 1:01 - loss: 0.2105 - acc: 0.921 - ETA: 1:01 - loss: 0.2104 - acc: 0.921 - ETA: 1:01 - loss: 0.2100 - acc: 0.921 - ETA: 1:01 - loss: 0.2100 - acc: 0.921 - ETA: 1:01 - loss: 0.2101 - acc: 0.921 - ETA: 1:01 - loss: 0.2098 - acc: 0.921 - ETA: 1:00 - loss: 0.2104 - acc: 0.921 - ETA: 1:00 - loss: 0.2104 - acc: 0.921 - ETA: 1:00 - loss: 0.2108 - acc: 0.920 - ETA: 1:00 - loss: 0.2109 - acc: 0.920 - ETA: 1:00 - loss: 0.2110 - acc: 0.920 - ETA: 1:00 - loss: 0.2114 - acc: 0.920 - ETA: 59s - loss: 0.2116 - acc: 0.920 - ETA: 59s - loss: 0.2117 - acc: 0.92 - ETA: 59s - loss: 0.2117 - acc: 0.92 - ETA: 59s - loss: 0.2118 - acc: 0.91 - ETA: 59s - loss: 0.2117 - acc: 0.91 - ETA: 58s - loss: 0.2118 - acc: 0.91 - ETA: 58s - loss: 0.2117 - acc: 0.91 - ETA: 58s - loss: 0.2115 - acc: 0.91 - ETA: 58s - loss: 0.2114 - acc: 0.91 - ETA: 58s - loss: 0.2113 - acc: 0.92 - ETA: 58s - loss: 0.2113 - acc: 0.91 - ETA: 57s - loss: 0.2117 - acc: 0.91 - ETA: 57s - loss: 0.2114 - acc: 0.91 - ETA: 57s - loss: 0.2115 - acc: 0.91 - ETA: 57s - loss: 0.2120 - acc: 0.91 - ETA: 57s - loss: 0.2125 - acc: 0.91 - ETA: 56s - loss: 0.2124 - acc: 0.91 - ETA: 56s - loss: 0.2127 - acc: 0.91 - ETA: 56s - loss: 0.2133 - acc: 0.91 - ETA: 56s - loss: 0.2134 - acc: 0.91 - ETA: 56s - loss: 0.2141 - acc: 0.91 - ETA: 56s - loss: 0.2142 - acc: 0.91 - ETA: 56s - loss: 0.2141 - acc: 0.91 - ETA: 55s - loss: 0.2140 - acc: 0.91 - ETA: 55s - loss: 0.2141 - acc: 0.91 - ETA: 55s - loss: 0.2142 - acc: 0.91 - ETA: 55s - loss: 0.2142 - acc: 0.91 - ETA: 55s - loss: 0.2143 - acc: 0.91 - ETA: 54s - loss: 0.2143 - acc: 0.91 - ETA: 54s - loss: 0.2144 - acc: 0.91 - ETA: 54s - loss: 0.2144 - acc: 0.91 - ETA: 54s - loss: 0.2143 - acc: 0.91 - ETA: 54s - loss: 0.2141 - acc: 0.91 - ETA: 54s - loss: 0.2140 - acc: 0.91 - ETA: 53s - loss: 0.2139 - acc: 0.91 - ETA: 53s - loss: 0.2140 - acc: 0.91 - ETA: 53s - loss: 0.2140 - acc: 0.91 - ETA: 53s - loss: 0.2140 - acc: 0.91 - ETA: 53s - loss: 0.2137 - acc: 0.91 - ETA: 52s - loss: 0.2138 - acc: 0.91 - ETA: 52s - loss: 0.2138 - acc: 0.91 - ETA: 52s - loss: 0.2140 - acc: 0.91 - ETA: 52s - loss: 0.2138 - acc: 0.91 - ETA: 52s - loss: 0.2139 - acc: 0.91 - ETA: 52s - loss: 0.2137 - acc: 0.91 - ETA: 51s - loss: 0.2137 - acc: 0.91 - ETA: 51s - loss: 0.2136 - acc: 0.91 - ETA: 51s - loss: 0.2137 - acc: 0.91 - ETA: 51s - loss: 0.2135 - acc: 0.91 - ETA: 51s - loss: 0.2135 - acc: 0.91 - ETA: 51s - loss: 0.2132 - acc: 0.91 - ETA: 50s - loss: 0.2130 - acc: 0.91 - ETA: 50s - loss: 0.2127 - acc: 0.91 - ETA: 50s - loss: 0.2129 - acc: 0.91 - ETA: 50s - loss: 0.2129 - acc: 0.91 - ETA: 50s - loss: 0.2131 - acc: 0.91 - ETA: 49s - loss: 0.2136 - acc: 0.91 - ETA: 49s - loss: 0.2135 - acc: 0.91 - ETA: 49s - loss: 0.2135 - acc: 0.91 - ETA: 49s - loss: 0.2138 - acc: 0.91 - ETA: 49s - loss: 0.2137 - acc: 0.91 - ETA: 49s - loss: 0.2138 - acc: 0.91 - ETA: 48s - loss: 0.2139 - acc: 0.91 - ETA: 48s - loss: 0.2141 - acc: 0.91 - ETA: 48s - loss: 0.2145 - acc: 0.91 - ETA: 48s - loss: 0.2147 - acc: 0.91 - ETA: 48s - loss: 0.2146 - acc: 0.91 - ETA: 48s - loss: 0.2145 - acc: 0.91 - ETA: 47s - loss: 0.2147 - acc: 0.91 - ETA: 47s - loss: 0.2148 - acc: 0.91 - ETA: 47s - loss: 0.2147 - acc: 0.91 - ETA: 47s - loss: 0.2147 - acc: 0.91 - ETA: 47s - loss: 0.2151 - acc: 0.91 - ETA: 46s - loss: 0.2154 - acc: 0.91 - ETA: 46s - loss: 0.2153 - acc: 0.91 - ETA: 46s - loss: 0.2152 - acc: 0.91 - ETA: 46s - loss: 0.2152 - acc: 0.91 - ETA: 46s - loss: 0.2150 - acc: 0.91 - ETA: 46s - loss: 0.2148 - acc: 0.91 - ETA: 46s - loss: 0.2148 - acc: 0.91 - ETA: 45s - loss: 0.2147 - acc: 0.91 - ETA: 45s - loss: 0.2151 - acc: 0.91 - ETA: 45s - loss: 0.2154 - acc: 0.91 - ETA: 45s - loss: 0.2151 - acc: 0.91 - ETA: 45s - loss: 0.2151 - acc: 0.91 - ETA: 45s - loss: 0.2150 - acc: 0.91 - ETA: 44s - loss: 0.2149 - acc: 0.91 - ETA: 44s - loss: 0.2151 - acc: 0.91 - ETA: 44s - loss: 0.2151 - acc: 0.91 - ETA: 44s - loss: 0.2151 - acc: 0.91 - ETA: 44s - loss: 0.2149 - acc: 0.91 - ETA: 44s - loss: 0.2154 - acc: 0.91 - ETA: 44s - loss: 0.2153 - acc: 0.91 - ETA: 43s - loss: 0.2163 - acc: 0.91 - ETA: 43s - loss: 0.2163 - acc: 0.91 - ETA: 43s - loss: 0.2163 - acc: 0.91 - ETA: 43s - loss: 0.2161 - acc: 0.91 - ETA: 43s - loss: 0.2163 - acc: 0.91 - ETA: 43s - loss: 0.2165 - acc: 0.91 - ETA: 43s - loss: 0.2165 - acc: 0.91 - ETA: 42s - loss: 0.2167 - acc: 0.91 - ETA: 42s - loss: 0.2167 - acc: 0.91 - ETA: 42s - loss: 0.2168 - acc: 0.91 - ETA: 42s - loss: 0.2172 - acc: 0.91 - ETA: 42s - loss: 0.2172 - acc: 0.91 - ETA: 42s - loss: 0.2171 - acc: 0.91 - ETA: 41s - loss: 0.2170 - acc: 0.91 - ETA: 41s - loss: 0.2172 - acc: 0.91 - ETA: 41s - loss: 0.2176 - acc: 0.91 - ETA: 41s - loss: 0.2179 - acc: 0.91 - ETA: 41s - loss: 0.2183 - acc: 0.91 - ETA: 41s - loss: 0.2183 - acc: 0.91 - ETA: 41s - loss: 0.2183 - acc: 0.91 - ETA: 40s - loss: 0.2182 - acc: 0.91 - ETA: 40s - loss: 0.2182 - acc: 0.91 - ETA: 40s - loss: 0.2179 - acc: 0.91 - ETA: 40s - loss: 0.2178 - acc: 0.91 - ETA: 40s - loss: 0.2176 - acc: 0.91 - ETA: 40s - loss: 0.2176 - acc: 0.91 - ETA: 39s - loss: 0.2176 - acc: 0.91 - ETA: 39s - loss: 0.2174 - acc: 0.91 - ETA: 39s - loss: 0.2176 - acc: 0.91 - ETA: 39s - loss: 0.2179 - acc: 0.91 - ETA: 39s - loss: 0.2180 - acc: 0.91 - ETA: 39s - loss: 0.2183 - acc: 0.91 - ETA: 38s - loss: 0.2182 - acc: 0.91 - ETA: 38s - loss: 0.2186 - acc: 0.91 - ETA: 38s - loss: 0.2185 - acc: 0.91 - ETA: 38s - loss: 0.2187 - acc: 0.91 - ETA: 38s - loss: 0.2184 - acc: 0.91 - ETA: 38s - loss: 0.2184 - acc: 0.91 - ETA: 37s - loss: 0.2185 - acc: 0.91 - ETA: 37s - loss: 0.2184 - acc: 0.91 - ETA: 37s - loss: 0.2187 - acc: 0.91 - ETA: 37s - loss: 0.2187 - acc: 0.91 - ETA: 37s - loss: 0.2185 - acc: 0.91 - ETA: 37s - loss: 0.2184 - acc: 0.91 - ETA: 36s - loss: 0.2185 - acc: 0.91 - ETA: 36s - loss: 0.2187 - acc: 0.91 - ETA: 36s - loss: 0.2187 - acc: 0.91 - ETA: 36s - loss: 0.2189 - acc: 0.91 - ETA: 36s - loss: 0.2187 - acc: 0.91 - ETA: 36s - loss: 0.2186 - acc: 0.91 - ETA: 35s - loss: 0.2186 - acc: 0.91 - ETA: 35s - loss: 0.2188 - acc: 0.91 - ETA: 35s - loss: 0.2190 - acc: 0.91 - ETA: 35s - loss: 0.2190 - acc: 0.91 - ETA: 35s - loss: 0.2193 - acc: 0.91 - ETA: 35s - loss: 0.2196 - acc: 0.91 - ETA: 34s - loss: 0.2197 - acc: 0.91 - ETA: 34s - loss: 0.2197 - acc: 0.91 - ETA: 34s - loss: 0.2198 - acc: 0.91 - ETA: 34s - loss: 0.2197 - acc: 0.91 - ETA: 34s - loss: 0.2197 - acc: 0.91 - ETA: 34s - loss: 0.2197 - acc: 0.91 - ETA: 33s - loss: 0.2195 - acc: 0.91 - ETA: 33s - loss: 0.2196 - acc: 0.91 - ETA: 33s - loss: 0.2194 - acc: 0.91 - ETA: 33s - loss: 0.2195 - acc: 0.91 - ETA: 33s - loss: 0.2196 - acc: 0.91 - ETA: 33s - loss: 0.2195 - acc: 0.91 - ETA: 32s - loss: 0.2196 - acc: 0.91 - ETA: 32s - loss: 0.2196 - acc: 0.91 - ETA: 32s - loss: 0.2194 - acc: 0.91 - ETA: 32s - loss: 0.2194 - acc: 0.91 - ETA: 32s - loss: 0.2194 - acc: 0.91 - ETA: 32s - loss: 0.2198 - acc: 0.91 - ETA: 32s - loss: 0.2197 - acc: 0.91 - ETA: 31s - loss: 0.2195 - acc: 0.91 - ETA: 31s - loss: 0.2197 - acc: 0.91 - ETA: 31s - loss: 0.2197 - acc: 0.91 - ETA: 31s - loss: 0.2197 - acc: 0.91 - ETA: 31s - loss: 0.2197 - acc: 0.91 - ETA: 31s - loss: 0.2200 - acc: 0.91 - ETA: 30s - loss: 0.2202 - acc: 0.91 - ETA: 30s - loss: 0.2201 - acc: 0.91 - ETA: 30s - loss: 0.2202 - acc: 0.91 - ETA: 30s - loss: 0.2204 - acc: 0.91 - ETA: 30s - loss: 0.2205 - acc: 0.91 - ETA: 30s - loss: 0.2204 - acc: 0.91 - ETA: 29s - loss: 0.2203 - acc: 0.91 - ETA: 29s - loss: 0.2203 - acc: 0.91 - ETA: 29s - loss: 0.2202 - acc: 0.91 - ETA: 29s - loss: 0.2202 - acc: 0.91 - ETA: 29s - loss: 0.2205 - acc: 0.91 - ETA: 29s - loss: 0.2206 - acc: 0.91 - ETA: 28s - loss: 0.2205 - acc: 0.91 - ETA: 28s - loss: 0.2207 - acc: 0.91 - ETA: 28s - loss: 0.2208 - acc: 0.91 - ETA: 28s - loss: 0.2208 - acc: 0.91 - ETA: 28s - loss: 0.2206 - acc: 0.91 - ETA: 28s - loss: 0.2205 - acc: 0.91 - ETA: 28s - loss: 0.2206 - acc: 0.91 - ETA: 27s - loss: 0.2207 - acc: 0.91 - ETA: 27s - loss: 0.2206 - acc: 0.91 - ETA: 27s - loss: 0.2205 - acc: 0.91 - ETA: 27s - loss: 0.2205 - acc: 0.91 - ETA: 27s - loss: 0.2205 - acc: 0.91 - ETA: 27s - loss: 0.2206 - acc: 0.91 - ETA: 26s - loss: 0.2208 - acc: 0.91 - ETA: 26s - loss: 0.2207 - acc: 0.9152"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - ETA: 26s - loss: 0.2206 - acc: 0.91 - ETA: 26s - loss: 0.2206 - acc: 0.91 - ETA: 26s - loss: 0.2206 - acc: 0.91 - ETA: 26s - loss: 0.2210 - acc: 0.91 - ETA: 25s - loss: 0.2211 - acc: 0.91 - ETA: 25s - loss: 0.2209 - acc: 0.91 - ETA: 25s - loss: 0.2210 - acc: 0.91 - ETA: 25s - loss: 0.2210 - acc: 0.91 - ETA: 25s - loss: 0.2210 - acc: 0.91 - ETA: 25s - loss: 0.2209 - acc: 0.91 - ETA: 24s - loss: 0.2209 - acc: 0.91 - ETA: 24s - loss: 0.2208 - acc: 0.91 - ETA: 24s - loss: 0.2208 - acc: 0.91 - ETA: 24s - loss: 0.2210 - acc: 0.91 - ETA: 24s - loss: 0.2212 - acc: 0.91 - ETA: 24s - loss: 0.2213 - acc: 0.91 - ETA: 23s - loss: 0.2215 - acc: 0.91 - ETA: 23s - loss: 0.2215 - acc: 0.91 - ETA: 23s - loss: 0.2215 - acc: 0.91 - ETA: 23s - loss: 0.2218 - acc: 0.91 - ETA: 23s - loss: 0.2217 - acc: 0.91 - ETA: 23s - loss: 0.2216 - acc: 0.91 - ETA: 22s - loss: 0.2218 - acc: 0.91 - ETA: 22s - loss: 0.2217 - acc: 0.91 - ETA: 22s - loss: 0.2220 - acc: 0.91 - ETA: 22s - loss: 0.2221 - acc: 0.91 - ETA: 22s - loss: 0.2219 - acc: 0.91 - ETA: 22s - loss: 0.2218 - acc: 0.91 - ETA: 21s - loss: 0.2221 - acc: 0.91 - ETA: 21s - loss: 0.2221 - acc: 0.91 - ETA: 21s - loss: 0.2221 - acc: 0.91 - ETA: 21s - loss: 0.2220 - acc: 0.91 - ETA: 21s - loss: 0.2221 - acc: 0.91 - ETA: 21s - loss: 0.2222 - acc: 0.91 - ETA: 20s - loss: 0.2221 - acc: 0.91 - ETA: 20s - loss: 0.2221 - acc: 0.91 - ETA: 20s - loss: 0.2220 - acc: 0.91 - ETA: 20s - loss: 0.2218 - acc: 0.91 - ETA: 20s - loss: 0.2220 - acc: 0.91 - ETA: 20s - loss: 0.2221 - acc: 0.91 - ETA: 19s - loss: 0.2220 - acc: 0.91 - ETA: 19s - loss: 0.2224 - acc: 0.91 - ETA: 19s - loss: 0.2224 - acc: 0.91 - ETA: 19s - loss: 0.2223 - acc: 0.91 - ETA: 19s - loss: 0.2222 - acc: 0.91 - ETA: 19s - loss: 0.2221 - acc: 0.91 - ETA: 18s - loss: 0.2221 - acc: 0.91 - ETA: 18s - loss: 0.2220 - acc: 0.91 - ETA: 18s - loss: 0.2223 - acc: 0.91 - ETA: 18s - loss: 0.2224 - acc: 0.91 - ETA: 18s - loss: 0.2225 - acc: 0.91 - ETA: 18s - loss: 0.2226 - acc: 0.91 - ETA: 17s - loss: 0.2226 - acc: 0.91 - ETA: 17s - loss: 0.2227 - acc: 0.91 - ETA: 17s - loss: 0.2229 - acc: 0.91 - ETA: 17s - loss: 0.2229 - acc: 0.91 - ETA: 17s - loss: 0.2228 - acc: 0.91 - ETA: 17s - loss: 0.2228 - acc: 0.91 - ETA: 16s - loss: 0.2227 - acc: 0.91 - ETA: 16s - loss: 0.2227 - acc: 0.91 - ETA: 16s - loss: 0.2227 - acc: 0.91 - ETA: 16s - loss: 0.2230 - acc: 0.91 - ETA: 16s - loss: 0.2229 - acc: 0.91 - ETA: 16s - loss: 0.2230 - acc: 0.91 - ETA: 15s - loss: 0.2231 - acc: 0.91 - ETA: 15s - loss: 0.2231 - acc: 0.91 - ETA: 15s - loss: 0.2230 - acc: 0.91 - ETA: 15s - loss: 0.2229 - acc: 0.91 - ETA: 15s - loss: 0.2228 - acc: 0.91 - ETA: 15s - loss: 0.2228 - acc: 0.91 - ETA: 14s - loss: 0.2228 - acc: 0.91 - ETA: 14s - loss: 0.2227 - acc: 0.91 - ETA: 14s - loss: 0.2226 - acc: 0.91 - ETA: 14s - loss: 0.2225 - acc: 0.91 - ETA: 14s - loss: 0.2224 - acc: 0.91 - ETA: 14s - loss: 0.2224 - acc: 0.91 - ETA: 13s - loss: 0.2224 - acc: 0.91 - ETA: 13s - loss: 0.2223 - acc: 0.91 - ETA: 13s - loss: 0.2224 - acc: 0.91 - ETA: 13s - loss: 0.2223 - acc: 0.91 - ETA: 13s - loss: 0.2224 - acc: 0.91 - ETA: 13s - loss: 0.2225 - acc: 0.91 - ETA: 12s - loss: 0.2223 - acc: 0.91 - ETA: 12s - loss: 0.2225 - acc: 0.91 - ETA: 12s - loss: 0.2226 - acc: 0.91 - ETA: 12s - loss: 0.2225 - acc: 0.91 - ETA: 12s - loss: 0.2224 - acc: 0.91 - ETA: 11s - loss: 0.2223 - acc: 0.91 - ETA: 11s - loss: 0.2223 - acc: 0.91 - ETA: 11s - loss: 0.2223 - acc: 0.91 - ETA: 11s - loss: 0.2223 - acc: 0.91 - ETA: 11s - loss: 0.2223 - acc: 0.91 - ETA: 11s - loss: 0.2222 - acc: 0.91 - ETA: 10s - loss: 0.2222 - acc: 0.91 - ETA: 10s - loss: 0.2221 - acc: 0.91 - ETA: 10s - loss: 0.2220 - acc: 0.91 - ETA: 10s - loss: 0.2221 - acc: 0.91 - ETA: 10s - loss: 0.2219 - acc: 0.91 - ETA: 10s - loss: 0.2220 - acc: 0.91 - ETA: 9s - loss: 0.2219 - acc: 0.9140 - ETA: 9s - loss: 0.2220 - acc: 0.914 - ETA: 9s - loss: 0.2221 - acc: 0.913 - ETA: 9s - loss: 0.2224 - acc: 0.913 - ETA: 9s - loss: 0.2222 - acc: 0.913 - ETA: 9s - loss: 0.2223 - acc: 0.913 - ETA: 8s - loss: 0.2223 - acc: 0.913 - ETA: 8s - loss: 0.2222 - acc: 0.913 - ETA: 8s - loss: 0.2222 - acc: 0.913 - ETA: 8s - loss: 0.2223 - acc: 0.913 - ETA: 8s - loss: 0.2225 - acc: 0.913 - ETA: 8s - loss: 0.2224 - acc: 0.913 - ETA: 7s - loss: 0.2224 - acc: 0.913 - ETA: 7s - loss: 0.2224 - acc: 0.913 - ETA: 7s - loss: 0.2226 - acc: 0.913 - ETA: 7s - loss: 0.2225 - acc: 0.913 - ETA: 7s - loss: 0.2226 - acc: 0.913 - ETA: 7s - loss: 0.2227 - acc: 0.913 - ETA: 6s - loss: 0.2228 - acc: 0.913 - ETA: 6s - loss: 0.2230 - acc: 0.913 - ETA: 6s - loss: 0.2230 - acc: 0.913 - ETA: 6s - loss: 0.2232 - acc: 0.913 - ETA: 6s - loss: 0.2232 - acc: 0.913 - ETA: 6s - loss: 0.2232 - acc: 0.913 - ETA: 5s - loss: 0.2231 - acc: 0.913 - ETA: 5s - loss: 0.2230 - acc: 0.913 - ETA: 5s - loss: 0.2230 - acc: 0.913 - ETA: 5s - loss: 0.2229 - acc: 0.913 - ETA: 5s - loss: 0.2229 - acc: 0.913 - ETA: 5s - loss: 0.2229 - acc: 0.913 - ETA: 4s - loss: 0.2228 - acc: 0.913 - ETA: 4s - loss: 0.2227 - acc: 0.913 - ETA: 4s - loss: 0.2226 - acc: 0.913 - ETA: 4s - loss: 0.2226 - acc: 0.913 - ETA: 4s - loss: 0.2225 - acc: 0.913 - ETA: 4s - loss: 0.2229 - acc: 0.913 - ETA: 3s - loss: 0.2231 - acc: 0.913 - ETA: 3s - loss: 0.2234 - acc: 0.913 - ETA: 3s - loss: 0.2236 - acc: 0.913 - ETA: 3s - loss: 0.2237 - acc: 0.913 - ETA: 3s - loss: 0.2237 - acc: 0.912 - ETA: 3s - loss: 0.2239 - acc: 0.912 - ETA: 2s - loss: 0.2241 - acc: 0.912 - ETA: 2s - loss: 0.2241 - acc: 0.912 - ETA: 2s - loss: 0.2240 - acc: 0.912 - ETA: 2s - loss: 0.2240 - acc: 0.912 - ETA: 2s - loss: 0.2240 - acc: 0.912 - ETA: 2s - loss: 0.2241 - acc: 0.912 - ETA: 1s - loss: 0.2241 - acc: 0.912 - ETA: 1s - loss: 0.2242 - acc: 0.912 - ETA: 1s - loss: 0.2243 - acc: 0.912 - ETA: 1s - loss: 0.2243 - acc: 0.912 - ETA: 1s - loss: 0.2243 - acc: 0.912 - ETA: 1s - loss: 0.2244 - acc: 0.912 - ETA: 0s - loss: 0.2245 - acc: 0.912 - ETA: 0s - loss: 0.2246 - acc: 0.912 - ETA: 0s - loss: 0.2245 - acc: 0.912 - ETA: 0s - loss: 0.2246 - acc: 0.912 - ETA: 0s - loss: 0.2248 - acc: 0.912 - ETA: 0s - loss: 0.2247 - acc: 0.912 - 151s 6ms/step - loss: 0.2246 - acc: 0.9126 - val_loss: 0.3890 - val_acc: 0.8468\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6528/25000 [======>.......................] - ETA: 2:05 - loss: 0.0988 - acc: 1.000 - ETA: 2:03 - loss: 0.0723 - acc: 1.000 - ETA: 2:03 - loss: 0.0668 - acc: 0.989 - ETA: 2:03 - loss: 0.1107 - acc: 0.968 - ETA: 2:05 - loss: 0.1002 - acc: 0.975 - ETA: 2:04 - loss: 0.1088 - acc: 0.968 - ETA: 2:04 - loss: 0.1061 - acc: 0.968 - ETA: 2:03 - loss: 0.1166 - acc: 0.968 - ETA: 2:03 - loss: 0.1548 - acc: 0.958 - ETA: 2:02 - loss: 0.1439 - acc: 0.962 - ETA: 2:02 - loss: 0.1361 - acc: 0.965 - ETA: 2:02 - loss: 0.1346 - acc: 0.963 - ETA: 2:02 - loss: 0.1298 - acc: 0.966 - ETA: 2:02 - loss: 0.1611 - acc: 0.950 - ETA: 2:02 - loss: 0.1590 - acc: 0.952 - ETA: 2:02 - loss: 0.1549 - acc: 0.953 - ETA: 2:01 - loss: 0.1486 - acc: 0.955 - ETA: 2:01 - loss: 0.1449 - acc: 0.958 - ETA: 2:01 - loss: 0.1427 - acc: 0.958 - ETA: 2:01 - loss: 0.1382 - acc: 0.959 - ETA: 2:01 - loss: 0.1332 - acc: 0.961 - ETA: 2:01 - loss: 0.1313 - acc: 0.963 - ETA: 2:01 - loss: 0.1304 - acc: 0.962 - ETA: 2:00 - loss: 0.1278 - acc: 0.963 - ETA: 2:00 - loss: 0.1278 - acc: 0.962 - ETA: 2:00 - loss: 0.1309 - acc: 0.961 - ETA: 2:00 - loss: 0.1294 - acc: 0.961 - ETA: 2:00 - loss: 0.1261 - acc: 0.963 - ETA: 1:59 - loss: 0.1310 - acc: 0.961 - ETA: 1:59 - loss: 0.1286 - acc: 0.962 - ETA: 1:59 - loss: 0.1258 - acc: 0.963 - ETA: 1:59 - loss: 0.1244 - acc: 0.963 - ETA: 1:59 - loss: 0.1232 - acc: 0.964 - ETA: 1:59 - loss: 0.1206 - acc: 0.965 - ETA: 1:59 - loss: 0.1181 - acc: 0.966 - ETA: 1:58 - loss: 0.1167 - acc: 0.966 - ETA: 1:58 - loss: 0.1181 - acc: 0.965 - ETA: 1:58 - loss: 0.1164 - acc: 0.966 - ETA: 1:58 - loss: 0.1165 - acc: 0.966 - ETA: 1:58 - loss: 0.1190 - acc: 0.964 - ETA: 1:57 - loss: 0.1175 - acc: 0.965 - ETA: 1:57 - loss: 0.1155 - acc: 0.966 - ETA: 1:57 - loss: 0.1132 - acc: 0.967 - ETA: 1:57 - loss: 0.1123 - acc: 0.967 - ETA: 1:57 - loss: 0.1102 - acc: 0.968 - ETA: 1:57 - loss: 0.1097 - acc: 0.968 - ETA: 1:56 - loss: 0.1087 - acc: 0.968 - ETA: 1:56 - loss: 0.1082 - acc: 0.968 - ETA: 1:56 - loss: 0.1092 - acc: 0.968 - ETA: 1:56 - loss: 0.1082 - acc: 0.968 - ETA: 1:56 - loss: 0.1072 - acc: 0.969 - ETA: 1:56 - loss: 0.1065 - acc: 0.969 - ETA: 1:56 - loss: 0.1052 - acc: 0.969 - ETA: 1:56 - loss: 0.1049 - acc: 0.969 - ETA: 1:56 - loss: 0.1043 - acc: 0.970 - ETA: 1:56 - loss: 0.1052 - acc: 0.969 - ETA: 1:56 - loss: 0.1040 - acc: 0.970 - ETA: 1:56 - loss: 0.1041 - acc: 0.969 - ETA: 1:56 - loss: 0.1026 - acc: 0.970 - ETA: 1:57 - loss: 0.1022 - acc: 0.970 - ETA: 1:57 - loss: 0.1016 - acc: 0.970 - ETA: 1:57 - loss: 0.1009 - acc: 0.970 - ETA: 1:57 - loss: 0.1003 - acc: 0.970 - ETA: 1:57 - loss: 0.1001 - acc: 0.971 - ETA: 1:57 - loss: 0.1006 - acc: 0.970 - ETA: 1:57 - loss: 0.0997 - acc: 0.971 - ETA: 1:57 - loss: 0.0997 - acc: 0.971 - ETA: 1:57 - loss: 0.0994 - acc: 0.970 - ETA: 1:57 - loss: 0.0989 - acc: 0.970 - ETA: 1:58 - loss: 0.0982 - acc: 0.970 - ETA: 1:58 - loss: 0.0982 - acc: 0.970 - ETA: 1:58 - loss: 0.0996 - acc: 0.969 - ETA: 1:59 - loss: 0.1004 - acc: 0.969 - ETA: 1:59 - loss: 0.1006 - acc: 0.969 - ETA: 1:59 - loss: 0.1042 - acc: 0.968 - ETA: 1:59 - loss: 0.1061 - acc: 0.967 - ETA: 1:59 - loss: 0.1071 - acc: 0.967 - ETA: 2:00 - loss: 0.1071 - acc: 0.967 - ETA: 2:00 - loss: 0.1079 - acc: 0.966 - ETA: 2:00 - loss: 0.1102 - acc: 0.966 - ETA: 2:00 - loss: 0.1103 - acc: 0.966 - ETA: 2:00 - loss: 0.1112 - acc: 0.965 - ETA: 2:00 - loss: 0.1115 - acc: 0.965 - ETA: 1:59 - loss: 0.1111 - acc: 0.966 - ETA: 2:00 - loss: 0.1106 - acc: 0.966 - ETA: 1:59 - loss: 0.1110 - acc: 0.965 - ETA: 1:59 - loss: 0.1108 - acc: 0.965 - ETA: 1:59 - loss: 0.1126 - acc: 0.964 - ETA: 1:59 - loss: 0.1127 - acc: 0.964 - ETA: 1:59 - loss: 0.1131 - acc: 0.964 - ETA: 1:59 - loss: 0.1156 - acc: 0.963 - ETA: 1:58 - loss: 0.1167 - acc: 0.963 - ETA: 1:58 - loss: 0.1158 - acc: 0.963 - ETA: 1:58 - loss: 0.1154 - acc: 0.964 - ETA: 1:58 - loss: 0.1151 - acc: 0.964 - ETA: 1:57 - loss: 0.1144 - acc: 0.964 - ETA: 1:57 - loss: 0.1140 - acc: 0.964 - ETA: 1:57 - loss: 0.1136 - acc: 0.964 - ETA: 1:57 - loss: 0.1133 - acc: 0.964 - ETA: 1:57 - loss: 0.1144 - acc: 0.964 - ETA: 1:56 - loss: 0.1143 - acc: 0.963 - ETA: 1:56 - loss: 0.1147 - acc: 0.962 - ETA: 1:56 - loss: 0.1139 - acc: 0.963 - ETA: 1:56 - loss: 0.1130 - acc: 0.963 - ETA: 1:56 - loss: 0.1126 - acc: 0.963 - ETA: 1:56 - loss: 0.1121 - acc: 0.963 - ETA: 1:56 - loss: 0.1120 - acc: 0.963 - ETA: 1:56 - loss: 0.1119 - acc: 0.963 - ETA: 1:56 - loss: 0.1121 - acc: 0.963 - ETA: 1:56 - loss: 0.1114 - acc: 0.963 - ETA: 1:56 - loss: 0.1139 - acc: 0.962 - ETA: 1:55 - loss: 0.1130 - acc: 0.963 - ETA: 1:55 - loss: 0.1129 - acc: 0.962 - ETA: 1:55 - loss: 0.1135 - acc: 0.962 - ETA: 1:55 - loss: 0.1147 - acc: 0.962 - ETA: 1:55 - loss: 0.1144 - acc: 0.962 - ETA: 1:55 - loss: 0.1144 - acc: 0.962 - ETA: 1:54 - loss: 0.1139 - acc: 0.962 - ETA: 1:54 - loss: 0.1151 - acc: 0.962 - ETA: 1:54 - loss: 0.1145 - acc: 0.962 - ETA: 1:54 - loss: 0.1144 - acc: 0.962 - ETA: 1:54 - loss: 0.1141 - acc: 0.962 - ETA: 1:54 - loss: 0.1148 - acc: 0.962 - ETA: 1:53 - loss: 0.1145 - acc: 0.963 - ETA: 1:53 - loss: 0.1145 - acc: 0.963 - ETA: 1:53 - loss: 0.1144 - acc: 0.963 - ETA: 1:53 - loss: 0.1150 - acc: 0.962 - ETA: 1:53 - loss: 0.1155 - acc: 0.962 - ETA: 1:53 - loss: 0.1150 - acc: 0.962 - ETA: 1:52 - loss: 0.1146 - acc: 0.962 - ETA: 1:52 - loss: 0.1141 - acc: 0.963 - ETA: 1:52 - loss: 0.1138 - acc: 0.963 - ETA: 1:52 - loss: 0.1140 - acc: 0.962 - ETA: 1:52 - loss: 0.1138 - acc: 0.962 - ETA: 1:51 - loss: 0.1155 - acc: 0.962 - ETA: 1:51 - loss: 0.1154 - acc: 0.962 - ETA: 1:51 - loss: 0.1155 - acc: 0.962 - ETA: 1:51 - loss: 0.1150 - acc: 0.962 - ETA: 1:51 - loss: 0.1153 - acc: 0.962 - ETA: 1:51 - loss: 0.1150 - acc: 0.962 - ETA: 1:50 - loss: 0.1154 - acc: 0.961 - ETA: 1:50 - loss: 0.1151 - acc: 0.961 - ETA: 1:50 - loss: 0.1147 - acc: 0.962 - ETA: 1:50 - loss: 0.1144 - acc: 0.961 - ETA: 1:50 - loss: 0.1139 - acc: 0.962 - ETA: 1:50 - loss: 0.1136 - acc: 0.962 - ETA: 1:50 - loss: 0.1131 - acc: 0.962 - ETA: 1:49 - loss: 0.1136 - acc: 0.962 - ETA: 1:49 - loss: 0.1133 - acc: 0.962 - ETA: 1:49 - loss: 0.1128 - acc: 0.962 - ETA: 1:49 - loss: 0.1130 - acc: 0.962 - ETA: 1:49 - loss: 0.1132 - acc: 0.962 - ETA: 1:49 - loss: 0.1132 - acc: 0.962 - ETA: 1:49 - loss: 0.1131 - acc: 0.961 - ETA: 1:49 - loss: 0.1128 - acc: 0.962 - ETA: 1:48 - loss: 0.1130 - acc: 0.961 - ETA: 1:48 - loss: 0.1137 - acc: 0.961 - ETA: 1:48 - loss: 0.1135 - acc: 0.961 - ETA: 1:48 - loss: 0.1135 - acc: 0.961 - ETA: 1:48 - loss: 0.1140 - acc: 0.961 - ETA: 1:48 - loss: 0.1138 - acc: 0.961 - ETA: 1:47 - loss: 0.1136 - acc: 0.961 - ETA: 1:47 - loss: 0.1134 - acc: 0.961 - ETA: 1:47 - loss: 0.1132 - acc: 0.961 - ETA: 1:47 - loss: 0.1135 - acc: 0.961 - ETA: 1:47 - loss: 0.1133 - acc: 0.962 - ETA: 1:47 - loss: 0.1134 - acc: 0.961 - ETA: 1:47 - loss: 0.1132 - acc: 0.961 - ETA: 1:46 - loss: 0.1132 - acc: 0.961 - ETA: 1:46 - loss: 0.1133 - acc: 0.961 - ETA: 1:46 - loss: 0.1131 - acc: 0.961 - ETA: 1:46 - loss: 0.1129 - acc: 0.961 - ETA: 1:46 - loss: 0.1125 - acc: 0.961 - ETA: 1:46 - loss: 0.1120 - acc: 0.962 - ETA: 1:45 - loss: 0.1118 - acc: 0.962 - ETA: 1:45 - loss: 0.1121 - acc: 0.962 - ETA: 1:45 - loss: 0.1122 - acc: 0.962 - ETA: 1:45 - loss: 0.1118 - acc: 0.962 - ETA: 1:45 - loss: 0.1120 - acc: 0.962 - ETA: 1:44 - loss: 0.1118 - acc: 0.962 - ETA: 1:44 - loss: 0.1113 - acc: 0.962 - ETA: 1:44 - loss: 0.1110 - acc: 0.962 - ETA: 1:44 - loss: 0.1106 - acc: 0.962 - ETA: 1:44 - loss: 0.1116 - acc: 0.962 - ETA: 1:43 - loss: 0.1117 - acc: 0.962 - ETA: 1:43 - loss: 0.1114 - acc: 0.962 - ETA: 1:43 - loss: 0.1123 - acc: 0.961 - ETA: 1:43 - loss: 0.1122 - acc: 0.961 - ETA: 1:43 - loss: 0.1139 - acc: 0.961 - ETA: 1:42 - loss: 0.1139 - acc: 0.961 - ETA: 1:42 - loss: 0.1139 - acc: 0.961 - ETA: 1:42 - loss: 0.1139 - acc: 0.961 - ETA: 1:42 - loss: 0.1140 - acc: 0.961 - ETA: 1:42 - loss: 0.1138 - acc: 0.961 - ETA: 1:41 - loss: 0.1139 - acc: 0.961 - ETA: 1:41 - loss: 0.1139 - acc: 0.961 - ETA: 1:41 - loss: 0.1142 - acc: 0.961 - ETA: 1:41 - loss: 0.1150 - acc: 0.961 - ETA: 1:41 - loss: 0.1147 - acc: 0.961 - ETA: 1:40 - loss: 0.1145 - acc: 0.961 - ETA: 1:40 - loss: 0.1145 - acc: 0.961 - ETA: 1:40 - loss: 0.1145 - acc: 0.961 - ETA: 1:40 - loss: 0.1147 - acc: 0.961 - ETA: 1:40 - loss: 0.1146 - acc: 0.961413056/25000 [==============>...............] - ETA: 1:40 - loss: 0.1143 - acc: 0.961 - ETA: 1:40 - loss: 0.1140 - acc: 0.961 - ETA: 1:39 - loss: 0.1138 - acc: 0.961 - ETA: 1:39 - loss: 0.1140 - acc: 0.961 - ETA: 1:39 - loss: 0.1139 - acc: 0.961 - ETA: 1:39 - loss: 0.1137 - acc: 0.961 - ETA: 1:39 - loss: 0.1132 - acc: 0.961 - ETA: 1:39 - loss: 0.1129 - acc: 0.962 - ETA: 1:39 - loss: 0.1134 - acc: 0.961 - ETA: 1:38 - loss: 0.1143 - acc: 0.961 - ETA: 1:38 - loss: 0.1146 - acc: 0.961 - ETA: 1:38 - loss: 0.1157 - acc: 0.961 - ETA: 1:38 - loss: 0.1152 - acc: 0.961 - ETA: 1:38 - loss: 0.1152 - acc: 0.961 - ETA: 1:38 - loss: 0.1148 - acc: 0.961 - ETA: 1:37 - loss: 0.1153 - acc: 0.961 - ETA: 1:37 - loss: 0.1159 - acc: 0.961 - ETA: 1:37 - loss: 0.1158 - acc: 0.961 - ETA: 1:37 - loss: 0.1159 - acc: 0.961 - ETA: 1:37 - loss: 0.1164 - acc: 0.960 - ETA: 1:36 - loss: 0.1161 - acc: 0.961 - ETA: 1:36 - loss: 0.1161 - acc: 0.961 - ETA: 1:36 - loss: 0.1160 - acc: 0.961 - ETA: 1:36 - loss: 0.1162 - acc: 0.960 - ETA: 1:36 - loss: 0.1163 - acc: 0.960 - ETA: 1:36 - loss: 0.1170 - acc: 0.960 - ETA: 1:36 - loss: 0.1174 - acc: 0.960 - ETA: 1:36 - loss: 0.1173 - acc: 0.960 - ETA: 1:36 - loss: 0.1181 - acc: 0.960 - ETA: 1:35 - loss: 0.1179 - acc: 0.960 - ETA: 1:35 - loss: 0.1182 - acc: 0.960 - ETA: 1:35 - loss: 0.1180 - acc: 0.960 - ETA: 1:35 - loss: 0.1178 - acc: 0.960 - ETA: 1:35 - loss: 0.1179 - acc: 0.960 - ETA: 1:34 - loss: 0.1179 - acc: 0.960 - ETA: 1:34 - loss: 0.1175 - acc: 0.960 - ETA: 1:34 - loss: 0.1172 - acc: 0.960 - ETA: 1:34 - loss: 0.1176 - acc: 0.960 - ETA: 1:34 - loss: 0.1176 - acc: 0.960 - ETA: 1:34 - loss: 0.1174 - acc: 0.960 - ETA: 1:33 - loss: 0.1175 - acc: 0.960 - ETA: 1:33 - loss: 0.1175 - acc: 0.960 - ETA: 1:33 - loss: 0.1172 - acc: 0.960 - ETA: 1:33 - loss: 0.1177 - acc: 0.960 - ETA: 1:33 - loss: 0.1176 - acc: 0.960 - ETA: 1:33 - loss: 0.1176 - acc: 0.960 - ETA: 1:33 - loss: 0.1182 - acc: 0.960 - ETA: 1:33 - loss: 0.1184 - acc: 0.959 - ETA: 1:32 - loss: 0.1184 - acc: 0.960 - ETA: 1:32 - loss: 0.1182 - acc: 0.960 - ETA: 1:32 - loss: 0.1181 - acc: 0.960 - ETA: 1:32 - loss: 0.1179 - acc: 0.960 - ETA: 1:32 - loss: 0.1180 - acc: 0.960 - ETA: 1:32 - loss: 0.1179 - acc: 0.959 - ETA: 1:32 - loss: 0.1178 - acc: 0.959 - ETA: 1:32 - loss: 0.1176 - acc: 0.960 - ETA: 1:32 - loss: 0.1177 - acc: 0.960 - ETA: 1:32 - loss: 0.1182 - acc: 0.959 - ETA: 1:32 - loss: 0.1178 - acc: 0.960 - ETA: 1:32 - loss: 0.1185 - acc: 0.959 - ETA: 1:32 - loss: 0.1185 - acc: 0.959 - ETA: 1:32 - loss: 0.1183 - acc: 0.960 - ETA: 1:32 - loss: 0.1181 - acc: 0.960 - ETA: 1:32 - loss: 0.1180 - acc: 0.960 - ETA: 1:32 - loss: 0.1182 - acc: 0.960 - ETA: 1:31 - loss: 0.1182 - acc: 0.960 - ETA: 1:31 - loss: 0.1179 - acc: 0.960 - ETA: 1:31 - loss: 0.1177 - acc: 0.960 - ETA: 1:31 - loss: 0.1176 - acc: 0.960 - ETA: 1:31 - loss: 0.1174 - acc: 0.960 - ETA: 1:31 - loss: 0.1171 - acc: 0.960 - ETA: 1:31 - loss: 0.1171 - acc: 0.960 - ETA: 1:31 - loss: 0.1170 - acc: 0.960 - ETA: 1:31 - loss: 0.1167 - acc: 0.960 - ETA: 1:30 - loss: 0.1164 - acc: 0.960 - ETA: 1:30 - loss: 0.1164 - acc: 0.960 - ETA: 1:30 - loss: 0.1166 - acc: 0.960 - ETA: 1:30 - loss: 0.1166 - acc: 0.960 - ETA: 1:30 - loss: 0.1166 - acc: 0.960 - ETA: 1:30 - loss: 0.1166 - acc: 0.960 - ETA: 1:30 - loss: 0.1168 - acc: 0.960 - ETA: 1:30 - loss: 0.1167 - acc: 0.960 - ETA: 1:29 - loss: 0.1168 - acc: 0.960 - ETA: 1:29 - loss: 0.1166 - acc: 0.960 - ETA: 1:29 - loss: 0.1163 - acc: 0.960 - ETA: 1:29 - loss: 0.1165 - acc: 0.960 - ETA: 1:29 - loss: 0.1166 - acc: 0.960 - ETA: 1:28 - loss: 0.1167 - acc: 0.960 - ETA: 1:28 - loss: 0.1164 - acc: 0.960 - ETA: 1:28 - loss: 0.1161 - acc: 0.960 - ETA: 1:28 - loss: 0.1169 - acc: 0.960 - ETA: 1:28 - loss: 0.1170 - acc: 0.960 - ETA: 1:27 - loss: 0.1167 - acc: 0.960 - ETA: 1:27 - loss: 0.1167 - acc: 0.960 - ETA: 1:27 - loss: 0.1165 - acc: 0.960 - ETA: 1:27 - loss: 0.1164 - acc: 0.960 - ETA: 1:27 - loss: 0.1164 - acc: 0.960 - ETA: 1:27 - loss: 0.1169 - acc: 0.960 - ETA: 1:26 - loss: 0.1172 - acc: 0.960 - ETA: 1:26 - loss: 0.1171 - acc: 0.960 - ETA: 1:26 - loss: 0.1174 - acc: 0.959 - ETA: 1:26 - loss: 0.1174 - acc: 0.960 - ETA: 1:26 - loss: 0.1174 - acc: 0.959 - ETA: 1:25 - loss: 0.1174 - acc: 0.959 - ETA: 1:25 - loss: 0.1175 - acc: 0.959 - ETA: 1:25 - loss: 0.1176 - acc: 0.959 - ETA: 1:25 - loss: 0.1175 - acc: 0.959 - ETA: 1:24 - loss: 0.1175 - acc: 0.959 - ETA: 1:24 - loss: 0.1176 - acc: 0.959 - ETA: 1:24 - loss: 0.1173 - acc: 0.959 - ETA: 1:24 - loss: 0.1175 - acc: 0.959 - ETA: 1:24 - loss: 0.1176 - acc: 0.959 - ETA: 1:24 - loss: 0.1176 - acc: 0.959 - ETA: 1:23 - loss: 0.1177 - acc: 0.959 - ETA: 1:23 - loss: 0.1175 - acc: 0.959 - ETA: 1:23 - loss: 0.1174 - acc: 0.959 - ETA: 1:23 - loss: 0.1176 - acc: 0.959 - ETA: 1:23 - loss: 0.1174 - acc: 0.959 - ETA: 1:22 - loss: 0.1175 - acc: 0.959 - ETA: 1:22 - loss: 0.1173 - acc: 0.959 - ETA: 1:22 - loss: 0.1171 - acc: 0.959 - ETA: 1:22 - loss: 0.1170 - acc: 0.959 - ETA: 1:22 - loss: 0.1169 - acc: 0.959 - ETA: 1:21 - loss: 0.1167 - acc: 0.959 - ETA: 1:21 - loss: 0.1167 - acc: 0.959 - ETA: 1:21 - loss: 0.1166 - acc: 0.959 - ETA: 1:21 - loss: 0.1163 - acc: 0.959 - ETA: 1:21 - loss: 0.1166 - acc: 0.959 - ETA: 1:20 - loss: 0.1169 - acc: 0.959 - ETA: 1:20 - loss: 0.1167 - acc: 0.959 - ETA: 1:20 - loss: 0.1167 - acc: 0.959 - ETA: 1:20 - loss: 0.1167 - acc: 0.959 - ETA: 1:20 - loss: 0.1170 - acc: 0.959 - ETA: 1:19 - loss: 0.1170 - acc: 0.959 - ETA: 1:19 - loss: 0.1175 - acc: 0.959 - ETA: 1:19 - loss: 0.1173 - acc: 0.959 - ETA: 1:19 - loss: 0.1177 - acc: 0.959 - ETA: 1:19 - loss: 0.1178 - acc: 0.959 - ETA: 1:18 - loss: 0.1181 - acc: 0.958 - ETA: 1:18 - loss: 0.1179 - acc: 0.958 - ETA: 1:18 - loss: 0.1183 - acc: 0.958 - ETA: 1:18 - loss: 0.1181 - acc: 0.959 - ETA: 1:18 - loss: 0.1179 - acc: 0.959 - ETA: 1:18 - loss: 0.1178 - acc: 0.959 - ETA: 1:17 - loss: 0.1178 - acc: 0.959 - ETA: 1:17 - loss: 0.1183 - acc: 0.959 - ETA: 1:17 - loss: 0.1184 - acc: 0.959 - ETA: 1:17 - loss: 0.1182 - acc: 0.959 - ETA: 1:17 - loss: 0.1188 - acc: 0.959 - ETA: 1:16 - loss: 0.1186 - acc: 0.959 - ETA: 1:16 - loss: 0.1185 - acc: 0.959 - ETA: 1:16 - loss: 0.1183 - acc: 0.959 - ETA: 1:16 - loss: 0.1182 - acc: 0.959 - ETA: 1:16 - loss: 0.1183 - acc: 0.959 - ETA: 1:15 - loss: 0.1183 - acc: 0.959 - ETA: 1:15 - loss: 0.1181 - acc: 0.959 - ETA: 1:15 - loss: 0.1178 - acc: 0.959 - ETA: 1:15 - loss: 0.1178 - acc: 0.959 - ETA: 1:15 - loss: 0.1177 - acc: 0.959 - ETA: 1:14 - loss: 0.1177 - acc: 0.959 - ETA: 1:14 - loss: 0.1175 - acc: 0.959 - ETA: 1:14 - loss: 0.1172 - acc: 0.959 - ETA: 1:14 - loss: 0.1174 - acc: 0.959 - ETA: 1:13 - loss: 0.1175 - acc: 0.959 - ETA: 1:13 - loss: 0.1173 - acc: 0.959 - ETA: 1:13 - loss: 0.1177 - acc: 0.959 - ETA: 1:13 - loss: 0.1176 - acc: 0.959 - ETA: 1:13 - loss: 0.1175 - acc: 0.959 - ETA: 1:13 - loss: 0.1175 - acc: 0.959 - ETA: 1:12 - loss: 0.1175 - acc: 0.959 - ETA: 1:12 - loss: 0.1175 - acc: 0.959 - ETA: 1:12 - loss: 0.1174 - acc: 0.959 - ETA: 1:12 - loss: 0.1173 - acc: 0.959 - ETA: 1:12 - loss: 0.1173 - acc: 0.959 - ETA: 1:12 - loss: 0.1173 - acc: 0.959 - ETA: 1:12 - loss: 0.1172 - acc: 0.959 - ETA: 1:12 - loss: 0.1171 - acc: 0.959 - ETA: 1:12 - loss: 0.1171 - acc: 0.959 - ETA: 1:11 - loss: 0.1168 - acc: 0.959 - ETA: 1:11 - loss: 0.1168 - acc: 0.959 - ETA: 1:11 - loss: 0.1168 - acc: 0.959 - ETA: 1:11 - loss: 0.1165 - acc: 0.959 - ETA: 1:11 - loss: 0.1168 - acc: 0.959 - ETA: 1:11 - loss: 0.1168 - acc: 0.959 - ETA: 1:11 - loss: 0.1170 - acc: 0.959 - ETA: 1:11 - loss: 0.1169 - acc: 0.959 - ETA: 1:11 - loss: 0.1172 - acc: 0.959 - ETA: 1:10 - loss: 0.1171 - acc: 0.959 - ETA: 1:10 - loss: 0.1172 - acc: 0.959 - ETA: 1:10 - loss: 0.1176 - acc: 0.959 - ETA: 1:10 - loss: 0.1180 - acc: 0.959 - ETA: 1:10 - loss: 0.1179 - acc: 0.959 - ETA: 1:10 - loss: 0.1176 - acc: 0.959 - ETA: 1:10 - loss: 0.1177 - acc: 0.959 - ETA: 1:10 - loss: 0.1180 - acc: 0.959 - ETA: 1:09 - loss: 0.1180 - acc: 0.959 - ETA: 1:09 - loss: 0.1179 - acc: 0.959 - ETA: 1:09 - loss: 0.1177 - acc: 0.959 - ETA: 1:09 - loss: 0.1176 - acc: 0.959 - ETA: 1:09 - loss: 0.1179 - acc: 0.959 - ETA: 1:09 - loss: 0.1181 - acc: 0.959 - ETA: 1:09 - loss: 0.1179 - acc: 0.959 - ETA: 1:09 - loss: 0.1180 - acc: 0.959 - ETA: 1:09 - loss: 0.1183 - acc: 0.959119840/25000 [======================>.......] - ETA: 1:08 - loss: 0.1181 - acc: 0.959 - ETA: 1:08 - loss: 0.1180 - acc: 0.959 - ETA: 1:08 - loss: 0.1177 - acc: 0.959 - ETA: 1:08 - loss: 0.1177 - acc: 0.959 - ETA: 1:08 - loss: 0.1179 - acc: 0.959 - ETA: 1:08 - loss: 0.1178 - acc: 0.959 - ETA: 1:08 - loss: 0.1178 - acc: 0.959 - ETA: 1:08 - loss: 0.1177 - acc: 0.959 - ETA: 1:07 - loss: 0.1177 - acc: 0.959 - ETA: 1:07 - loss: 0.1176 - acc: 0.959 - ETA: 1:07 - loss: 0.1177 - acc: 0.959 - ETA: 1:07 - loss: 0.1179 - acc: 0.959 - ETA: 1:07 - loss: 0.1176 - acc: 0.959 - ETA: 1:07 - loss: 0.1179 - acc: 0.959 - ETA: 1:07 - loss: 0.1181 - acc: 0.958 - ETA: 1:06 - loss: 0.1184 - acc: 0.958 - ETA: 1:06 - loss: 0.1183 - acc: 0.959 - ETA: 1:06 - loss: 0.1183 - acc: 0.958 - ETA: 1:06 - loss: 0.1182 - acc: 0.958 - ETA: 1:06 - loss: 0.1181 - acc: 0.959 - ETA: 1:06 - loss: 0.1180 - acc: 0.959 - ETA: 1:06 - loss: 0.1179 - acc: 0.959 - ETA: 1:05 - loss: 0.1177 - acc: 0.959 - ETA: 1:05 - loss: 0.1178 - acc: 0.959 - ETA: 1:05 - loss: 0.1180 - acc: 0.959 - ETA: 1:05 - loss: 0.1182 - acc: 0.958 - ETA: 1:05 - loss: 0.1185 - acc: 0.958 - ETA: 1:05 - loss: 0.1187 - acc: 0.958 - ETA: 1:04 - loss: 0.1187 - acc: 0.958 - ETA: 1:04 - loss: 0.1190 - acc: 0.958 - ETA: 1:04 - loss: 0.1190 - acc: 0.958 - ETA: 1:04 - loss: 0.1193 - acc: 0.958 - ETA: 1:04 - loss: 0.1194 - acc: 0.958 - ETA: 1:04 - loss: 0.1195 - acc: 0.958 - ETA: 1:04 - loss: 0.1197 - acc: 0.958 - ETA: 1:03 - loss: 0.1199 - acc: 0.958 - ETA: 1:03 - loss: 0.1202 - acc: 0.958 - ETA: 1:03 - loss: 0.1204 - acc: 0.957 - ETA: 1:03 - loss: 0.1204 - acc: 0.957 - ETA: 1:03 - loss: 0.1206 - acc: 0.957 - ETA: 1:03 - loss: 0.1206 - acc: 0.957 - ETA: 1:02 - loss: 0.1207 - acc: 0.957 - ETA: 1:02 - loss: 0.1206 - acc: 0.957 - ETA: 1:02 - loss: 0.1206 - acc: 0.957 - ETA: 1:02 - loss: 0.1205 - acc: 0.957 - ETA: 1:02 - loss: 0.1204 - acc: 0.957 - ETA: 1:02 - loss: 0.1207 - acc: 0.957 - ETA: 1:02 - loss: 0.1207 - acc: 0.957 - ETA: 1:01 - loss: 0.1207 - acc: 0.957 - ETA: 1:01 - loss: 0.1207 - acc: 0.957 - ETA: 1:01 - loss: 0.1207 - acc: 0.957 - ETA: 1:01 - loss: 0.1208 - acc: 0.957 - ETA: 1:01 - loss: 0.1206 - acc: 0.957 - ETA: 1:01 - loss: 0.1205 - acc: 0.957 - ETA: 1:00 - loss: 0.1204 - acc: 0.957 - ETA: 1:00 - loss: 0.1203 - acc: 0.957 - ETA: 1:00 - loss: 0.1204 - acc: 0.957 - ETA: 1:00 - loss: 0.1205 - acc: 0.957 - ETA: 1:00 - loss: 0.1206 - acc: 0.957 - ETA: 1:00 - loss: 0.1206 - acc: 0.957 - ETA: 59s - loss: 0.1206 - acc: 0.957 - ETA: 59s - loss: 0.1207 - acc: 0.95 - ETA: 59s - loss: 0.1205 - acc: 0.95 - ETA: 59s - loss: 0.1206 - acc: 0.95 - ETA: 59s - loss: 0.1205 - acc: 0.95 - ETA: 59s - loss: 0.1203 - acc: 0.95 - ETA: 58s - loss: 0.1201 - acc: 0.95 - ETA: 58s - loss: 0.1200 - acc: 0.95 - ETA: 58s - loss: 0.1202 - acc: 0.95 - ETA: 58s - loss: 0.1201 - acc: 0.95 - ETA: 58s - loss: 0.1204 - acc: 0.95 - ETA: 58s - loss: 0.1205 - acc: 0.95 - ETA: 58s - loss: 0.1207 - acc: 0.95 - ETA: 57s - loss: 0.1206 - acc: 0.95 - ETA: 57s - loss: 0.1206 - acc: 0.95 - ETA: 57s - loss: 0.1205 - acc: 0.95 - ETA: 57s - loss: 0.1206 - acc: 0.95 - ETA: 57s - loss: 0.1207 - acc: 0.95 - ETA: 57s - loss: 0.1209 - acc: 0.95 - ETA: 56s - loss: 0.1207 - acc: 0.95 - ETA: 56s - loss: 0.1210 - acc: 0.95 - ETA: 56s - loss: 0.1209 - acc: 0.95 - ETA: 56s - loss: 0.1208 - acc: 0.95 - ETA: 56s - loss: 0.1210 - acc: 0.95 - ETA: 56s - loss: 0.1211 - acc: 0.95 - ETA: 55s - loss: 0.1210 - acc: 0.95 - ETA: 55s - loss: 0.1212 - acc: 0.95 - ETA: 55s - loss: 0.1210 - acc: 0.95 - ETA: 55s - loss: 0.1210 - acc: 0.95 - ETA: 55s - loss: 0.1213 - acc: 0.95 - ETA: 54s - loss: 0.1213 - acc: 0.95 - ETA: 54s - loss: 0.1213 - acc: 0.95 - ETA: 54s - loss: 0.1211 - acc: 0.95 - ETA: 54s - loss: 0.1210 - acc: 0.95 - ETA: 54s - loss: 0.1212 - acc: 0.95 - ETA: 54s - loss: 0.1215 - acc: 0.95 - ETA: 53s - loss: 0.1214 - acc: 0.95 - ETA: 53s - loss: 0.1213 - acc: 0.95 - ETA: 53s - loss: 0.1213 - acc: 0.95 - ETA: 53s - loss: 0.1215 - acc: 0.95 - ETA: 53s - loss: 0.1219 - acc: 0.95 - ETA: 53s - loss: 0.1218 - acc: 0.95 - ETA: 52s - loss: 0.1216 - acc: 0.95 - ETA: 52s - loss: 0.1221 - acc: 0.95 - ETA: 52s - loss: 0.1222 - acc: 0.95 - ETA: 52s - loss: 0.1222 - acc: 0.95 - ETA: 52s - loss: 0.1221 - acc: 0.95 - ETA: 52s - loss: 0.1222 - acc: 0.95 - ETA: 51s - loss: 0.1222 - acc: 0.95 - ETA: 51s - loss: 0.1225 - acc: 0.95 - ETA: 51s - loss: 0.1227 - acc: 0.95 - ETA: 51s - loss: 0.1226 - acc: 0.95 - ETA: 51s - loss: 0.1227 - acc: 0.95 - ETA: 51s - loss: 0.1225 - acc: 0.95 - ETA: 50s - loss: 0.1226 - acc: 0.95 - ETA: 50s - loss: 0.1227 - acc: 0.95 - ETA: 50s - loss: 0.1226 - acc: 0.95 - ETA: 50s - loss: 0.1224 - acc: 0.95 - ETA: 50s - loss: 0.1225 - acc: 0.95 - ETA: 49s - loss: 0.1224 - acc: 0.95 - ETA: 49s - loss: 0.1225 - acc: 0.95 - ETA: 49s - loss: 0.1225 - acc: 0.95 - ETA: 49s - loss: 0.1223 - acc: 0.95 - ETA: 49s - loss: 0.1223 - acc: 0.95 - ETA: 49s - loss: 0.1222 - acc: 0.95 - ETA: 48s - loss: 0.1223 - acc: 0.95 - ETA: 48s - loss: 0.1225 - acc: 0.95 - ETA: 48s - loss: 0.1224 - acc: 0.95 - ETA: 48s - loss: 0.1224 - acc: 0.95 - ETA: 48s - loss: 0.1223 - acc: 0.95 - ETA: 47s - loss: 0.1221 - acc: 0.95 - ETA: 47s - loss: 0.1220 - acc: 0.95 - ETA: 47s - loss: 0.1220 - acc: 0.95 - ETA: 47s - loss: 0.1223 - acc: 0.95 - ETA: 47s - loss: 0.1222 - acc: 0.95 - ETA: 47s - loss: 0.1222 - acc: 0.95 - ETA: 46s - loss: 0.1223 - acc: 0.95 - ETA: 46s - loss: 0.1223 - acc: 0.95 - ETA: 46s - loss: 0.1224 - acc: 0.95 - ETA: 46s - loss: 0.1224 - acc: 0.95 - ETA: 46s - loss: 0.1223 - acc: 0.95 - ETA: 45s - loss: 0.1223 - acc: 0.95 - ETA: 45s - loss: 0.1223 - acc: 0.95 - ETA: 45s - loss: 0.1222 - acc: 0.95 - ETA: 45s - loss: 0.1221 - acc: 0.95 - ETA: 45s - loss: 0.1220 - acc: 0.95 - ETA: 44s - loss: 0.1220 - acc: 0.95 - ETA: 44s - loss: 0.1220 - acc: 0.95 - ETA: 44s - loss: 0.1221 - acc: 0.95 - ETA: 44s - loss: 0.1220 - acc: 0.95 - ETA: 44s - loss: 0.1222 - acc: 0.95 - ETA: 44s - loss: 0.1222 - acc: 0.95 - ETA: 43s - loss: 0.1221 - acc: 0.95 - ETA: 43s - loss: 0.1220 - acc: 0.95 - ETA: 43s - loss: 0.1218 - acc: 0.95 - ETA: 43s - loss: 0.1217 - acc: 0.95 - ETA: 43s - loss: 0.1217 - acc: 0.95 - ETA: 42s - loss: 0.1217 - acc: 0.95 - ETA: 42s - loss: 0.1217 - acc: 0.95 - ETA: 42s - loss: 0.1217 - acc: 0.95 - ETA: 42s - loss: 0.1217 - acc: 0.95 - ETA: 42s - loss: 0.1217 - acc: 0.95 - ETA: 42s - loss: 0.1217 - acc: 0.95 - ETA: 41s - loss: 0.1215 - acc: 0.95 - ETA: 41s - loss: 0.1215 - acc: 0.95 - ETA: 41s - loss: 0.1217 - acc: 0.95 - ETA: 41s - loss: 0.1218 - acc: 0.95 - ETA: 41s - loss: 0.1218 - acc: 0.95 - ETA: 40s - loss: 0.1216 - acc: 0.95 - ETA: 40s - loss: 0.1216 - acc: 0.95 - ETA: 40s - loss: 0.1215 - acc: 0.95 - ETA: 40s - loss: 0.1216 - acc: 0.95 - ETA: 40s - loss: 0.1216 - acc: 0.95 - ETA: 39s - loss: 0.1215 - acc: 0.95 - ETA: 39s - loss: 0.1216 - acc: 0.95 - ETA: 39s - loss: 0.1216 - acc: 0.95 - ETA: 39s - loss: 0.1219 - acc: 0.95 - ETA: 39s - loss: 0.1220 - acc: 0.95 - ETA: 39s - loss: 0.1220 - acc: 0.95 - ETA: 38s - loss: 0.1220 - acc: 0.95 - ETA: 38s - loss: 0.1221 - acc: 0.95 - ETA: 38s - loss: 0.1220 - acc: 0.95 - ETA: 38s - loss: 0.1222 - acc: 0.95 - ETA: 38s - loss: 0.1224 - acc: 0.95 - ETA: 37s - loss: 0.1223 - acc: 0.95 - ETA: 37s - loss: 0.1225 - acc: 0.95 - ETA: 37s - loss: 0.1227 - acc: 0.95 - ETA: 37s - loss: 0.1229 - acc: 0.95 - ETA: 37s - loss: 0.1229 - acc: 0.95 - ETA: 36s - loss: 0.1228 - acc: 0.95 - ETA: 36s - loss: 0.1230 - acc: 0.95 - ETA: 36s - loss: 0.1231 - acc: 0.95 - ETA: 36s - loss: 0.1231 - acc: 0.95 - ETA: 36s - loss: 0.1231 - acc: 0.95 - ETA: 35s - loss: 0.1231 - acc: 0.95 - ETA: 35s - loss: 0.1231 - acc: 0.95 - ETA: 35s - loss: 0.1230 - acc: 0.95 - ETA: 35s - loss: 0.1230 - acc: 0.95 - ETA: 35s - loss: 0.1230 - acc: 0.95 - ETA: 35s - loss: 0.1229 - acc: 0.95 - ETA: 34s - loss: 0.1228 - acc: 0.95 - ETA: 34s - loss: 0.1230 - acc: 0.95 - ETA: 34s - loss: 0.1228 - acc: 0.95 - ETA: 34s - loss: 0.1227 - acc: 0.95 - ETA: 34s - loss: 0.1227 - acc: 0.95 - ETA: 33s - loss: 0.1227 - acc: 0.95 - ETA: 33s - loss: 0.1229 - acc: 0.95 - ETA: 33s - loss: 0.1228 - acc: 0.95 - ETA: 33s - loss: 0.1229 - acc: 0.95 - ETA: 33s - loss: 0.1228 - acc: 0.95 - ETA: 32s - loss: 0.1230 - acc: 0.95 - ETA: 32s - loss: 0.1230 - acc: 0.9569"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - ETA: 32s - loss: 0.1232 - acc: 0.95 - ETA: 32s - loss: 0.1231 - acc: 0.95 - ETA: 32s - loss: 0.1231 - acc: 0.95 - ETA: 31s - loss: 0.1230 - acc: 0.95 - ETA: 31s - loss: 0.1231 - acc: 0.95 - ETA: 31s - loss: 0.1231 - acc: 0.95 - ETA: 31s - loss: 0.1232 - acc: 0.95 - ETA: 31s - loss: 0.1232 - acc: 0.95 - ETA: 30s - loss: 0.1233 - acc: 0.95 - ETA: 30s - loss: 0.1233 - acc: 0.95 - ETA: 30s - loss: 0.1235 - acc: 0.95 - ETA: 30s - loss: 0.1236 - acc: 0.95 - ETA: 30s - loss: 0.1237 - acc: 0.95 - ETA: 29s - loss: 0.1236 - acc: 0.95 - ETA: 29s - loss: 0.1238 - acc: 0.95 - ETA: 29s - loss: 0.1237 - acc: 0.95 - ETA: 29s - loss: 0.1236 - acc: 0.95 - ETA: 29s - loss: 0.1236 - acc: 0.95 - ETA: 29s - loss: 0.1234 - acc: 0.95 - ETA: 28s - loss: 0.1234 - acc: 0.95 - ETA: 28s - loss: 0.1236 - acc: 0.95 - ETA: 28s - loss: 0.1239 - acc: 0.95 - ETA: 28s - loss: 0.1238 - acc: 0.95 - ETA: 28s - loss: 0.1240 - acc: 0.95 - ETA: 27s - loss: 0.1240 - acc: 0.95 - ETA: 27s - loss: 0.1239 - acc: 0.95 - ETA: 27s - loss: 0.1241 - acc: 0.95 - ETA: 27s - loss: 0.1241 - acc: 0.95 - ETA: 27s - loss: 0.1241 - acc: 0.95 - ETA: 26s - loss: 0.1240 - acc: 0.95 - ETA: 26s - loss: 0.1241 - acc: 0.95 - ETA: 26s - loss: 0.1241 - acc: 0.95 - ETA: 26s - loss: 0.1241 - acc: 0.95 - ETA: 26s - loss: 0.1240 - acc: 0.95 - ETA: 25s - loss: 0.1240 - acc: 0.95 - ETA: 25s - loss: 0.1242 - acc: 0.95 - ETA: 25s - loss: 0.1242 - acc: 0.95 - ETA: 25s - loss: 0.1241 - acc: 0.95 - ETA: 25s - loss: 0.1240 - acc: 0.95 - ETA: 24s - loss: 0.1238 - acc: 0.95 - ETA: 24s - loss: 0.1239 - acc: 0.95 - ETA: 24s - loss: 0.1242 - acc: 0.95 - ETA: 24s - loss: 0.1241 - acc: 0.95 - ETA: 24s - loss: 0.1244 - acc: 0.95 - ETA: 23s - loss: 0.1244 - acc: 0.95 - ETA: 23s - loss: 0.1245 - acc: 0.95 - ETA: 23s - loss: 0.1244 - acc: 0.95 - ETA: 23s - loss: 0.1247 - acc: 0.95 - ETA: 23s - loss: 0.1249 - acc: 0.95 - ETA: 22s - loss: 0.1251 - acc: 0.95 - ETA: 22s - loss: 0.1256 - acc: 0.95 - ETA: 22s - loss: 0.1256 - acc: 0.95 - ETA: 22s - loss: 0.1257 - acc: 0.95 - ETA: 22s - loss: 0.1259 - acc: 0.95 - ETA: 21s - loss: 0.1261 - acc: 0.95 - ETA: 21s - loss: 0.1261 - acc: 0.95 - ETA: 21s - loss: 0.1261 - acc: 0.95 - ETA: 21s - loss: 0.1262 - acc: 0.95 - ETA: 21s - loss: 0.1262 - acc: 0.95 - ETA: 20s - loss: 0.1262 - acc: 0.95 - ETA: 20s - loss: 0.1265 - acc: 0.95 - ETA: 20s - loss: 0.1266 - acc: 0.95 - ETA: 20s - loss: 0.1267 - acc: 0.95 - ETA: 20s - loss: 0.1269 - acc: 0.95 - ETA: 19s - loss: 0.1268 - acc: 0.95 - ETA: 19s - loss: 0.1268 - acc: 0.95 - ETA: 19s - loss: 0.1268 - acc: 0.95 - ETA: 19s - loss: 0.1270 - acc: 0.95 - ETA: 19s - loss: 0.1272 - acc: 0.95 - ETA: 18s - loss: 0.1273 - acc: 0.95 - ETA: 18s - loss: 0.1274 - acc: 0.95 - ETA: 18s - loss: 0.1274 - acc: 0.95 - ETA: 18s - loss: 0.1274 - acc: 0.95 - ETA: 17s - loss: 0.1274 - acc: 0.95 - ETA: 17s - loss: 0.1276 - acc: 0.95 - ETA: 17s - loss: 0.1276 - acc: 0.95 - ETA: 17s - loss: 0.1277 - acc: 0.95 - ETA: 17s - loss: 0.1276 - acc: 0.95 - ETA: 16s - loss: 0.1276 - acc: 0.95 - ETA: 16s - loss: 0.1277 - acc: 0.95 - ETA: 16s - loss: 0.1276 - acc: 0.95 - ETA: 16s - loss: 0.1278 - acc: 0.95 - ETA: 16s - loss: 0.1278 - acc: 0.95 - ETA: 15s - loss: 0.1277 - acc: 0.95 - ETA: 15s - loss: 0.1276 - acc: 0.95 - ETA: 15s - loss: 0.1276 - acc: 0.95 - ETA: 15s - loss: 0.1276 - acc: 0.95 - ETA: 15s - loss: 0.1275 - acc: 0.95 - ETA: 14s - loss: 0.1276 - acc: 0.95 - ETA: 14s - loss: 0.1275 - acc: 0.95 - ETA: 14s - loss: 0.1276 - acc: 0.95 - ETA: 14s - loss: 0.1277 - acc: 0.95 - ETA: 14s - loss: 0.1278 - acc: 0.95 - ETA: 13s - loss: 0.1278 - acc: 0.95 - ETA: 13s - loss: 0.1277 - acc: 0.95 - ETA: 13s - loss: 0.1278 - acc: 0.95 - ETA: 13s - loss: 0.1279 - acc: 0.95 - ETA: 13s - loss: 0.1280 - acc: 0.95 - ETA: 12s - loss: 0.1280 - acc: 0.95 - ETA: 12s - loss: 0.1279 - acc: 0.95 - ETA: 12s - loss: 0.1280 - acc: 0.95 - ETA: 12s - loss: 0.1279 - acc: 0.95 - ETA: 12s - loss: 0.1280 - acc: 0.95 - ETA: 11s - loss: 0.1281 - acc: 0.95 - ETA: 11s - loss: 0.1280 - acc: 0.95 - ETA: 11s - loss: 0.1282 - acc: 0.95 - ETA: 11s - loss: 0.1281 - acc: 0.95 - ETA: 11s - loss: 0.1282 - acc: 0.95 - ETA: 10s - loss: 0.1282 - acc: 0.95 - ETA: 10s - loss: 0.1281 - acc: 0.95 - ETA: 10s - loss: 0.1280 - acc: 0.95 - ETA: 10s - loss: 0.1280 - acc: 0.95 - ETA: 10s - loss: 0.1281 - acc: 0.95 - ETA: 9s - loss: 0.1281 - acc: 0.9535 - ETA: 9s - loss: 0.1282 - acc: 0.953 - ETA: 9s - loss: 0.1283 - acc: 0.953 - ETA: 9s - loss: 0.1282 - acc: 0.953 - ETA: 8s - loss: 0.1285 - acc: 0.953 - ETA: 8s - loss: 0.1284 - acc: 0.953 - ETA: 8s - loss: 0.1284 - acc: 0.953 - ETA: 8s - loss: 0.1284 - acc: 0.953 - ETA: 8s - loss: 0.1285 - acc: 0.953 - ETA: 7s - loss: 0.1285 - acc: 0.953 - ETA: 7s - loss: 0.1285 - acc: 0.953 - ETA: 7s - loss: 0.1284 - acc: 0.953 - ETA: 7s - loss: 0.1282 - acc: 0.953 - ETA: 7s - loss: 0.1282 - acc: 0.953 - ETA: 6s - loss: 0.1281 - acc: 0.953 - ETA: 6s - loss: 0.1280 - acc: 0.953 - ETA: 6s - loss: 0.1280 - acc: 0.953 - ETA: 6s - loss: 0.1280 - acc: 0.953 - ETA: 6s - loss: 0.1282 - acc: 0.953 - ETA: 5s - loss: 0.1284 - acc: 0.953 - ETA: 5s - loss: 0.1284 - acc: 0.953 - ETA: 5s - loss: 0.1286 - acc: 0.953 - ETA: 5s - loss: 0.1286 - acc: 0.953 - ETA: 5s - loss: 0.1286 - acc: 0.953 - ETA: 4s - loss: 0.1288 - acc: 0.953 - ETA: 4s - loss: 0.1288 - acc: 0.953 - ETA: 4s - loss: 0.1288 - acc: 0.953 - ETA: 4s - loss: 0.1288 - acc: 0.953 - ETA: 3s - loss: 0.1289 - acc: 0.953 - ETA: 3s - loss: 0.1287 - acc: 0.953 - ETA: 3s - loss: 0.1287 - acc: 0.953 - ETA: 3s - loss: 0.1288 - acc: 0.953 - ETA: 3s - loss: 0.1288 - acc: 0.953 - ETA: 2s - loss: 0.1287 - acc: 0.953 - ETA: 2s - loss: 0.1286 - acc: 0.953 - ETA: 2s - loss: 0.1286 - acc: 0.953 - ETA: 2s - loss: 0.1286 - acc: 0.953 - ETA: 2s - loss: 0.1286 - acc: 0.953 - ETA: 1s - loss: 0.1288 - acc: 0.953 - ETA: 1s - loss: 0.1288 - acc: 0.952 - ETA: 1s - loss: 0.1288 - acc: 0.952 - ETA: 1s - loss: 0.1287 - acc: 0.952 - ETA: 1s - loss: 0.1290 - acc: 0.952 - ETA: 0s - loss: 0.1289 - acc: 0.952 - ETA: 0s - loss: 0.1289 - acc: 0.952 - ETA: 0s - loss: 0.1291 - acc: 0.952 - ETA: 0s - loss: 0.1291 - acc: 0.952 - ETA: 0s - loss: 0.1290 - acc: 0.952 - 188s 8ms/step - loss: 0.1290 - acc: 0.9529 - val_loss: 0.4639 - val_acc: 0.8422\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6528/25000 [======>.......................] - ETA: 2:45 - loss: 0.0691 - acc: 0.968 - ETA: 2:43 - loss: 0.0463 - acc: 0.984 - ETA: 2:46 - loss: 0.0443 - acc: 0.989 - ETA: 2:48 - loss: 0.0442 - acc: 0.992 - ETA: 2:53 - loss: 0.0480 - acc: 0.987 - ETA: 2:57 - loss: 0.0575 - acc: 0.979 - ETA: 2:58 - loss: 0.0596 - acc: 0.977 - ETA: 2:59 - loss: 0.0604 - acc: 0.976 - ETA: 2:59 - loss: 0.0558 - acc: 0.979 - ETA: 2:59 - loss: 0.0516 - acc: 0.981 - ETA: 2:59 - loss: 0.0498 - acc: 0.983 - ETA: 3:00 - loss: 0.0593 - acc: 0.976 - ETA: 3:00 - loss: 0.0560 - acc: 0.978 - ETA: 2:59 - loss: 0.0632 - acc: 0.977 - ETA: 2:59 - loss: 0.0620 - acc: 0.979 - ETA: 2:59 - loss: 0.0611 - acc: 0.980 - ETA: 2:59 - loss: 0.0585 - acc: 0.981 - ETA: 2:59 - loss: 0.0592 - acc: 0.980 - ETA: 2:58 - loss: 0.0570 - acc: 0.981 - ETA: 2:58 - loss: 0.0558 - acc: 0.982 - ETA: 2:58 - loss: 0.0541 - acc: 0.983 - ETA: 2:56 - loss: 0.0526 - acc: 0.984 - ETA: 2:55 - loss: 0.0518 - acc: 0.985 - ETA: 2:55 - loss: 0.0519 - acc: 0.984 - ETA: 2:54 - loss: 0.0504 - acc: 0.985 - ETA: 2:53 - loss: 0.0505 - acc: 0.984 - ETA: 2:51 - loss: 0.0517 - acc: 0.983 - ETA: 2:50 - loss: 0.0504 - acc: 0.984 - ETA: 2:49 - loss: 0.0498 - acc: 0.984 - ETA: 2:47 - loss: 0.0486 - acc: 0.985 - ETA: 2:46 - loss: 0.0568 - acc: 0.983 - ETA: 2:45 - loss: 0.0567 - acc: 0.983 - ETA: 2:44 - loss: 0.0606 - acc: 0.983 - ETA: 2:43 - loss: 0.0598 - acc: 0.983 - ETA: 2:42 - loss: 0.0598 - acc: 0.983 - ETA: 2:41 - loss: 0.0597 - acc: 0.983 - ETA: 2:41 - loss: 0.0591 - acc: 0.984 - ETA: 2:40 - loss: 0.0583 - acc: 0.984 - ETA: 2:41 - loss: 0.0581 - acc: 0.984 - ETA: 2:40 - loss: 0.0590 - acc: 0.982 - ETA: 2:40 - loss: 0.0577 - acc: 0.983 - ETA: 2:41 - loss: 0.0570 - acc: 0.983 - ETA: 2:41 - loss: 0.0602 - acc: 0.982 - ETA: 2:40 - loss: 0.0594 - acc: 0.983 - ETA: 2:41 - loss: 0.0586 - acc: 0.983 - ETA: 2:41 - loss: 0.0576 - acc: 0.983 - ETA: 2:41 - loss: 0.0568 - acc: 0.984 - ETA: 2:41 - loss: 0.0563 - acc: 0.984 - ETA: 2:41 - loss: 0.0555 - acc: 0.984 - ETA: 2:41 - loss: 0.0552 - acc: 0.985 - ETA: 2:41 - loss: 0.0547 - acc: 0.985 - ETA: 2:41 - loss: 0.0538 - acc: 0.985 - ETA: 2:40 - loss: 0.0529 - acc: 0.985 - ETA: 2:40 - loss: 0.0543 - acc: 0.985 - ETA: 2:40 - loss: 0.0584 - acc: 0.984 - ETA: 2:40 - loss: 0.0574 - acc: 0.984 - ETA: 2:40 - loss: 0.0578 - acc: 0.984 - ETA: 2:40 - loss: 0.0575 - acc: 0.983 - ETA: 2:40 - loss: 0.0568 - acc: 0.984 - ETA: 2:40 - loss: 0.0629 - acc: 0.982 - ETA: 2:41 - loss: 0.0621 - acc: 0.982 - ETA: 2:41 - loss: 0.0612 - acc: 0.982 - ETA: 2:41 - loss: 0.0610 - acc: 0.983 - ETA: 2:41 - loss: 0.0606 - acc: 0.983 - ETA: 2:40 - loss: 0.0601 - acc: 0.983 - ETA: 2:40 - loss: 0.0598 - acc: 0.983 - ETA: 2:40 - loss: 0.0600 - acc: 0.984 - ETA: 2:40 - loss: 0.0597 - acc: 0.984 - ETA: 2:40 - loss: 0.0601 - acc: 0.984 - ETA: 2:40 - loss: 0.0598 - acc: 0.984 - ETA: 2:39 - loss: 0.0599 - acc: 0.984 - ETA: 2:39 - loss: 0.0627 - acc: 0.983 - ETA: 2:39 - loss: 0.0623 - acc: 0.983 - ETA: 2:40 - loss: 0.0630 - acc: 0.983 - ETA: 2:40 - loss: 0.0636 - acc: 0.982 - ETA: 2:39 - loss: 0.0632 - acc: 0.983 - ETA: 2:39 - loss: 0.0626 - acc: 0.983 - ETA: 2:39 - loss: 0.0626 - acc: 0.983 - ETA: 2:39 - loss: 0.0624 - acc: 0.983 - ETA: 2:39 - loss: 0.0620 - acc: 0.983 - ETA: 2:39 - loss: 0.0628 - acc: 0.982 - ETA: 2:39 - loss: 0.0626 - acc: 0.982 - ETA: 2:39 - loss: 0.0619 - acc: 0.982 - ETA: 2:39 - loss: 0.0614 - acc: 0.982 - ETA: 2:38 - loss: 0.0608 - acc: 0.983 - ETA: 2:38 - loss: 0.0605 - acc: 0.983 - ETA: 2:38 - loss: 0.0602 - acc: 0.983 - ETA: 2:38 - loss: 0.0616 - acc: 0.983 - ETA: 2:38 - loss: 0.0611 - acc: 0.983 - ETA: 2:38 - loss: 0.0627 - acc: 0.982 - ETA: 2:38 - loss: 0.0625 - acc: 0.982 - ETA: 2:38 - loss: 0.0623 - acc: 0.982 - ETA: 2:38 - loss: 0.0617 - acc: 0.982 - ETA: 2:37 - loss: 0.0625 - acc: 0.982 - ETA: 2:37 - loss: 0.0622 - acc: 0.982 - ETA: 2:37 - loss: 0.0622 - acc: 0.982 - ETA: 2:37 - loss: 0.0618 - acc: 0.982 - ETA: 2:37 - loss: 0.0613 - acc: 0.983 - ETA: 2:36 - loss: 0.0609 - acc: 0.983 - ETA: 2:36 - loss: 0.0606 - acc: 0.983 - ETA: 2:36 - loss: 0.0603 - acc: 0.983 - ETA: 2:36 - loss: 0.0599 - acc: 0.983 - ETA: 2:36 - loss: 0.0596 - acc: 0.983 - ETA: 2:35 - loss: 0.0593 - acc: 0.983 - ETA: 2:36 - loss: 0.0595 - acc: 0.983 - ETA: 2:35 - loss: 0.0601 - acc: 0.983 - ETA: 2:35 - loss: 0.0605 - acc: 0.983 - ETA: 2:35 - loss: 0.0602 - acc: 0.983 - ETA: 2:35 - loss: 0.0611 - acc: 0.983 - ETA: 2:35 - loss: 0.0610 - acc: 0.983 - ETA: 2:35 - loss: 0.0604 - acc: 0.983 - ETA: 2:34 - loss: 0.0603 - acc: 0.983 - ETA: 2:34 - loss: 0.0601 - acc: 0.983 - ETA: 2:34 - loss: 0.0599 - acc: 0.983 - ETA: 2:34 - loss: 0.0595 - acc: 0.984 - ETA: 2:34 - loss: 0.0596 - acc: 0.983 - ETA: 2:34 - loss: 0.0593 - acc: 0.984 - ETA: 2:33 - loss: 0.0589 - acc: 0.984 - ETA: 2:33 - loss: 0.0590 - acc: 0.984 - ETA: 2:33 - loss: 0.0586 - acc: 0.984 - ETA: 2:33 - loss: 0.0602 - acc: 0.983 - ETA: 2:33 - loss: 0.0600 - acc: 0.983 - ETA: 2:32 - loss: 0.0599 - acc: 0.983 - ETA: 2:32 - loss: 0.0596 - acc: 0.983 - ETA: 2:32 - loss: 0.0596 - acc: 0.983 - ETA: 2:32 - loss: 0.0594 - acc: 0.983 - ETA: 2:32 - loss: 0.0597 - acc: 0.983 - ETA: 2:31 - loss: 0.0601 - acc: 0.983 - ETA: 2:31 - loss: 0.0599 - acc: 0.983 - ETA: 2:31 - loss: 0.0596 - acc: 0.983 - ETA: 2:31 - loss: 0.0606 - acc: 0.983 - ETA: 2:31 - loss: 0.0609 - acc: 0.983 - ETA: 2:30 - loss: 0.0606 - acc: 0.983 - ETA: 2:30 - loss: 0.0603 - acc: 0.983 - ETA: 2:30 - loss: 0.0603 - acc: 0.983 - ETA: 2:30 - loss: 0.0602 - acc: 0.983 - ETA: 2:30 - loss: 0.0600 - acc: 0.983 - ETA: 2:29 - loss: 0.0597 - acc: 0.983 - ETA: 2:29 - loss: 0.0596 - acc: 0.983 - ETA: 2:30 - loss: 0.0601 - acc: 0.983 - ETA: 2:29 - loss: 0.0599 - acc: 0.983 - ETA: 2:29 - loss: 0.0597 - acc: 0.983 - ETA: 2:29 - loss: 0.0603 - acc: 0.983 - ETA: 2:29 - loss: 0.0623 - acc: 0.982 - ETA: 2:28 - loss: 0.0621 - acc: 0.983 - ETA: 2:28 - loss: 0.0619 - acc: 0.983 - ETA: 2:28 - loss: 0.0621 - acc: 0.983 - ETA: 2:28 - loss: 0.0624 - acc: 0.982 - ETA: 2:28 - loss: 0.0625 - acc: 0.982 - ETA: 2:28 - loss: 0.0628 - acc: 0.982 - ETA: 2:28 - loss: 0.0626 - acc: 0.982 - ETA: 2:27 - loss: 0.0622 - acc: 0.982 - ETA: 2:27 - loss: 0.0621 - acc: 0.982 - ETA: 2:27 - loss: 0.0622 - acc: 0.982 - ETA: 2:27 - loss: 0.0623 - acc: 0.982 - ETA: 2:27 - loss: 0.0625 - acc: 0.982 - ETA: 2:27 - loss: 0.0626 - acc: 0.982 - ETA: 2:26 - loss: 0.0625 - acc: 0.982 - ETA: 2:26 - loss: 0.0630 - acc: 0.982 - ETA: 2:26 - loss: 0.0634 - acc: 0.981 - ETA: 2:26 - loss: 0.0634 - acc: 0.981 - ETA: 2:26 - loss: 0.0634 - acc: 0.981 - ETA: 2:26 - loss: 0.0632 - acc: 0.981 - ETA: 2:25 - loss: 0.0630 - acc: 0.981 - ETA: 2:25 - loss: 0.0632 - acc: 0.981 - ETA: 2:25 - loss: 0.0640 - acc: 0.981 - ETA: 2:25 - loss: 0.0638 - acc: 0.981 - ETA: 2:24 - loss: 0.0636 - acc: 0.981 - ETA: 2:24 - loss: 0.0636 - acc: 0.981 - ETA: 2:24 - loss: 0.0634 - acc: 0.981 - ETA: 2:23 - loss: 0.0633 - acc: 0.981 - ETA: 2:23 - loss: 0.0630 - acc: 0.981 - ETA: 2:23 - loss: 0.0629 - acc: 0.981 - ETA: 2:23 - loss: 0.0631 - acc: 0.981 - ETA: 2:23 - loss: 0.0631 - acc: 0.981 - ETA: 2:23 - loss: 0.0629 - acc: 0.981 - ETA: 2:22 - loss: 0.0630 - acc: 0.981 - ETA: 2:22 - loss: 0.0627 - acc: 0.981 - ETA: 2:21 - loss: 0.0626 - acc: 0.981 - ETA: 2:21 - loss: 0.0624 - acc: 0.981 - ETA: 2:21 - loss: 0.0621 - acc: 0.981 - ETA: 2:21 - loss: 0.0622 - acc: 0.981 - ETA: 2:21 - loss: 0.0625 - acc: 0.981 - ETA: 2:21 - loss: 0.0626 - acc: 0.981 - ETA: 2:20 - loss: 0.0630 - acc: 0.981 - ETA: 2:20 - loss: 0.0631 - acc: 0.981 - ETA: 2:20 - loss: 0.0633 - acc: 0.980 - ETA: 2:20 - loss: 0.0631 - acc: 0.981 - ETA: 2:19 - loss: 0.0631 - acc: 0.981 - ETA: 2:19 - loss: 0.0629 - acc: 0.981 - ETA: 2:19 - loss: 0.0629 - acc: 0.981 - ETA: 2:19 - loss: 0.0629 - acc: 0.981 - ETA: 2:19 - loss: 0.0628 - acc: 0.980 - ETA: 2:18 - loss: 0.0628 - acc: 0.980 - ETA: 2:18 - loss: 0.0628 - acc: 0.980 - ETA: 2:18 - loss: 0.0629 - acc: 0.980 - ETA: 2:18 - loss: 0.0629 - acc: 0.980 - ETA: 2:17 - loss: 0.0630 - acc: 0.980 - ETA: 2:17 - loss: 0.0628 - acc: 0.980 - ETA: 2:17 - loss: 0.0626 - acc: 0.980 - ETA: 2:17 - loss: 0.0625 - acc: 0.980 - ETA: 2:16 - loss: 0.0627 - acc: 0.980 - ETA: 2:16 - loss: 0.0625 - acc: 0.980 - ETA: 2:16 - loss: 0.0622 - acc: 0.981013056/25000 [==============>...............] - ETA: 2:16 - loss: 0.0620 - acc: 0.981 - ETA: 2:16 - loss: 0.0618 - acc: 0.981 - ETA: 2:15 - loss: 0.0615 - acc: 0.981 - ETA: 2:15 - loss: 0.0618 - acc: 0.981 - ETA: 2:15 - loss: 0.0619 - acc: 0.981 - ETA: 2:15 - loss: 0.0626 - acc: 0.980 - ETA: 2:14 - loss: 0.0626 - acc: 0.980 - ETA: 2:14 - loss: 0.0623 - acc: 0.980 - ETA: 2:14 - loss: 0.0621 - acc: 0.980 - ETA: 2:14 - loss: 0.0622 - acc: 0.980 - ETA: 2:14 - loss: 0.0623 - acc: 0.980 - ETA: 2:13 - loss: 0.0625 - acc: 0.980 - ETA: 2:13 - loss: 0.0627 - acc: 0.980 - ETA: 2:13 - loss: 0.0630 - acc: 0.980 - ETA: 2:13 - loss: 0.0632 - acc: 0.980 - ETA: 2:13 - loss: 0.0631 - acc: 0.980 - ETA: 2:12 - loss: 0.0630 - acc: 0.980 - ETA: 2:12 - loss: 0.0636 - acc: 0.980 - ETA: 2:12 - loss: 0.0637 - acc: 0.980 - ETA: 2:12 - loss: 0.0636 - acc: 0.980 - ETA: 2:11 - loss: 0.0638 - acc: 0.980 - ETA: 2:11 - loss: 0.0642 - acc: 0.980 - ETA: 2:11 - loss: 0.0644 - acc: 0.979 - ETA: 2:11 - loss: 0.0649 - acc: 0.979 - ETA: 2:10 - loss: 0.0652 - acc: 0.979 - ETA: 2:10 - loss: 0.0650 - acc: 0.979 - ETA: 2:10 - loss: 0.0656 - acc: 0.979 - ETA: 2:10 - loss: 0.0658 - acc: 0.979 - ETA: 2:09 - loss: 0.0662 - acc: 0.978 - ETA: 2:09 - loss: 0.0662 - acc: 0.979 - ETA: 2:09 - loss: 0.0663 - acc: 0.979 - ETA: 2:09 - loss: 0.0668 - acc: 0.978 - ETA: 2:08 - loss: 0.0670 - acc: 0.978 - ETA: 2:08 - loss: 0.0669 - acc: 0.979 - ETA: 2:08 - loss: 0.0671 - acc: 0.979 - ETA: 2:08 - loss: 0.0673 - acc: 0.978 - ETA: 2:07 - loss: 0.0673 - acc: 0.979 - ETA: 2:07 - loss: 0.0676 - acc: 0.978 - ETA: 2:07 - loss: 0.0676 - acc: 0.978 - ETA: 2:07 - loss: 0.0677 - acc: 0.978 - ETA: 2:07 - loss: 0.0678 - acc: 0.978 - ETA: 2:06 - loss: 0.0678 - acc: 0.978 - ETA: 2:06 - loss: 0.0678 - acc: 0.978 - ETA: 2:06 - loss: 0.0677 - acc: 0.978 - ETA: 2:06 - loss: 0.0677 - acc: 0.978 - ETA: 2:05 - loss: 0.0679 - acc: 0.978 - ETA: 2:05 - loss: 0.0677 - acc: 0.978 - ETA: 2:05 - loss: 0.0678 - acc: 0.978 - ETA: 2:05 - loss: 0.0678 - acc: 0.978 - ETA: 2:05 - loss: 0.0679 - acc: 0.977 - ETA: 2:04 - loss: 0.0682 - acc: 0.977 - ETA: 2:04 - loss: 0.0688 - acc: 0.977 - ETA: 2:04 - loss: 0.0686 - acc: 0.977 - ETA: 2:04 - loss: 0.0684 - acc: 0.977 - ETA: 2:03 - loss: 0.0686 - acc: 0.977 - ETA: 2:03 - loss: 0.0688 - acc: 0.977 - ETA: 2:03 - loss: 0.0685 - acc: 0.977 - ETA: 2:03 - loss: 0.0685 - acc: 0.977 - ETA: 2:02 - loss: 0.0683 - acc: 0.977 - ETA: 2:02 - loss: 0.0687 - acc: 0.977 - ETA: 2:02 - loss: 0.0688 - acc: 0.977 - ETA: 2:02 - loss: 0.0689 - acc: 0.977 - ETA: 2:02 - loss: 0.0689 - acc: 0.977 - ETA: 2:01 - loss: 0.0687 - acc: 0.977 - ETA: 2:01 - loss: 0.0686 - acc: 0.977 - ETA: 2:01 - loss: 0.0692 - acc: 0.977 - ETA: 2:01 - loss: 0.0691 - acc: 0.977 - ETA: 2:00 - loss: 0.0690 - acc: 0.977 - ETA: 2:00 - loss: 0.0689 - acc: 0.977 - ETA: 2:00 - loss: 0.0689 - acc: 0.977 - ETA: 2:00 - loss: 0.0690 - acc: 0.977 - ETA: 2:00 - loss: 0.0692 - acc: 0.977 - ETA: 1:59 - loss: 0.0694 - acc: 0.977 - ETA: 1:59 - loss: 0.0692 - acc: 0.977 - ETA: 1:59 - loss: 0.0693 - acc: 0.977 - ETA: 1:59 - loss: 0.0691 - acc: 0.977 - ETA: 1:59 - loss: 0.0691 - acc: 0.977 - ETA: 1:58 - loss: 0.0689 - acc: 0.977 - ETA: 1:58 - loss: 0.0692 - acc: 0.977 - ETA: 1:58 - loss: 0.0693 - acc: 0.977 - ETA: 1:58 - loss: 0.0698 - acc: 0.977 - ETA: 1:57 - loss: 0.0696 - acc: 0.977 - ETA: 1:57 - loss: 0.0694 - acc: 0.977 - ETA: 1:57 - loss: 0.0699 - acc: 0.977 - ETA: 1:56 - loss: 0.0703 - acc: 0.977 - ETA: 1:56 - loss: 0.0701 - acc: 0.977 - ETA: 1:55 - loss: 0.0704 - acc: 0.977 - ETA: 1:55 - loss: 0.0707 - acc: 0.977 - ETA: 1:55 - loss: 0.0706 - acc: 0.977 - ETA: 1:54 - loss: 0.0706 - acc: 0.977 - ETA: 1:54 - loss: 0.0706 - acc: 0.977 - ETA: 1:54 - loss: 0.0708 - acc: 0.976 - ETA: 1:54 - loss: 0.0706 - acc: 0.977 - ETA: 1:53 - loss: 0.0708 - acc: 0.976 - ETA: 1:53 - loss: 0.0707 - acc: 0.976 - ETA: 1:53 - loss: 0.0706 - acc: 0.977 - ETA: 1:52 - loss: 0.0704 - acc: 0.977 - ETA: 1:52 - loss: 0.0703 - acc: 0.977 - ETA: 1:52 - loss: 0.0703 - acc: 0.977 - ETA: 1:51 - loss: 0.0702 - acc: 0.977 - ETA: 1:51 - loss: 0.0702 - acc: 0.977 - ETA: 1:51 - loss: 0.0703 - acc: 0.977 - ETA: 1:50 - loss: 0.0710 - acc: 0.976 - ETA: 1:50 - loss: 0.0709 - acc: 0.977 - ETA: 1:50 - loss: 0.0709 - acc: 0.976 - ETA: 1:50 - loss: 0.0708 - acc: 0.977 - ETA: 1:49 - loss: 0.0709 - acc: 0.976 - ETA: 1:49 - loss: 0.0708 - acc: 0.977 - ETA: 1:49 - loss: 0.0707 - acc: 0.977 - ETA: 1:48 - loss: 0.0708 - acc: 0.976 - ETA: 1:48 - loss: 0.0707 - acc: 0.976 - ETA: 1:48 - loss: 0.0706 - acc: 0.976 - ETA: 1:47 - loss: 0.0706 - acc: 0.976 - ETA: 1:47 - loss: 0.0707 - acc: 0.976 - ETA: 1:47 - loss: 0.0706 - acc: 0.976 - ETA: 1:46 - loss: 0.0705 - acc: 0.976 - ETA: 1:46 - loss: 0.0703 - acc: 0.976 - ETA: 1:46 - loss: 0.0704 - acc: 0.976 - ETA: 1:45 - loss: 0.0705 - acc: 0.976 - ETA: 1:45 - loss: 0.0703 - acc: 0.976 - ETA: 1:45 - loss: 0.0702 - acc: 0.977 - ETA: 1:44 - loss: 0.0700 - acc: 0.977 - ETA: 1:44 - loss: 0.0698 - acc: 0.977 - ETA: 1:44 - loss: 0.0699 - acc: 0.977 - ETA: 1:43 - loss: 0.0702 - acc: 0.977 - ETA: 1:43 - loss: 0.0705 - acc: 0.976 - ETA: 1:43 - loss: 0.0704 - acc: 0.977 - ETA: 1:42 - loss: 0.0705 - acc: 0.976 - ETA: 1:42 - loss: 0.0704 - acc: 0.977 - ETA: 1:42 - loss: 0.0706 - acc: 0.976 - ETA: 1:42 - loss: 0.0705 - acc: 0.977 - ETA: 1:41 - loss: 0.0704 - acc: 0.976 - ETA: 1:41 - loss: 0.0703 - acc: 0.977 - ETA: 1:41 - loss: 0.0703 - acc: 0.977 - ETA: 1:40 - loss: 0.0701 - acc: 0.977 - ETA: 1:40 - loss: 0.0703 - acc: 0.976 - ETA: 1:40 - loss: 0.0703 - acc: 0.976 - ETA: 1:40 - loss: 0.0702 - acc: 0.976 - ETA: 1:39 - loss: 0.0701 - acc: 0.976 - ETA: 1:39 - loss: 0.0706 - acc: 0.976 - ETA: 1:39 - loss: 0.0711 - acc: 0.976 - ETA: 1:39 - loss: 0.0709 - acc: 0.976 - ETA: 1:39 - loss: 0.0708 - acc: 0.976 - ETA: 1:38 - loss: 0.0706 - acc: 0.977 - ETA: 1:38 - loss: 0.0707 - acc: 0.977 - ETA: 1:38 - loss: 0.0712 - acc: 0.977 - ETA: 1:38 - loss: 0.0712 - acc: 0.976 - ETA: 1:38 - loss: 0.0710 - acc: 0.977 - ETA: 1:37 - loss: 0.0711 - acc: 0.977 - ETA: 1:37 - loss: 0.0710 - acc: 0.977 - ETA: 1:37 - loss: 0.0709 - acc: 0.977 - ETA: 1:37 - loss: 0.0708 - acc: 0.977 - ETA: 1:36 - loss: 0.0708 - acc: 0.977 - ETA: 1:36 - loss: 0.0706 - acc: 0.977 - ETA: 1:36 - loss: 0.0705 - acc: 0.977 - ETA: 1:36 - loss: 0.0703 - acc: 0.977 - ETA: 1:35 - loss: 0.0708 - acc: 0.977 - ETA: 1:35 - loss: 0.0709 - acc: 0.977 - ETA: 1:35 - loss: 0.0708 - acc: 0.977 - ETA: 1:35 - loss: 0.0711 - acc: 0.976 - ETA: 1:35 - loss: 0.0715 - acc: 0.976 - ETA: 1:34 - loss: 0.0714 - acc: 0.976 - ETA: 1:34 - loss: 0.0713 - acc: 0.976 - ETA: 1:34 - loss: 0.0712 - acc: 0.976 - ETA: 1:34 - loss: 0.0713 - acc: 0.976 - ETA: 1:33 - loss: 0.0713 - acc: 0.976 - ETA: 1:33 - loss: 0.0716 - acc: 0.976 - ETA: 1:33 - loss: 0.0716 - acc: 0.976 - ETA: 1:33 - loss: 0.0716 - acc: 0.976 - ETA: 1:33 - loss: 0.0716 - acc: 0.976 - ETA: 1:32 - loss: 0.0717 - acc: 0.976 - ETA: 1:32 - loss: 0.0718 - acc: 0.976 - ETA: 1:32 - loss: 0.0721 - acc: 0.976 - ETA: 1:32 - loss: 0.0719 - acc: 0.976 - ETA: 1:32 - loss: 0.0718 - acc: 0.976 - ETA: 1:31 - loss: 0.0718 - acc: 0.976 - ETA: 1:31 - loss: 0.0717 - acc: 0.976 - ETA: 1:31 - loss: 0.0724 - acc: 0.976 - ETA: 1:31 - loss: 0.0727 - acc: 0.976 - ETA: 1:31 - loss: 0.0726 - acc: 0.976 - ETA: 1:30 - loss: 0.0726 - acc: 0.976 - ETA: 1:30 - loss: 0.0729 - acc: 0.976 - ETA: 1:30 - loss: 0.0729 - acc: 0.976 - ETA: 1:30 - loss: 0.0731 - acc: 0.976 - ETA: 1:30 - loss: 0.0733 - acc: 0.976 - ETA: 1:29 - loss: 0.0736 - acc: 0.976 - ETA: 1:29 - loss: 0.0739 - acc: 0.976 - ETA: 1:29 - loss: 0.0739 - acc: 0.976 - ETA: 1:29 - loss: 0.0741 - acc: 0.976 - ETA: 1:28 - loss: 0.0741 - acc: 0.976 - ETA: 1:28 - loss: 0.0741 - acc: 0.976 - ETA: 1:28 - loss: 0.0741 - acc: 0.976 - ETA: 1:28 - loss: 0.0741 - acc: 0.976 - ETA: 1:28 - loss: 0.0741 - acc: 0.976 - ETA: 1:27 - loss: 0.0744 - acc: 0.975 - ETA: 1:27 - loss: 0.0746 - acc: 0.975 - ETA: 1:27 - loss: 0.0747 - acc: 0.975 - ETA: 1:27 - loss: 0.0749 - acc: 0.975 - ETA: 1:27 - loss: 0.0750 - acc: 0.975 - ETA: 1:26 - loss: 0.0750 - acc: 0.975 - ETA: 1:26 - loss: 0.0753 - acc: 0.975 - ETA: 1:26 - loss: 0.0753 - acc: 0.975 - ETA: 1:26 - loss: 0.0752 - acc: 0.975 - ETA: 1:25 - loss: 0.0752 - acc: 0.975319744/25000 [======================>.......] - ETA: 1:25 - loss: 0.0752 - acc: 0.975 - ETA: 1:25 - loss: 0.0753 - acc: 0.975 - ETA: 1:25 - loss: 0.0752 - acc: 0.975 - ETA: 1:25 - loss: 0.0754 - acc: 0.975 - ETA: 1:24 - loss: 0.0753 - acc: 0.975 - ETA: 1:24 - loss: 0.0752 - acc: 0.975 - ETA: 1:24 - loss: 0.0754 - acc: 0.975 - ETA: 1:24 - loss: 0.0753 - acc: 0.975 - ETA: 1:24 - loss: 0.0752 - acc: 0.975 - ETA: 1:23 - loss: 0.0751 - acc: 0.975 - ETA: 1:23 - loss: 0.0750 - acc: 0.975 - ETA: 1:23 - loss: 0.0751 - acc: 0.975 - ETA: 1:23 - loss: 0.0752 - acc: 0.975 - ETA: 1:22 - loss: 0.0751 - acc: 0.975 - ETA: 1:22 - loss: 0.0752 - acc: 0.975 - ETA: 1:22 - loss: 0.0752 - acc: 0.975 - ETA: 1:22 - loss: 0.0753 - acc: 0.975 - ETA: 1:22 - loss: 0.0755 - acc: 0.974 - ETA: 1:21 - loss: 0.0754 - acc: 0.975 - ETA: 1:21 - loss: 0.0752 - acc: 0.975 - ETA: 1:21 - loss: 0.0751 - acc: 0.975 - ETA: 1:21 - loss: 0.0750 - acc: 0.975 - ETA: 1:21 - loss: 0.0749 - acc: 0.975 - ETA: 1:20 - loss: 0.0750 - acc: 0.975 - ETA: 1:20 - loss: 0.0749 - acc: 0.975 - ETA: 1:20 - loss: 0.0750 - acc: 0.975 - ETA: 1:19 - loss: 0.0752 - acc: 0.975 - ETA: 1:19 - loss: 0.0754 - acc: 0.975 - ETA: 1:19 - loss: 0.0755 - acc: 0.975 - ETA: 1:19 - loss: 0.0755 - acc: 0.975 - ETA: 1:18 - loss: 0.0755 - acc: 0.974 - ETA: 1:18 - loss: 0.0755 - acc: 0.974 - ETA: 1:18 - loss: 0.0755 - acc: 0.974 - ETA: 1:18 - loss: 0.0754 - acc: 0.974 - ETA: 1:18 - loss: 0.0755 - acc: 0.974 - ETA: 1:17 - loss: 0.0755 - acc: 0.974 - ETA: 1:17 - loss: 0.0754 - acc: 0.974 - ETA: 1:17 - loss: 0.0756 - acc: 0.974 - ETA: 1:17 - loss: 0.0756 - acc: 0.974 - ETA: 1:16 - loss: 0.0756 - acc: 0.974 - ETA: 1:16 - loss: 0.0756 - acc: 0.974 - ETA: 1:16 - loss: 0.0755 - acc: 0.974 - ETA: 1:16 - loss: 0.0753 - acc: 0.974 - ETA: 1:16 - loss: 0.0752 - acc: 0.975 - ETA: 1:15 - loss: 0.0751 - acc: 0.975 - ETA: 1:15 - loss: 0.0750 - acc: 0.975 - ETA: 1:15 - loss: 0.0750 - acc: 0.975 - ETA: 1:15 - loss: 0.0748 - acc: 0.975 - ETA: 1:14 - loss: 0.0748 - acc: 0.975 - ETA: 1:14 - loss: 0.0751 - acc: 0.974 - ETA: 1:14 - loss: 0.0750 - acc: 0.974 - ETA: 1:14 - loss: 0.0751 - acc: 0.974 - ETA: 1:14 - loss: 0.0754 - acc: 0.974 - ETA: 1:13 - loss: 0.0753 - acc: 0.975 - ETA: 1:13 - loss: 0.0752 - acc: 0.975 - ETA: 1:13 - loss: 0.0751 - acc: 0.975 - ETA: 1:13 - loss: 0.0751 - acc: 0.975 - ETA: 1:13 - loss: 0.0750 - acc: 0.975 - ETA: 1:12 - loss: 0.0751 - acc: 0.975 - ETA: 1:12 - loss: 0.0751 - acc: 0.975 - ETA: 1:12 - loss: 0.0752 - acc: 0.974 - ETA: 1:12 - loss: 0.0751 - acc: 0.975 - ETA: 1:11 - loss: 0.0751 - acc: 0.975 - ETA: 1:11 - loss: 0.0751 - acc: 0.975 - ETA: 1:11 - loss: 0.0755 - acc: 0.974 - ETA: 1:11 - loss: 0.0754 - acc: 0.974 - ETA: 1:11 - loss: 0.0754 - acc: 0.974 - ETA: 1:10 - loss: 0.0755 - acc: 0.974 - ETA: 1:10 - loss: 0.0755 - acc: 0.974 - ETA: 1:10 - loss: 0.0754 - acc: 0.974 - ETA: 1:10 - loss: 0.0754 - acc: 0.974 - ETA: 1:10 - loss: 0.0753 - acc: 0.974 - ETA: 1:09 - loss: 0.0753 - acc: 0.974 - ETA: 1:09 - loss: 0.0752 - acc: 0.974 - ETA: 1:09 - loss: 0.0751 - acc: 0.974 - ETA: 1:09 - loss: 0.0751 - acc: 0.974 - ETA: 1:08 - loss: 0.0750 - acc: 0.974 - ETA: 1:08 - loss: 0.0753 - acc: 0.974 - ETA: 1:08 - loss: 0.0751 - acc: 0.974 - ETA: 1:08 - loss: 0.0753 - acc: 0.974 - ETA: 1:07 - loss: 0.0757 - acc: 0.974 - ETA: 1:07 - loss: 0.0762 - acc: 0.974 - ETA: 1:07 - loss: 0.0762 - acc: 0.974 - ETA: 1:06 - loss: 0.0761 - acc: 0.974 - ETA: 1:06 - loss: 0.0760 - acc: 0.974 - ETA: 1:06 - loss: 0.0761 - acc: 0.974 - ETA: 1:06 - loss: 0.0760 - acc: 0.974 - ETA: 1:06 - loss: 0.0759 - acc: 0.974 - ETA: 1:05 - loss: 0.0759 - acc: 0.974 - ETA: 1:05 - loss: 0.0758 - acc: 0.974 - ETA: 1:05 - loss: 0.0761 - acc: 0.974 - ETA: 1:05 - loss: 0.0762 - acc: 0.974 - ETA: 1:04 - loss: 0.0761 - acc: 0.974 - ETA: 1:04 - loss: 0.0762 - acc: 0.974 - ETA: 1:04 - loss: 0.0762 - acc: 0.974 - ETA: 1:03 - loss: 0.0762 - acc: 0.974 - ETA: 1:03 - loss: 0.0762 - acc: 0.974 - ETA: 1:03 - loss: 0.0763 - acc: 0.974 - ETA: 1:03 - loss: 0.0765 - acc: 0.974 - ETA: 1:02 - loss: 0.0764 - acc: 0.974 - ETA: 1:02 - loss: 0.0763 - acc: 0.974 - ETA: 1:02 - loss: 0.0762 - acc: 0.974 - ETA: 1:02 - loss: 0.0761 - acc: 0.974 - ETA: 1:01 - loss: 0.0761 - acc: 0.974 - ETA: 1:01 - loss: 0.0763 - acc: 0.974 - ETA: 1:01 - loss: 0.0762 - acc: 0.974 - ETA: 1:01 - loss: 0.0762 - acc: 0.974 - ETA: 1:00 - loss: 0.0761 - acc: 0.974 - ETA: 1:00 - loss: 0.0761 - acc: 0.974 - ETA: 1:00 - loss: 0.0763 - acc: 0.974 - ETA: 59s - loss: 0.0763 - acc: 0.974 - ETA: 59s - loss: 0.0766 - acc: 0.97 - ETA: 59s - loss: 0.0766 - acc: 0.97 - ETA: 59s - loss: 0.0765 - acc: 0.97 - ETA: 58s - loss: 0.0765 - acc: 0.97 - ETA: 58s - loss: 0.0764 - acc: 0.97 - ETA: 58s - loss: 0.0763 - acc: 0.97 - ETA: 58s - loss: 0.0764 - acc: 0.97 - ETA: 57s - loss: 0.0763 - acc: 0.97 - ETA: 57s - loss: 0.0764 - acc: 0.97 - ETA: 57s - loss: 0.0763 - acc: 0.97 - ETA: 57s - loss: 0.0764 - acc: 0.97 - ETA: 56s - loss: 0.0765 - acc: 0.97 - ETA: 56s - loss: 0.0764 - acc: 0.97 - ETA: 56s - loss: 0.0764 - acc: 0.97 - ETA: 56s - loss: 0.0766 - acc: 0.97 - ETA: 55s - loss: 0.0765 - acc: 0.97 - ETA: 55s - loss: 0.0765 - acc: 0.97 - ETA: 55s - loss: 0.0765 - acc: 0.97 - ETA: 55s - loss: 0.0764 - acc: 0.97 - ETA: 54s - loss: 0.0765 - acc: 0.97 - ETA: 54s - loss: 0.0765 - acc: 0.97 - ETA: 54s - loss: 0.0764 - acc: 0.97 - ETA: 54s - loss: 0.0763 - acc: 0.97 - ETA: 53s - loss: 0.0762 - acc: 0.97 - ETA: 53s - loss: 0.0761 - acc: 0.97 - ETA: 53s - loss: 0.0760 - acc: 0.97 - ETA: 52s - loss: 0.0759 - acc: 0.97 - ETA: 52s - loss: 0.0760 - acc: 0.97 - ETA: 52s - loss: 0.0761 - acc: 0.97 - ETA: 52s - loss: 0.0763 - acc: 0.97 - ETA: 51s - loss: 0.0763 - acc: 0.97 - ETA: 51s - loss: 0.0763 - acc: 0.97 - ETA: 51s - loss: 0.0763 - acc: 0.97 - ETA: 51s - loss: 0.0762 - acc: 0.97 - ETA: 50s - loss: 0.0762 - acc: 0.97 - ETA: 50s - loss: 0.0762 - acc: 0.97 - ETA: 50s - loss: 0.0762 - acc: 0.97 - ETA: 50s - loss: 0.0762 - acc: 0.97 - ETA: 49s - loss: 0.0761 - acc: 0.97 - ETA: 49s - loss: 0.0760 - acc: 0.97 - ETA: 49s - loss: 0.0759 - acc: 0.97 - ETA: 49s - loss: 0.0759 - acc: 0.97 - ETA: 48s - loss: 0.0758 - acc: 0.97 - ETA: 48s - loss: 0.0757 - acc: 0.97 - ETA: 48s - loss: 0.0756 - acc: 0.97 - ETA: 48s - loss: 0.0755 - acc: 0.97 - ETA: 47s - loss: 0.0755 - acc: 0.97 - ETA: 47s - loss: 0.0755 - acc: 0.97 - ETA: 47s - loss: 0.0755 - acc: 0.97 - ETA: 47s - loss: 0.0756 - acc: 0.97 - ETA: 46s - loss: 0.0755 - acc: 0.97 - ETA: 46s - loss: 0.0754 - acc: 0.97 - ETA: 46s - loss: 0.0753 - acc: 0.97 - ETA: 46s - loss: 0.0752 - acc: 0.97 - ETA: 46s - loss: 0.0752 - acc: 0.97 - ETA: 45s - loss: 0.0751 - acc: 0.97 - ETA: 45s - loss: 0.0751 - acc: 0.97 - ETA: 45s - loss: 0.0752 - acc: 0.97 - ETA: 45s - loss: 0.0752 - acc: 0.97 - ETA: 44s - loss: 0.0752 - acc: 0.97 - ETA: 44s - loss: 0.0752 - acc: 0.97 - ETA: 44s - loss: 0.0752 - acc: 0.97 - ETA: 44s - loss: 0.0751 - acc: 0.97 - ETA: 43s - loss: 0.0751 - acc: 0.97 - ETA: 43s - loss: 0.0751 - acc: 0.97 - ETA: 43s - loss: 0.0750 - acc: 0.97 - ETA: 43s - loss: 0.0749 - acc: 0.97 - ETA: 42s - loss: 0.0749 - acc: 0.97 - ETA: 42s - loss: 0.0748 - acc: 0.97 - ETA: 42s - loss: 0.0747 - acc: 0.97 - ETA: 42s - loss: 0.0746 - acc: 0.97 - ETA: 41s - loss: 0.0748 - acc: 0.97 - ETA: 41s - loss: 0.0749 - acc: 0.97 - ETA: 41s - loss: 0.0749 - acc: 0.97 - ETA: 41s - loss: 0.0749 - acc: 0.97 - ETA: 40s - loss: 0.0749 - acc: 0.97 - ETA: 40s - loss: 0.0749 - acc: 0.97 - ETA: 40s - loss: 0.0748 - acc: 0.97 - ETA: 40s - loss: 0.0747 - acc: 0.97 - ETA: 39s - loss: 0.0748 - acc: 0.97 - ETA: 39s - loss: 0.0747 - acc: 0.97 - ETA: 39s - loss: 0.0746 - acc: 0.97 - ETA: 39s - loss: 0.0745 - acc: 0.97 - ETA: 39s - loss: 0.0744 - acc: 0.97 - ETA: 38s - loss: 0.0744 - acc: 0.97 - ETA: 38s - loss: 0.0747 - acc: 0.97 - ETA: 38s - loss: 0.0749 - acc: 0.97 - ETA: 38s - loss: 0.0748 - acc: 0.97 - ETA: 37s - loss: 0.0747 - acc: 0.97 - ETA: 37s - loss: 0.0747 - acc: 0.97 - ETA: 37s - loss: 0.0746 - acc: 0.97 - ETA: 37s - loss: 0.0745 - acc: 0.97 - ETA: 36s - loss: 0.0745 - acc: 0.97 - ETA: 36s - loss: 0.0745 - acc: 0.97 - ETA: 36s - loss: 0.0745 - acc: 0.97 - ETA: 36s - loss: 0.0746 - acc: 0.97 - ETA: 35s - loss: 0.0745 - acc: 0.97 - ETA: 35s - loss: 0.0744 - acc: 0.9755"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - ETA: 35s - loss: 0.0744 - acc: 0.97 - ETA: 35s - loss: 0.0743 - acc: 0.97 - ETA: 34s - loss: 0.0742 - acc: 0.97 - ETA: 34s - loss: 0.0742 - acc: 0.97 - ETA: 34s - loss: 0.0742 - acc: 0.97 - ETA: 34s - loss: 0.0743 - acc: 0.97 - ETA: 34s - loss: 0.0745 - acc: 0.97 - ETA: 33s - loss: 0.0745 - acc: 0.97 - ETA: 33s - loss: 0.0745 - acc: 0.97 - ETA: 33s - loss: 0.0745 - acc: 0.97 - ETA: 33s - loss: 0.0745 - acc: 0.97 - ETA: 32s - loss: 0.0744 - acc: 0.97 - ETA: 32s - loss: 0.0743 - acc: 0.97 - ETA: 32s - loss: 0.0746 - acc: 0.97 - ETA: 32s - loss: 0.0745 - acc: 0.97 - ETA: 31s - loss: 0.0744 - acc: 0.97 - ETA: 31s - loss: 0.0744 - acc: 0.97 - ETA: 31s - loss: 0.0744 - acc: 0.97 - ETA: 31s - loss: 0.0744 - acc: 0.97 - ETA: 31s - loss: 0.0745 - acc: 0.97 - ETA: 30s - loss: 0.0745 - acc: 0.97 - ETA: 30s - loss: 0.0745 - acc: 0.97 - ETA: 30s - loss: 0.0745 - acc: 0.97 - ETA: 30s - loss: 0.0744 - acc: 0.97 - ETA: 29s - loss: 0.0744 - acc: 0.97 - ETA: 29s - loss: 0.0744 - acc: 0.97 - ETA: 29s - loss: 0.0743 - acc: 0.97 - ETA: 29s - loss: 0.0744 - acc: 0.97 - ETA: 28s - loss: 0.0746 - acc: 0.97 - ETA: 28s - loss: 0.0747 - acc: 0.97 - ETA: 28s - loss: 0.0746 - acc: 0.97 - ETA: 28s - loss: 0.0745 - acc: 0.97 - ETA: 28s - loss: 0.0744 - acc: 0.97 - ETA: 27s - loss: 0.0744 - acc: 0.97 - ETA: 27s - loss: 0.0743 - acc: 0.97 - ETA: 27s - loss: 0.0742 - acc: 0.97 - ETA: 27s - loss: 0.0743 - acc: 0.97 - ETA: 26s - loss: 0.0744 - acc: 0.97 - ETA: 26s - loss: 0.0744 - acc: 0.97 - ETA: 26s - loss: 0.0745 - acc: 0.97 - ETA: 26s - loss: 0.0745 - acc: 0.97 - ETA: 26s - loss: 0.0747 - acc: 0.97 - ETA: 25s - loss: 0.0748 - acc: 0.97 - ETA: 25s - loss: 0.0749 - acc: 0.97 - ETA: 25s - loss: 0.0748 - acc: 0.97 - ETA: 25s - loss: 0.0748 - acc: 0.97 - ETA: 24s - loss: 0.0749 - acc: 0.97 - ETA: 24s - loss: 0.0749 - acc: 0.97 - ETA: 24s - loss: 0.0748 - acc: 0.97 - ETA: 24s - loss: 0.0749 - acc: 0.97 - ETA: 24s - loss: 0.0749 - acc: 0.97 - ETA: 23s - loss: 0.0749 - acc: 0.97 - ETA: 23s - loss: 0.0748 - acc: 0.97 - ETA: 23s - loss: 0.0751 - acc: 0.97 - ETA: 23s - loss: 0.0752 - acc: 0.97 - ETA: 22s - loss: 0.0751 - acc: 0.97 - ETA: 22s - loss: 0.0751 - acc: 0.97 - ETA: 22s - loss: 0.0751 - acc: 0.97 - ETA: 22s - loss: 0.0750 - acc: 0.97 - ETA: 22s - loss: 0.0753 - acc: 0.97 - ETA: 21s - loss: 0.0752 - acc: 0.97 - ETA: 21s - loss: 0.0752 - acc: 0.97 - ETA: 21s - loss: 0.0752 - acc: 0.97 - ETA: 21s - loss: 0.0753 - acc: 0.97 - ETA: 20s - loss: 0.0752 - acc: 0.97 - ETA: 20s - loss: 0.0751 - acc: 0.97 - ETA: 20s - loss: 0.0751 - acc: 0.97 - ETA: 20s - loss: 0.0752 - acc: 0.97 - ETA: 20s - loss: 0.0753 - acc: 0.97 - ETA: 19s - loss: 0.0755 - acc: 0.97 - ETA: 19s - loss: 0.0755 - acc: 0.97 - ETA: 19s - loss: 0.0755 - acc: 0.97 - ETA: 19s - loss: 0.0755 - acc: 0.97 - ETA: 18s - loss: 0.0754 - acc: 0.97 - ETA: 18s - loss: 0.0753 - acc: 0.97 - ETA: 18s - loss: 0.0754 - acc: 0.97 - ETA: 18s - loss: 0.0756 - acc: 0.97 - ETA: 18s - loss: 0.0756 - acc: 0.97 - ETA: 17s - loss: 0.0757 - acc: 0.97 - ETA: 17s - loss: 0.0756 - acc: 0.97 - ETA: 17s - loss: 0.0756 - acc: 0.97 - ETA: 17s - loss: 0.0756 - acc: 0.97 - ETA: 17s - loss: 0.0755 - acc: 0.97 - ETA: 16s - loss: 0.0755 - acc: 0.97 - ETA: 16s - loss: 0.0755 - acc: 0.97 - ETA: 16s - loss: 0.0755 - acc: 0.97 - ETA: 16s - loss: 0.0755 - acc: 0.97 - ETA: 15s - loss: 0.0755 - acc: 0.97 - ETA: 15s - loss: 0.0755 - acc: 0.97 - ETA: 15s - loss: 0.0754 - acc: 0.97 - ETA: 15s - loss: 0.0753 - acc: 0.97 - ETA: 15s - loss: 0.0754 - acc: 0.97 - ETA: 14s - loss: 0.0756 - acc: 0.97 - ETA: 14s - loss: 0.0755 - acc: 0.97 - ETA: 14s - loss: 0.0756 - acc: 0.97 - ETA: 14s - loss: 0.0759 - acc: 0.97 - ETA: 14s - loss: 0.0759 - acc: 0.97 - ETA: 13s - loss: 0.0759 - acc: 0.97 - ETA: 13s - loss: 0.0758 - acc: 0.97 - ETA: 13s - loss: 0.0758 - acc: 0.97 - ETA: 13s - loss: 0.0758 - acc: 0.97 - ETA: 12s - loss: 0.0758 - acc: 0.97 - ETA: 12s - loss: 0.0757 - acc: 0.97 - ETA: 12s - loss: 0.0756 - acc: 0.97 - ETA: 12s - loss: 0.0757 - acc: 0.97 - ETA: 12s - loss: 0.0758 - acc: 0.97 - ETA: 11s - loss: 0.0757 - acc: 0.97 - ETA: 11s - loss: 0.0757 - acc: 0.97 - ETA: 11s - loss: 0.0757 - acc: 0.97 - ETA: 11s - loss: 0.0756 - acc: 0.97 - ETA: 11s - loss: 0.0759 - acc: 0.97 - ETA: 10s - loss: 0.0759 - acc: 0.97 - ETA: 10s - loss: 0.0759 - acc: 0.97 - ETA: 10s - loss: 0.0759 - acc: 0.97 - ETA: 10s - loss: 0.0758 - acc: 0.97 - ETA: 9s - loss: 0.0758 - acc: 0.9745 - ETA: 9s - loss: 0.0757 - acc: 0.974 - ETA: 9s - loss: 0.0757 - acc: 0.974 - ETA: 9s - loss: 0.0756 - acc: 0.974 - ETA: 9s - loss: 0.0757 - acc: 0.974 - ETA: 8s - loss: 0.0757 - acc: 0.974 - ETA: 8s - loss: 0.0757 - acc: 0.974 - ETA: 8s - loss: 0.0757 - acc: 0.974 - ETA: 8s - loss: 0.0757 - acc: 0.974 - ETA: 8s - loss: 0.0759 - acc: 0.974 - ETA: 7s - loss: 0.0759 - acc: 0.974 - ETA: 7s - loss: 0.0759 - acc: 0.974 - ETA: 7s - loss: 0.0758 - acc: 0.974 - ETA: 7s - loss: 0.0758 - acc: 0.974 - ETA: 7s - loss: 0.0757 - acc: 0.974 - ETA: 6s - loss: 0.0757 - acc: 0.974 - ETA: 6s - loss: 0.0761 - acc: 0.974 - ETA: 6s - loss: 0.0761 - acc: 0.974 - ETA: 6s - loss: 0.0760 - acc: 0.974 - ETA: 6s - loss: 0.0760 - acc: 0.974 - ETA: 5s - loss: 0.0761 - acc: 0.974 - ETA: 5s - loss: 0.0761 - acc: 0.974 - ETA: 5s - loss: 0.0760 - acc: 0.974 - ETA: 5s - loss: 0.0760 - acc: 0.974 - ETA: 4s - loss: 0.0760 - acc: 0.974 - ETA: 4s - loss: 0.0761 - acc: 0.974 - ETA: 4s - loss: 0.0760 - acc: 0.974 - ETA: 4s - loss: 0.0759 - acc: 0.974 - ETA: 4s - loss: 0.0760 - acc: 0.974 - ETA: 3s - loss: 0.0759 - acc: 0.974 - ETA: 3s - loss: 0.0759 - acc: 0.974 - ETA: 3s - loss: 0.0758 - acc: 0.974 - ETA: 3s - loss: 0.0759 - acc: 0.974 - ETA: 3s - loss: 0.0758 - acc: 0.974 - ETA: 2s - loss: 0.0758 - acc: 0.974 - ETA: 2s - loss: 0.0758 - acc: 0.974 - ETA: 2s - loss: 0.0758 - acc: 0.974 - ETA: 2s - loss: 0.0758 - acc: 0.974 - ETA: 2s - loss: 0.0757 - acc: 0.974 - ETA: 1s - loss: 0.0756 - acc: 0.974 - ETA: 1s - loss: 0.0757 - acc: 0.974 - ETA: 1s - loss: 0.0759 - acc: 0.974 - ETA: 1s - loss: 0.0759 - acc: 0.974 - ETA: 1s - loss: 0.0758 - acc: 0.974 - ETA: 0s - loss: 0.0758 - acc: 0.974 - ETA: 0s - loss: 0.0758 - acc: 0.974 - ETA: 0s - loss: 0.0758 - acc: 0.974 - ETA: 0s - loss: 0.0757 - acc: 0.974 - ETA: 0s - loss: 0.0759 - acc: 0.974 - 180s 7ms/step - loss: 0.0759 - acc: 0.9744 - val_loss: 0.5931 - val_acc: 0.8323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24087f2ebe0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
